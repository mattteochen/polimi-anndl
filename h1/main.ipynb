{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuwVgG3Vbbka"
   },
   "source": [
    "# Artificial Neural Networks and Deep Learning\n",
    "\n",
    "---\n",
    "\n",
    "## Homework 1: Minimal Working Example\n",
    "\n",
    "To make your first submission, follow these steps:\n",
    "1. Create a folder named `[2024-2025] AN2DL/Homework 1` in your Google Drive.\n",
    "2. Upload the `training_set.npz` file to this folder.\n",
    "3. Upload the Jupyter notebook `Homework 1 - Minimal Working Example.ipynb`.\n",
    "4. Load and process the data.\n",
    "5. Implement and train your model.\n",
    "6. Submit the generated `.zip` file to Codabench.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7IqZP5Iblna"
   },
   "source": [
    "## ‚öôÔ∏è Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CO6_Ft_8T56A"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define if we wanna assign different class weights (for class imbalance) during model fitting\n",
    "USE_CLASS_WEIGHTS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GN_cpHlSboXV"
   },
   "source": [
    "## ‚è≥ Load and inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define img shape for vgg: https://arxiv.org/pdf/2110.09508\n",
    "IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"training_set.npz\"\n",
    "OUTLIERS = \"training-data-filter/blacklist.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: maybe adjust\n",
    "train_ratio = 0.64\n",
    "validation_ratio = 0.24\n",
    "test_ratio = 1.0 - train_ratio - validation_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLaoDaG1V1Yg"
   },
   "outputs": [],
   "source": [
    "data = np.load(DATASET)\n",
    "X = data['images']\n",
    "y = data['labels']\n",
    "\n",
    "X = (X).astype('float32')\n",
    "\n",
    "print('Before data points filter shape:', X.shape, y.shape)\n",
    "\n",
    "with open(OUTLIERS, 'r') as file:\n",
    "\tblacklist = json.load(file)\n",
    "blacklist = sorted(blacklist['blacklist'])\n",
    "X = np.delete(X, blacklist, axis=0)\n",
    "y = np.delete(y, blacklist, axis=0)\n",
    "\n",
    "print('After data points filter shape:', X.shape, y.shape)\n",
    "\n",
    "# Percentages taken from:  https://arxiv.org/pdf/2110.09508\n",
    "train_size = int(X.shape[0] * train_ratio)\n",
    "val_size = int(X.shape[0] * validation_ratio)\n",
    "test_size = X.shape[0] - train_size - val_size\n",
    "\n",
    "if not USE_CLASS_WEIGHTS:\n",
    "\t# Convert to one hoot encoding\n",
    "\ty = tfk.utils.to_categorical(y)\n",
    "\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed, test_size=test_size, stratify=y)\n",
    "\tX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=seed, test_size=val_size, stratify=y_train)\n",
    "\tprint(X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape)\n",
    "else:\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed, test_size=test_size, stratify=y)\n",
    "\tX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=seed, test_size=val_size, stratify=y_train)\n",
    "\t# Copy for later\n",
    "\ty_train_cat = y_train\n",
    "\t# Convert to one hoot encoding\n",
    "\ty_train = tfk.utils.to_categorical(y_train)\n",
    "\ty_val = tfk.utils.to_categorical(y_val)\n",
    "\ty_test = tfk.utils.to_categorical(y_test)\n",
    "\tprint(X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "labels = {\n",
    "\t0: \"Basophil\",\n",
    "\t1: \"Eosinophil\",\n",
    "\t2: \"Erythroblast\",\n",
    "\t3: \"Immature granulocytes\",\n",
    "\t4: \"Lymphocyte\",\n",
    "\t5: \"Monocyte\",\n",
    "\t6: \"Neutrophil\",\n",
    "\t7: \"Platelet\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect data\n",
    "# Display a sample of images from the training-validation dataset\n",
    "num_img = 10\n",
    "random_indices = random.sample(range(len(X_val)), num_img)\n",
    "\n",
    "fig, axes = plt.subplots(1, num_img, figsize=(20, 20))\n",
    "\n",
    "def get_label(y):\n",
    "    index = np.where(y == 1)[0]\n",
    "    return labels[int(index)]\n",
    "\n",
    "# Iterate through the selected number of images\n",
    "for i, idx in enumerate(random_indices):\n",
    "    ax = axes[i % num_img]\n",
    "    ax.imshow(np.squeeze(X_val[idx] / 255), vmin=0., vmax=1.)\n",
    "    ax.set_title(get_label(y_val[idx]))\n",
    "    ax.axis('off')\n",
    "\n",
    "# Adjust layout and display the images\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåä Generate class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CLASS_WEIGHTS:\n",
    "    # Flat the train labels\n",
    "    y_train_cat_flat = np.ravel(y_train_cat)\n",
    "\n",
    "    # Make weights proportional to class imbalance\n",
    "\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced', \n",
    "        classes=np.unique(data['labels']), \n",
    "        y=y_train_cat_flat\n",
    "    )\n",
    "\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    from pprint import pprint\n",
    "    print('Class weights:')\n",
    "    pprint(class_weight_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSliIxBvbs2Q"
   },
   "source": [
    "## üõ†Ô∏è Train and Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 800\n",
    "batch_size = 32\n",
    "\n",
    "dropout = 0.4\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers to fine tune\n",
    "vgg_active_layers = set([\n",
    "  \"block5_conv4\",\n",
    "  \"block5_conv3\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ox9jqYyyUJo0"
   },
   "outputs": [],
   "source": [
    "# Initialise imageNet model with pretrained weights, for transfer learning\n",
    "vgg = tfk.applications.VGG19(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    pooling='avg',\n",
    ")\n",
    "\n",
    "# Freeze\n",
    "vgg.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_augmentation(name = 'preprocessing'):\n",
    "\taugmentation = tf.keras.Sequential([\n",
    "\t    tfkl.RandomFlip(\"horizontal_and_vertical\"),\n",
    "\t    tfkl.RandomRotation(0.167), # 60%\n",
    "\t    tfkl.CenterCrop(IMG_SIZE, IMG_SIZE),\n",
    "\t], name=name)\n",
    "\treturn augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(base_model = None, restore_base = True, out_shape = y_train.shape[-1]):\n",
    "\tif restore_base and base_model is not None:\n",
    "\t\tfor l in base_model.layers:\n",
    "\t\t\tl.trainable = False\n",
    "\n",
    "\tinputs = tfk.Input(shape=X_train[0].shape, name='input_layer')\n",
    "\t# Define augmentation layers\n",
    "\taugmentation = build_augmentation()\n",
    "\t# Define network\n",
    "\tx = augmentation(inputs)\n",
    "\tif base_model is not None:\n",
    "\t\tx = base_model(x)\n",
    "\tx = tfkl.Dropout(dropout, name='dropout')(x)\n",
    "\toutputs = tfkl.Dense(out_shape, activation='softmax', name='dense')(x)\n",
    "\n",
    "\t# Define the complete model linking input and output\n",
    "\tm = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
    "\treturn m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model):\n",
    "\tif USE_CLASS_WEIGHTS:\n",
    "\t\tprint('Fitting with class weights!')\n",
    "\t\tfit_history = model.fit(\n",
    "\t    x=X_train,\n",
    "\t    y=y_train,\n",
    "\t    batch_size=batch_size,\n",
    "\t    epochs=epochs,\n",
    "\t    validation_data=(X_val, y_val),\n",
    "\t    class_weight = class_weight_dict,\n",
    "\t    callbacks=[tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True)]\n",
    "\t  ).history\n",
    "\telse:\n",
    "\t\tfit_history = model.fit(\n",
    "\t    x=X_train,\n",
    "\t    y=y_train,\n",
    "\t    batch_size=batch_size,\n",
    "\t    epochs=epochs,\n",
    "\t    validation_data=(X_val, y_val),\n",
    "\t    callbacks=[tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True)]\n",
    "\t  ).history\n",
    "\treturn fit_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_feature_extractor_layers(extractor, layers):\n",
    "\textractor.trainable = True\n",
    "\tfor i, layer in enumerate(extractor.layers):\n",
    "\t\tlayer.trainable = False\n",
    "\t# Set the vgg_active_layers layers as trainable\n",
    "\tfor i, layer in enumerate(extractor.layers):\n",
    "\t\tif layer.name in layers:\n",
    "\t\t\tlayer.trainable = True\n",
    "\n",
    "\t# Print layer indices, names, and trainability status\n",
    "\tfor i, layer in enumerate(extractor.layers):\n",
    "\t\tprint(i, layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_model(model):\n",
    "\t# Display a summary of the model architecture\n",
    "\tmodel.summary(expand_nested=True)\n",
    "\n",
    "\t# Display model architecture with layer shapes and trainable parameters\n",
    "\ttfk.utils.plot_model(model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Classifier training\n",
    "'''\n",
    "# Aug and optimizer params are taken from: https://arxiv.org/pdf/2110.09508\n",
    "model = build_model(base_model=vgg)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "# Define a learning rate schedule that decays by a factor of 0.1 every 7 epochs\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=lr,\n",
    "    decay_steps=7 * (X_train.shape[0] // batch_size),  # Decay every 7 epochs\n",
    "    decay_rate=0.1,\n",
    "    staircase=True\n",
    ")\n",
    "optimizer.learning_rate = lr_schedule\n",
    "\n",
    "# Compile the model with categorical cross-entropy loss and Adam optimizer\n",
    "model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=optimizer, metrics=['accuracy'])\n",
    "display_model(model)\n",
    "\n",
    "# Fit the initial model\n",
    "print('\\n\\nFitting classifier\\n\\n')\n",
    "fit_history = fit_model(model)\n",
    "\n",
    "'''\n",
    "Fine tuning\n",
    "'''\n",
    "# Enable fine tuning\n",
    "enable_feature_extractor_layers(vgg, vgg_active_layers)\n",
    "\n",
    "# Lower lr for fine tuning\n",
    "lr = 1e-4\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "# Compile the model with categorical cross-entropy loss and Adam optimizer\n",
    "model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=optimizer, metrics=['accuracy'])\n",
    "display_model(model)\n",
    "\n",
    "# Fit the initial finetuned model\n",
    "print('\\n\\nFine tuning\\n\\n')\n",
    "fit_history = fit_model(model)\n",
    "\n",
    "# Calculate and print the best validation accuracy achieved\n",
    "final_val_accuracy = round(max(fit_history['val_accuracy']) * 100, 2)\n",
    "print(f'Final validation accuracy: {final_val_accuracy}%')\n",
    "\n",
    "# Save the trained model to a file, including final accuracy in the filename\n",
    "model_filename = f'vgg19-finetuned{len(vgg_active_layers)}layers-{str(final_val_accuracy)}-{datetime.now().strftime(\"%y%m%d_%H%M\")}.keras'\n",
    "model.save(model_filename)\n",
    "\n",
    "# Free memory by deleting the model instance\n",
    "#del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and subplots for loss and accuracy\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 6))\n",
    "\n",
    "# Plot loss for both re-trained and transfer learning models\n",
    "ax1.plot(fit_history['loss'], alpha=0.3, color='#4D61E2', label='training loss', linestyle='--')\n",
    "ax1.plot(fit_history['val_loss'], label='validation loss', alpha=0.8, color='#4D61E2')\n",
    "ax1.set_title('Categorical Crossentropy')\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot accuracy for both re-trained and transfer learning models\n",
    "ax2.plot(fit_history['accuracy'], alpha=0.3, color='#4D61E2', label='training accuracy', linestyle='--')\n",
    "ax2.plot(fit_history['val_accuracy'], label='validation accuracy', alpha=0.8, color='#4D61E2')\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.legend(loc='upper left')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Adjust layout to prevent label overlap and display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëî Load a trained model (if needed!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = tf.keras.models.load_model('KaggleEfficientNetV2L85.1241109_182031.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úçüèø Make evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##loss, acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "##print('Model, accuracy: {:5.2f}%'.format(100 * acc))\n",
    "\n",
    "# Predict labels for the entire test set\n",
    "predictions = model.predict(X_test, verbose=0)\n",
    "\n",
    "# Display the shape of the predictions\n",
    "print(\"Predictions Shape:\", predictions.shape)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "pred_classes = np.argmax(predictions, axis=-1)\n",
    "\n",
    "# Extract ground truth classes\n",
    "true_classes = np.argmax(y_test, axis=-1)\n",
    "\n",
    "# Calculate and display test set accuracy as percentage\n",
    "accuracy = accuracy_score(true_classes, pred_classes)\n",
    "print(f'Accuracy score over the test set: {round(100 * accuracy, 2)}%')\n",
    "\n",
    "# Calculate and display test set precision as percentage\n",
    "precision = precision_score(true_classes, pred_classes, average='weighted')\n",
    "print(f'Precision score over the test set: {round(100 * precision, 2)}%')\n",
    "\n",
    "# Calculate and display test set recall as percentage\n",
    "recall = recall_score(true_classes, pred_classes, average='weighted')\n",
    "print(f'Recall score over the test set: {round(100 * recall, 2)}%')\n",
    "\n",
    "# Calculate and display test set F1 score as percentage\n",
    "f1 = f1_score(true_classes, pred_classes, average='weighted')\n",
    "print(f'F1 score over the test set: {round(100 * f1, 2)}%')\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(true_classes, pred_classes)\n",
    "\n",
    "# Calculate the percentages for each element in the confusion matrix\n",
    "cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "# Combine numbers and percentages into a single string for annotation\n",
    "annot = np.array([f\"{num}\\n({percent:.2f}%)\" for num, percent in zip(cm.flatten(), cm_percentage.flatten())]).reshape(cm.shape)\n",
    "\n",
    "# Plot the confusion matrix with percentages\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_percentage.T, annot=annot, fmt='', xticklabels=list(labels.values()), yticklabels=list(labels.values()), cmap='Blues')\n",
    "plt.xlabel('True labels')\n",
    "plt.ylabel('Predicted labels')\n",
    "plt.title('Confusion Matrix (Percentages)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNp6pUZuddqC"
   },
   "source": [
    "## üìä Prepare Your Submission\n",
    "\n",
    "To prepare your submission, create a `.zip` file that includes all the necessary code to run your model. It **must** include a `model.py` file with the following class:\n",
    "\n",
    "```python\n",
    "# file: model.py\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the internal state of the model.\"\"\"\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return a numpy array with the labels corresponding to the input X.\"\"\"\n",
    "```\n",
    "\n",
    "The next cell shows an example implementation of the `model.py` file, which includes loading model weights from the `weights.keras` file and conducting predictions on provided input data. The `.zip` file is created and downloaded in the last notebook cell.\n",
    "\n",
    "‚ùó Feel free to modify the method implementations to better fit your specific requirements, but please ensure that the class name and method interfaces remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKT4h-9xYwiT"
   },
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the internal state of the model. Note that the __init__\n",
    "        method cannot accept any arguments.\n",
    "\n",
    "        The following is an example loading the weights of a pre-trained\n",
    "        model.\n",
    "        \"\"\"\n",
    "        self.neural_network = tfk.models.load_model('vgg19-88.78-241112_140000.keras')\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the labels corresponding to the input X. Note that X is a numpy\n",
    "        array of shape (n_samples, 96, 96, 3) and the output should be a numpy\n",
    "        array of shape (n_samples,). Therefore, outputs must no be one-hot\n",
    "        encoded.\n",
    "\n",
    "        The following is an example of a prediction from the pre-trained model\n",
    "        loaded in the __init__ method.\n",
    "        \"\"\"\n",
    "        preds = self.neural_network.predict(X)\n",
    "        if len(preds.shape) == 2:\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s18kX1uDconq"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "filename = f'submission_{datetime.now().strftime(\"%y%m%d_%H%M%S\")}.zip'\n",
    "\n",
    "# Add files to the zip command if needed\n",
    "!zip {filename} model.py vgg19-88.78-241112_140000.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPhN8z97sycURDAjAFsp+EI",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "anndl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
