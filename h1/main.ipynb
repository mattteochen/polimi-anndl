{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuwVgG3Vbbka"
   },
   "source": [
    "# Artificial Neural Networks and Deep Learning\n",
    "\n",
    "---\n",
    "\n",
    "## Homework 1: Minimal Working Example\n",
    "\n",
    "To make your first submission, follow these steps:\n",
    "1. Create a folder named `[2024-2025] AN2DL/Homework 1` in your Google Drive.\n",
    "2. Upload the `training_set.npz` file to this folder.\n",
    "3. Upload the Jupyter notebook `Homework 1 - Minimal Working Example.ipynb`.\n",
    "4. Load and process the data.\n",
    "5. Implement and train your model.\n",
    "6. Submit the generated `.zip` file to Codabench.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7IqZP5Iblna"
   },
   "source": [
    "## ‚öôÔ∏è Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CO6_Ft_8T56A"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GN_cpHlSboXV"
   },
   "source": [
    "## ‚è≥ Load and inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLaoDaG1V1Yg"
   },
   "outputs": [],
   "source": [
    "data = np.load('training_set.npz')\n",
    "X = data['images']\n",
    "y = data['labels']\n",
    "\n",
    "# Check class imbalance\n",
    "unique_labels, counts = np.unique(y, return_counts=True)\n",
    "print(\"Unique labels:\", unique_labels)\n",
    "print(\"Counts:\", counts)\n",
    "\n",
    "X = (X).astype('float32')\n",
    "# Convert to one hoot encoding\n",
    "y = tfk.utils.to_categorical(y)\n",
    "\n",
    "print('Before data points filter shape:', X.shape, y.shape)\n",
    "\n",
    "import json\n",
    "with open('training-data-filter/blacklist.json', 'r') as file:\n",
    "\tblacklist = json.load(file)\n",
    "blacklist = sorted(blacklist['blacklist'])\n",
    "X = np.delete(X, blacklist, axis=0)\n",
    "y = np.delete(y, blacklist, axis=0)\n",
    "\n",
    "print('After data points filter shape:', X.shape, y.shape)\n",
    "\n",
    "train_size = int(X.shape[0] * 0.9)\n",
    "val_size = int(X.shape[0] * 0.0)\n",
    "test_size = X.shape[0] - train_size - val_size\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state=seed, test_size=test_size, stratify=y)\n",
    "\n",
    "print(X_train_val.shape, X_test.shape, y_train_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "labels = {\n",
    "\t0: \"Basophil\",\n",
    "\t1: \"Eosinophil\",\n",
    "\t2: \"Erythroblast\",\n",
    "\t3: \"Immature granulocytes\",\n",
    "\t4: \"Lymphocyte\",\n",
    "\t5: \"Monocyte\",\n",
    "\t6: \"Neutrophil\",\n",
    "\t7: \"Platelet\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply random brightness using tf.image\n",
    "def random_brightness(image, max_delta=0.25):\n",
    "    # Adjust brightness by a random factor in the range [-max_delta, max_delta]\n",
    "    return tf.image.adjust_brightness(image, max_delta)\n",
    "\n",
    "\n",
    "plt.imshow(random_brightness(X_train_val[0] / 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect data\n",
    "# Display a sample of images from the training-validation dataset\n",
    "num_img = 10\n",
    "random_indices = random.sample(range(len(X_train_val)), num_img)\n",
    "\n",
    "fig, axes = plt.subplots(1, num_img, figsize=(20, 20))\n",
    "\n",
    "def get_label(y):\n",
    "    index = np.where(y == 1)[0]\n",
    "    return labels[int(index)]\n",
    "\n",
    "# Iterate through the selected number of images\n",
    "for i, idx in enumerate(random_indices):\n",
    "    ax = axes[i % num_img]\n",
    "    ax.imshow(np.squeeze(X_train_val[idx] / 255), vmin=0., vmax=1.)\n",
    "    ax.set_title(get_label(y_train_val[idx]))\n",
    "    ax.axis('off')\n",
    "\n",
    "# Adjust layout and display the images\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSliIxBvbs2Q"
   },
   "source": [
    "## üõ†Ô∏è Train and Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    efficientNetV2 = tfk.applications.EfficientNetV2L(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_tensor=None,\n",
    "        input_shape=X_train_val[0].shape,\n",
    "        classes=len(labels),\n",
    "        pooling='avg',\n",
    "        include_preprocessing=True,\n",
    "        #name=\"efficientnetv2-l\",\n",
    "    )\n",
    "    efficientNetV2.trainable = False\n",
    "    # Define input layer with shape matching the input images\n",
    "    inputs = tfk.Input(shape=X_train_val[0].shape, name='input_layer')\n",
    "\n",
    "    # Apply data augmentation for training robustness\n",
    "    augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomRotation(0.25),                   # Rotation up to ¬±25%\n",
    "        tf.keras.layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
    "        tf.keras.layers.RandomZoom(0.1),                        # Zoom in or out up to 10%\n",
    "        tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),  # Horizontal and vertical flip\n",
    "        tf.keras.layers.RandomBrightness(factor=0.2),           # Brightness adjustment up to ¬±20%\n",
    "    ], name='preprocessing')\n",
    "\n",
    "    x = augmentation(inputs)\n",
    "\n",
    "    # Pass augmented inputs through the MobileNetV3Small feature extractor\n",
    "    x = efficientNetV2(x)\n",
    "\n",
    "    # Add a dropout layer for regularisation\n",
    "    x = tfkl.Dropout(0.3, name='dropout')(x)\n",
    "\n",
    "    # Add final Dense layer for classification with softmax activation\n",
    "    outputs = tfkl.Dense(y_train_val.shape[-1], activation='softmax', name='dense')(x)\n",
    "\n",
    "    # Define the complete model linking input and output\n",
    "    tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
    "\n",
    "    # Compile the model with categorical cross-entropy loss and Adam optimiser\n",
    "    tl_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.AdamW(learning_rate=1e-4), metrics=['accuracy'])\n",
    "\n",
    "    return tl_model\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "#tl_model.summary(expand_nested=True)\n",
    "\n",
    "# Display model architecture with layer shapes and trainable parameters\n",
    "#tfk.utils.plot_model(tl_model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K CV\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "N = 2\n",
    "\n",
    "# Initialize lists to store training histories, scores, and best epochs\n",
    "histories = []\n",
    "scores = []\n",
    "best_epochs = []\n",
    "patience = 20\n",
    "\n",
    "# Create a KFold cross-validation object\n",
    "kfold = KFold(n_splits=N, shuffle=True, random_state=seed)\n",
    "\n",
    "# Loop through each fold\n",
    "for fold_idx, (train_idx, valid_idx) in enumerate(kfold.split(X_train_val, y_train_val)):\n",
    "    print(\"Starting training on fold num: {}\".format(fold_idx+1))\n",
    "\n",
    "    # Build a new dropout model for each fold\n",
    "    k_model = build_model()\n",
    "\n",
    "    # Create an EarlyStopping callback\n",
    "    early_stopping = tfk.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy', mode='max', patience=patience, restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    callbacks = [early_stopping]\n",
    "\n",
    "    # Train the model on the training data for this fold\n",
    "    history = k_model.fit(\n",
    "    x = X_train_val[train_idx],\n",
    "    y = y_train_val[train_idx],\n",
    "    validation_data=(X_train_val[valid_idx], y_train_val[valid_idx]),\n",
    "    batch_size = 1024,\n",
    "    epochs = 400,\n",
    "    callbacks = callbacks,\n",
    "    verbose = 1\n",
    "    ).history\n",
    "\n",
    "    # Evaluate the model on the validation data for this fold\n",
    "    score = k_model.evaluate(X_train_val[valid_idx], y_train_val[valid_idx], verbose=1)\n",
    "    scores.append(score[1])\n",
    "\n",
    "    # Calculate the best epoch for early stopping\n",
    "    best_epoch = len(history['loss']) - patience\n",
    "    best_epochs.append(best_epoch)\n",
    "\n",
    "    # Store the training history for this fold\n",
    "    histories.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of colors for plotting\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "\n",
    "# Print mean and standard deviation of MSE scores\n",
    "print(\"Accuracy\")\n",
    "print(f\"Mean: {np.mean(scores).round(4)}\\nStd:  {np.std(scores).round(4)}\")\n",
    "\n",
    "# Create a figure for MSE visualization\n",
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "# Plot MSE for each fold\n",
    "for fold_idx in range(N):\n",
    "    plt.plot(histories[fold_idx]['val_accuracy'][:-patience], color=colors[fold_idx], label=f'Fold N¬∞{fold_idx+1}')\n",
    "    plt.title('Mean Squared Error')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(alpha=.3)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average best epoch\n",
    "avg_epochs = int(np.mean(best_epochs))\n",
    "print(f\"Best average epoch: {avg_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the final model using the calculated average best epoch\n",
    "final_model = build_model()\n",
    "\n",
    "# Train the final model on the combined training and validation data\n",
    "final_history = final_model.fit(\n",
    "    x = X_train_val,\n",
    "    y = y_train_val,\n",
    "    batch_size = 2048,\n",
    "    epochs = avg_epochs\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and plot the performance of the final model on the test data\n",
    "print('Final Model Test Performance')\n",
    "loss, acc = final_model.evaluate(X_test, y_test, verbose=2)\n",
    "print(loss, acc)\n",
    "\n",
    "# Save the model\n",
    "from datetime import datetime\n",
    "model_filename = f'efficientNetV2L[test{acc}][{datetime.now().strftime(\"%y%m%d_%H%M%S\")}].keras'\n",
    "\n",
    "final_model.save(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëî Load a trained model (if needed!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_model = tf.keras.models.load_model('KaggleEfficientNetV2L85.1241109_182031.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úçüèø Make evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss, acc = tl_model.evaluate(X_test, y_test, verbose=2)\n",
    "#print('Model, accuracy: {:5.2f}%'.format(100 * acc))\n",
    "\n",
    "# Predict labels for the entire test set\n",
    "predictions = tl_model.predict(X_test, verbose=0)\n",
    "\n",
    "# Display the shape of the predictions\n",
    "print(\"Predictions Shape:\", predictions.shape)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "pred_classes = np.argmax(predictions, axis=-1)\n",
    "\n",
    "# Extract ground truth classes\n",
    "true_classes = np.argmax(y_test, axis=-1)\n",
    "\n",
    "# Calculate and display test set accuracy\n",
    "accuracy = accuracy_score(true_classes, pred_classes)\n",
    "print(f'Accuracy score over the test set: {round(accuracy, 4)}')\n",
    "\n",
    "# Calculate and display test set precision\n",
    "precision = precision_score(true_classes, pred_classes, average='weighted')\n",
    "print(f'Precision score over the test set: {round(precision, 4)}')\n",
    "\n",
    "# Calculate and display test set recall\n",
    "recall = recall_score(true_classes, pred_classes, average='weighted')\n",
    "print(f'Recall score over the test set: {round(recall, 4)}')\n",
    "\n",
    "# Calculate and display test set F1 score\n",
    "f1 = f1_score(true_classes, pred_classes, average='weighted')\n",
    "print(f'F1 score over the test set: {round(f1, 4)}')\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(true_classes, pred_classes)\n",
    "\n",
    "# Combine numbers and percentages into a single string for annotation\n",
    "annot = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm.T, annot=annot, fmt='', xticklabels=list(labels.values()), yticklabels=list(labels.values()), cmap='Blues')\n",
    "plt.xlabel('True labels')\n",
    "plt.ylabel('Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNp6pUZuddqC"
   },
   "source": [
    "## üìä Prepare Your Submission\n",
    "\n",
    "To prepare your submission, create a `.zip` file that includes all the necessary code to run your model. It **must** include a `model.py` file with the following class:\n",
    "\n",
    "```python\n",
    "# file: model.py\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the internal state of the model.\"\"\"\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return a numpy array with the labels corresponding to the input X.\"\"\"\n",
    "```\n",
    "\n",
    "The next cell shows an example implementation of the `model.py` file, which includes loading model weights from the `weights.keras` file and conducting predictions on provided input data. The `.zip` file is created and downloaded in the last notebook cell.\n",
    "\n",
    "‚ùó Feel free to modify the method implementations to better fit your specific requirements, but please ensure that the class name and method interfaces remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKT4h-9xYwiT"
   },
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the internal state of the model. Note that the __init__\n",
    "        method cannot accept any arguments.\n",
    "\n",
    "        The following is an example loading the weights of a pre-trained\n",
    "        model.\n",
    "        \"\"\"\n",
    "        self.neural_network = tfk.models.load_model('KaggleEfficientNetV2L90.46.keras')\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the labels corresponding to the input X. Note that X is a numpy\n",
    "        array of shape (n_samples, 96, 96, 3) and the output should be a numpy\n",
    "        array of shape (n_samples,). Therefore, outputs must no be one-hot\n",
    "        encoded.\n",
    "\n",
    "        The following is an example of a prediction from the pre-trained model\n",
    "        loaded in the __init__ method.\n",
    "        \"\"\"\n",
    "        preds = self.neural_network.predict(X)\n",
    "        if len(preds.shape) == 2:\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s18kX1uDconq"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "filename = f'submission_{datetime.now().strftime(\"%y%m%d_%H%M%S\")}.zip'\n",
    "\n",
    "# Add files to the zip command if needed\n",
    "!zip {filename} model.py KaggleEfficientNetV2L90.46.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPhN8z97sycURDAjAFsp+EI",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "anndl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
