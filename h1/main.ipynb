{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuwVgG3Vbbka"
   },
   "source": [
    "# Artificial Neural Networks and Deep Learning\n",
    "\n",
    "---\n",
    "\n",
    "## Homework 1: Minimal Working Example\n",
    "\n",
    "To make your first submission, follow these steps:\n",
    "1. Create a folder named `[2024-2025] AN2DL/Homework 1` in your Google Drive.\n",
    "2. Upload the `training_set.npz` file to this folder.\n",
    "3. Upload the Jupyter notebook `Homework 1 - Minimal Working Example.ipynb`.\n",
    "4. Load and process the data.\n",
    "5. Implement and train your model.\n",
    "6. Submit the generated `.zip` file to Codabench.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7IqZP5Iblna"
   },
   "source": [
    "## ‚öôÔ∏è Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to use Ranger optimizer install these (on Kaggle this tf seems to break CUDA...)\n",
    "#!pip3 install tensorflow==2.14.0\n",
    "#!pip3 install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CO6_Ft_8T56A"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define if we wanna assign different class weights (for class imbalance) during model fitting\n",
    "USE_CLASS_WEIGHTS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GN_cpHlSboXV"
   },
   "source": [
    "## ‚è≥ Load and inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define img shape. Input image is 96x96 hence based on the specified value it will be enlarged or CENTER cropped\n",
    "IMG_SIZE = 96\n",
    "\n",
    "if IMG_SIZE < 96:\n",
    "\tprint('Image will center cropped!')\n",
    "elif IMG_SIZE > 96:\n",
    "\tprint('Image will be enlarged!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"training_set.npz\"\n",
    "OUTLIERS = \"training-data-filter/blacklist.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: maybe adjust\n",
    "train_ratio = 0.80\n",
    "validation_ratio = 0.10\n",
    "test_ratio = 0.10\n",
    "\n",
    "assert train_ratio + validation_ratio + test_ratio == 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "labels = {\n",
    "\t0: \"Basophil\",\n",
    "\t1: \"Eosinophil\",\n",
    "\t2: \"Erythroblast\",\n",
    "\t3: \"Immature granulocytes\",\n",
    "\t4: \"Lymphocyte\",\n",
    "\t5: \"Monocyte\",\n",
    "\t6: \"Neutrophil\",\n",
    "\t7: \"Platelet\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLaoDaG1V1Yg"
   },
   "outputs": [],
   "source": [
    "data = np.load(DATASET)\n",
    "X = data['images']\n",
    "y = data['labels']\n",
    "\n",
    "X = (X).astype('float32')\n",
    "\n",
    "print('Before data points filter shape:', X.shape, y.shape)\n",
    "\n",
    "with open(OUTLIERS, 'r') as file:\n",
    "\tblacklist = json.load(file)\n",
    "blacklist = sorted(blacklist['blacklist'])\n",
    "X = np.delete(X, blacklist, axis=0)\n",
    "y = np.delete(y, blacklist, axis=0)\n",
    "\n",
    "print('After data points filter shape:', X.shape, y.shape)\n",
    "\n",
    "# Percentages taken from:  https://arxiv.org/pdf/2110.09508\n",
    "train_size = int(X.shape[0] * train_ratio)\n",
    "val_size = int(X.shape[0] * validation_ratio)\n",
    "test_size = X.shape[0] - train_size - val_size\n",
    "\n",
    "if not USE_CLASS_WEIGHTS:\n",
    "\t# Convert to one hot encoding\n",
    "\ty = tfk.utils.to_categorical(y)\n",
    "\n",
    "\tif test_size >= len(labels):\n",
    "\t\tX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed, test_size=test_size, stratify=y)\n",
    "\t\tX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=seed, test_size=val_size, stratify=y_train)\n",
    "\t\tprint(X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape)\n",
    "\telse:\n",
    "\t\tX_train, X_val, y_train, y_val = train_test_split(X, y, random_state=seed, test_size=val_size, stratify=y)\n",
    "\t\tprint(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "else:\n",
    "\tif test_size >= len(labels):\n",
    "\t\tX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed, test_size=test_size, stratify=y)\n",
    "\t\tX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=seed, test_size=val_size, stratify=y_train)\n",
    "\telse:\n",
    "\t\tX_train, X_val, y_train, y_val = train_test_split(X, y, random_state=seed, test_size=val_size, stratify=y)\n",
    "\t\tprint(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "\t# Copy for later\n",
    "\ty_train_cat = y_train\n",
    "\t# Convert to one hot encoding\n",
    "\ty_train = tfk.utils.to_categorical(y_train)\n",
    "\ty_val = tfk.utils.to_categorical(y_val)\n",
    "\tif test_size >= len(labels):\n",
    "\t\ty_test = tfk.utils.to_categorical(y_test)\n",
    "\t\tprint(X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape)\n",
    "\telse:\n",
    "\t\tprint(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect data\n",
    "# Display a sample of images from the training-validation dataset\n",
    "num_img = 10\n",
    "random_indices = random.sample(range(len(X_val)), num_img)\n",
    "\n",
    "fig, axes = plt.subplots(1, num_img, figsize=(20, 20))\n",
    "\n",
    "def get_label(y):\n",
    "    index = np.where(y == 1)[0]\n",
    "    return labels[int(index)]\n",
    "\n",
    "# Iterate through the selected number of images\n",
    "for i, idx in enumerate(random_indices):\n",
    "    ax = axes[i % num_img]\n",
    "    ax.imshow(np.squeeze(X_val[idx] / 255), vmin=0., vmax=1.)\n",
    "    ax.set_title(get_label(y_val[idx]))\n",
    "    ax.axis('off')\n",
    "\n",
    "# Adjust layout and display the images\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåä Generate class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CLASS_WEIGHTS:\n",
    "    # Flat the train labels\n",
    "    y_train_cat_flat = np.ravel(y_train_cat)\n",
    "\n",
    "    # Make weights proportional to class imbalance\n",
    "\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced', \n",
    "        classes=np.unique(data['labels']), \n",
    "        y=y_train_cat_flat\n",
    "    )\n",
    "\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    from pprint import pprint\n",
    "    print('Class weights:')\n",
    "    pprint(class_weight_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSliIxBvbs2Q"
   },
   "source": [
    "## üõ†Ô∏è Train and Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training setup\n",
    "epochs = 400\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer setup\n",
    "lr = 1e-3\n",
    "fine_tuning_lr = 1e-4\n",
    "# One of:\n",
    "# SGD\n",
    "# Adam\n",
    "# AdamW\n",
    "# Lion\n",
    "# Ranger\n",
    "opt_name = \"AdamW\"\n",
    "fine_tuning_opt_name = \"AdamW\"\n",
    "\n",
    "opt_exp_decay_rate: float | None = None\n",
    "# Decay at how many epochs\n",
    "opt_decay_epoch_delta = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define dense params\n",
    "dropouts_layers = [0.4]\n",
    "# Note, the base model outputs a size which is different based on the model being used, hence make attention on the first dense size\n",
    "dense_layers = [8]\n",
    "\n",
    "# Example for more layers:\n",
    "#dropouts_layers = [0.5, 0.3, 0.4]\n",
    "#dense_layers = [256, 64, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fitting callbacks. Comment out from dict the unwanted ones\n",
    "model_fit_callbacks = {\n",
    "\t#'ReduceLROnPlateau': tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6, verbose=1),\n",
    "\t'EarlyStopping': tfk.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to free or not the memory\n",
    "FREE_MODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model name being used. One of:\n",
    "# vgg19\n",
    "# vgg16\n",
    "# efficientnetv2-b0\n",
    "# efficientnetv2-b3\n",
    "# efficientnetv2-s\n",
    "# efficientnetv2-m\n",
    "# efficientnetv2-l\n",
    "# They all use global average pooling\n",
    "base_model_name = 'efficientnetv2-l'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define if to load a trained classier based on the same base model\n",
    "LOAD_TRAINED_CLASSIFIER = False\n",
    "trained_classifier_model_file = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers to fine tune. Use `all` to enable all layers.\n",
    "ACTIVATION_POLICY = 'prefix' #or 'same'\n",
    "based_model_layer_blocks_to_activate = set([\n",
    "\t# Excluding the first two blocks\n",
    "\t'block3',\n",
    "\t'block4',\n",
    "\t'block5',\n",
    "\t'block6',\n",
    "\t'block7',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ox9jqYyyUJo0"
   },
   "outputs": [],
   "source": [
    "# This will download every weight. If you have issues (e.g. on Kaggle) please delete the unwanted dict entries\n",
    "base_model_dict = {\n",
    "  'vgg19': tfk.applications.VGG19(\n",
    "      include_top=False,\n",
    "      input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "      input_tensor=None,\n",
    "      pooling='avg',\n",
    "      weights=\"imagenet\",\n",
    "  ),\n",
    "  'vgg16': tfk.applications.VGG16(\n",
    "      include_top=False,\n",
    "      input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "      input_tensor=None,\n",
    "      pooling='avg',\n",
    "      weights=\"imagenet\",\n",
    "  ),\n",
    "  'efficientnetv2-l': tfk.applications.EfficientNetV2L(\n",
    "    include_preprocessing=True,\n",
    "    include_top=False,\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    input_tensor=None,\n",
    "    pooling='avg',\n",
    "    weights=\"imagenet\",\n",
    "  ),\n",
    "  'efficientnetv2-s': tfk.applications.EfficientNetV2S(\n",
    "    include_preprocessing=True,\n",
    "    include_top=False,\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    input_tensor=None,\n",
    "    pooling='avg',\n",
    "    weights=\"imagenet\",\n",
    "  ),\n",
    "  'efficientnetv2-m': tfk.applications.EfficientNetV2M(\n",
    "    include_preprocessing=True,\n",
    "    include_top=False,\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    input_tensor=None,\n",
    "    pooling='avg',\n",
    "    weights=\"imagenet\",\n",
    "  ),\n",
    "  'efficientnetv2-b0': tfk.applications.EfficientNetV2B0(\n",
    "    include_preprocessing=True,\n",
    "    include_top=False,\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    input_tensor=None,\n",
    "    pooling='avg',\n",
    "    weights=\"imagenet\",\n",
    "  ),\n",
    "  'efficientnetv2-b3': tfk.applications.EfficientNetV2B3(\n",
    "    include_preprocessing=True,\n",
    "    include_top=False,\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    input_tensor=None,\n",
    "    pooling='avg',\n",
    "    weights=\"imagenet\",\n",
    "  ),\n",
    "\n",
    "}\n",
    "\n",
    "def get_base_model():\n",
    "  if LOAD_TRAINED_CLASSIFIER:\n",
    "    return None\n",
    "  # Initialise imageNet model with pretrained weights, for transfer learning\n",
    "  assert(base_model_name in base_model_dict)\n",
    "  m = base_model_dict[base_model_name]\n",
    "\n",
    "  # Freeze\n",
    "  m.trainable = False\n",
    "  return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your manual augmentation. This work on single and np array of images but attention on the random selection below (written for single inputs in this case)\n",
    "# This fn has to applied to the input images before injecting X_train to the network.\n",
    "def augment_image(input):\n",
    "  def apply(image):\n",
    "    import random\n",
    "    # Normalize\n",
    "    image = image / 255\n",
    "\n",
    "    # Random flip\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "\n",
    "    ## Random zoom by cropping\n",
    "    image = tf.image.central_crop(image, random.uniform(0.70, 0.9999))\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    ## Brightness and contrast adjustment\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
    "\n",
    "    ## Hue and saturation adjustment (only for RGB images)\n",
    "    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n",
    "    image = tf.image.random_hue(image, max_delta=0.1)\n",
    "\n",
    "    ## Gaussian noise\n",
    "    noise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=0.05)\n",
    "    image = tf.add(image, noise)\n",
    "\n",
    "    ## Clip values to keep them in [0, 1]\n",
    "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "\n",
    "    # Rescale back\n",
    "    image = image * 255\n",
    "    return image\n",
    "\n",
    "  # Randomly decide whether to augment\n",
    "  should_augment = tf.random.uniform([], 0, 1) > 0.5  # 50% chance\n",
    "  return tf.cond(\n",
    "      should_augment,\n",
    "      lambda: apply(input),  # If True, apply augmentation\n",
    "      lambda: input  # If False, return the image unchanged\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess batch images\n",
    "def preprocess_images(images, labels):\n",
    "  augmented = []\n",
    "  for i in images:\n",
    "    image = augment_image(i)\n",
    "    augmented.append(image)\n",
    "  return np.array(augmented), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test augmentation\n",
    "rows = 2\n",
    "cols = 2\n",
    "num_img = rows * cols\n",
    "partialX = X_train[:num_img]\n",
    "partialY = y_train[:num_img]\n",
    "\n",
    "augX, augY = preprocess_images(partialX, partialY)\n",
    "\n",
    "# Inspect data\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(20, 8))\n",
    "print(axes.shape)\n",
    "\n",
    "def get_label(y):\n",
    "  index = np.where(y == 1)[0]\n",
    "  return labels[int(index)]\n",
    "\n",
    "# Iterate through the selected number of images\n",
    "for i in range(num_img):\n",
    "  ax = axes[i // cols, i % cols]\n",
    "  ax.imshow(np.squeeze(augX[i] / 255), vmin=0., vmax=1.)\n",
    "  ax.set_title(get_label(augY[i]))\n",
    "  ax.axis('off')\n",
    "\n",
    "# Adjust layout and display the images\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General augmentations with Keras layers\n",
    "def build_augmentation(name = 'preprocessing'):\n",
    "\taugmentation = tf.keras.Sequential([\n",
    "\t\ttfkl.RandomRotation(0.167),                              # Rotate images randomly by ¬±60% of a full rotation\n",
    "\t\ttfkl.RandomTranslation(0.1, 0.1),                        # Randomly translate images by ¬±10% in x and y\n",
    "\t], name=name)\n",
    "\treturn augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation_fn_for_augmentation_generator_cutmix(images, labels, fractionCM = 0.90, fractionAUG = 0.01):\n",
    "  import keras_cv\n",
    "  cutmix_layer = keras_cv.layers.CutMix(alpha=1.0, seed=seed)\n",
    "  shuffled_indices = tf.random.shuffle(tf.range(len(labels)))\n",
    "\n",
    "  # Split the batch\n",
    "  cutmix_size = int(len(labels) * fractionCM)\n",
    "  aug_size = int(len(labels) * fractionAUG)\n",
    "  \n",
    "  cutmix_indices = shuffled_indices[:cutmix_size]\n",
    "  aug_indices = shuffled_indices[cutmix_size+1:cutmix_size+aug_size]\n",
    "  clean_indices = shuffled_indices[cutmix_size+aug_size+1:]\n",
    "\n",
    "  # Gather images and labels based on the shuffled indices\n",
    "  images_cutmix = tf.gather(images, cutmix_indices)\n",
    "  labels_cutmix = tf.gather(labels, cutmix_indices)\n",
    "\n",
    "  images_aug = tf.gather(images, aug_indices)\n",
    "  labels_aug = tf.gather(labels, aug_indices)\n",
    "\n",
    "  images_clean = tf.gather(images, clean_indices)\n",
    "  labels_clean = tf.gather(labels, clean_indices)\n",
    "  \n",
    "  # Apply augmentations\n",
    "  cutmix = cutmix_layer({\"images\": images_cutmix, \"labels\": labels_cutmix})\n",
    "  augmented = build_augmentation()(images_aug)\n",
    "\n",
    "  # Concatenate the CutMix and non-CutMix parts back together\n",
    "  images_combined = tf.concat([cutmix[\"images\"], images_clean, augmented, images_cutmix], axis=0)\n",
    "  labels_combined = tf.concat([cutmix[\"labels\"], labels_clean, labels_aug, labels_cutmix], axis=0)\n",
    "  \n",
    "  \n",
    "  shuffled_indices = tf.random.shuffle(tf.range(len(labels_combined)))\n",
    "  images_combined = tf.gather(images_combined, cutmix_indices)\n",
    "  labels_combined = tf.gather(labels_combined, cutmix_indices)\n",
    "  \n",
    "  return images_combined, labels_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform augmentation on X_train and send a batch of augmented images to the network. Using this tf structure as this is directly compatible with the `fit` method\n",
    "class AugmentationGeneratorCutMix(tf.keras.utils.Sequence):\n",
    "  def __init__(self, images, labels, batch_size, aug_fn, fractionCM=0.30, fractionAUG=0.30):\n",
    "    self.images = images\n",
    "    self.labels = labels\n",
    "    self.batch_size = batch_size\n",
    "    self.fractionCM = fractionCM\n",
    "    self.fractionAUG = fractionAUG\n",
    "    self.aug_fn = aug_fn\n",
    "\n",
    "  def __len__(self):\n",
    "    # Number of batches / epoch\n",
    "    return int(np.ceil(len(self.images) / self.batch_size))\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # Get a batch of images\n",
    "    batch_images = self.images[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "    batch_labels = self.labels[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "\n",
    "    # Apply aug\n",
    "    augmented_images, augmented_labels = self.aug_fn(batch_images, batch_labels, \n",
    "                    self.fractionCM, self.fractionAUG)\n",
    "    # Show example\n",
    "    #num_images = 10\n",
    "    #plt.figure(figsize=(15, 5))\n",
    "    #for i in range(min(num_images, len(augmented_images))):\n",
    "    #  plt.subplot(1, num_images, i + 1)\n",
    "    #  plt.imshow((augmented_images[i]).numpy().astype(np.uint8))\n",
    "    #  plt.axis('off')\n",
    "    #  plt.show()\n",
    "\n",
    "    return augmented_images, augmented_labels\n",
    "\n",
    "def get_AugmentationGeneratorCutMix():\n",
    "  global y_train\n",
    "  y_train = tf.convert_to_tensor(y_train, dtype=tf.float32) \n",
    "  train_generator = AugmentationGeneratorCutMix(X_train, y_train, batch_size, augmentation_fn_for_augmentation_generator_cutmix, fractionCM=0.90, fractionAUG=0.01)\n",
    "  return train_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This augmentation pipeline will double the inputs (X) and apply the augmix on half of them (the returned dataset will have both augmented and not augmented data shuffled).\n",
    "def get_AugmentationGeneratorAugMix(X, y, batch):\n",
    "  import keras_cv\n",
    "  augmix = keras_cv.layers.AugMix([0, 255])\n",
    "\n",
    "  assert(isinstance(X, np.ndarray))\n",
    "  assert(isinstance(y, np.ndarray))\n",
    "\n",
    "  # Define a keras_cv augmentation pipeline\n",
    "  augmentations = keras_cv.layers.Augmenter([\n",
    "      augmix\n",
    "  ])\n",
    "\n",
    "  # Apply augmentations only to X\n",
    "  def augment(features, labels):\n",
    "    features = augmentations(features)  # Apply keras_cv transformations to features\n",
    "    return features, labels  # Return features and labels unchanged\n",
    "\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "  # Apply the mapping function to the dataset\n",
    "  dataset_aug = dataset.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "  # Concat augmented with original\n",
    "  dataset_aug = dataset_aug.concatenate(tf.data.Dataset.from_tensor_slices((X, y)))\n",
    "\n",
    "  # Shuffle, batch, and optionally preprocess\n",
    "  dataset_aug = (dataset_aug\n",
    "             .shuffle(buffer_size=X.shape[0] * 2)   # Shuffle the entire dataset\n",
    "             .batch(batch)                  # Create batches\n",
    "             .prefetch(tf.data.AUTOTUNE))  \n",
    "  return dataset_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the augmix augmentation\n",
    "N = 5\n",
    "test_augmix = get_AugmentationGeneratorAugMix(X_train[:N], y_train[:N], N*2)\n",
    "\n",
    "# Take one batch from the dataset\n",
    "for batch in test_augmix.take(1):\n",
    "  X_batch, y_batch = batch  # Features and labels\n",
    "\n",
    "  # Inspect data\n",
    "  fig, axes = plt.subplots(1, N*2, figsize=(20, 8))\n",
    "  # Iterate through the selected number of images\n",
    "  for i in range(N*2):\n",
    "    ax = axes[i%(N*2)]\n",
    "    ax.imshow(np.squeeze(X_batch[i].numpy() / 255), vmin=0., vmax=1.)\n",
    "    ax.axis('off')\n",
    "\n",
    "  # Adjust layout and display the images\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "  break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default we build a new model with the given base model\n",
    "# If the flag LOAD_TRAINED_CLASSIFIER is True, we load the model and load the base_model from the loaded model, hence any base_model being passed is ignored\n",
    "def build_model(base_model = None, restore_base = True, out_shape = y_train.shape[-1], trained_classifier_model_name = 'vgg19'):\n",
    "\tassert(len(dropouts_layers) == len(dense_layers))\n",
    "\tassert(dense_layers[-1] == len(labels))\n",
    "\n",
    "\tif LOAD_TRAINED_CLASSIFIER:\n",
    "\t\tassert(trained_classifier_model_file != '')\n",
    "\t\tm = tf.keras.models.load_model(trained_classifier_model_file)\n",
    "\n",
    "\t\t# Extract the base model\n",
    "\t\tif restore_base:\n",
    "\t\t\tbase_model = m.get_layer(trained_classifier_model_name)\n",
    "\t\t\tfor l in base_model.layers:\n",
    "\t\t\t\tl.trainable = False\n",
    "\t\t\treturn m\n",
    "\n",
    "\tif restore_base and base_model is not None:\n",
    "\t\tfor l in base_model.layers:\n",
    "\t\t\tl.trainable = False\n",
    "\n",
    "\tinputs = tfk.Input(shape=X_train[0].shape, name='input_layer')\n",
    "\t# Define augmentation layers\n",
    "\taugmentation = build_augmentation()\n",
    "\t# Define network\n",
    "\tx = augmentation(inputs)\n",
    "\tif base_model is not None:\n",
    "\t\tx = base_model(x)\n",
    "\tfor i, (drop, dense) in enumerate(zip(dropouts_layers, dense_layers)):\n",
    "\t\tx = tfkl.Dropout(drop, name=f'dropout{i}')(x)\n",
    "\t\t# Skip last dense as it's the output\n",
    "\t\tif i == len(dropouts_layers)-1:\n",
    "\t\t\tbreak\n",
    "\t\tx = tfkl.Dense(dense, activation='relu', name=f'dense{i}')(x)\n",
    "\toutputs = tfkl.Dense(dense_layers[-1], activation='softmax', name=f'dense{len(dense_layers)-1}')(x)\n",
    "\n",
    "\t# Define the complete model linking input and output\n",
    "\tm = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
    "\treturn m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks():\n",
    "\treturn [i for i in model_fit_callbacks.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, data_loader = None):\n",
    "\tif data_loader is None:\n",
    "\t\tif USE_CLASS_WEIGHTS:\n",
    "\t\t\tprint('Fitting with class weights!')\n",
    "\t\t\tfit_history = model.fit(\n",
    "\t      x=X_train,\n",
    "\t      y=y_train,\n",
    "\t      batch_size=batch_size,\n",
    "\t      epochs=epochs,\n",
    "\t      validation_data=(X_val, y_val),\n",
    "\t      class_weight = class_weight_dict,\n",
    "\t      callbacks=get_callbacks()\n",
    "\t    ).history\n",
    "\t\telse:\n",
    "\t\t\tfit_history = model.fit(\n",
    "\t      x=X_train,\n",
    "\t      y=y_train,\n",
    "\t      batch_size=batch_size,\n",
    "\t      epochs=epochs,\n",
    "\t      validation_data=(X_val, y_val),\n",
    "\t      callbacks=get_callbacks()\n",
    "\t    ).history\n",
    "\t\treturn fit_history\n",
    "\telse:\n",
    "\t\tif USE_CLASS_WEIGHTS:\n",
    "\t\t\tprint('Fitting with class weights!')\n",
    "\t\t\tfit_history = model.fit(\n",
    "        data_loader,\n",
    "\t      batch_size=batch_size,\n",
    "\t      epochs=epochs,\n",
    "\t      validation_data=(X_val, y_val),\n",
    "\t      class_weight = class_weight_dict,\n",
    "\t      callbacks=get_callbacks()\n",
    "\t    ).history\n",
    "\t\telse:\n",
    "\t\t\tfit_history = model.fit(\n",
    "        data_loader,\n",
    "\t      batch_size=batch_size,\n",
    "\t      epochs=epochs,\n",
    "\t      validation_data=(X_val, y_val),\n",
    "\t      callbacks=get_callbacks()\n",
    "\t    ).history\n",
    "\t\treturn fit_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_feature_extractor_layers(extractor, layers):\n",
    "  extractor.trainable = True\n",
    "  for i, layer in enumerate(extractor.layers):\n",
    "    layer.trainable = False\n",
    "\t# Set the based_model_layer_blocks_to_activate layers as trainable\n",
    "  if 'all' in layers:\n",
    "    for i, layer in enumerate(extractor.layers):\n",
    "      layer.trainable = True\n",
    "  else:\n",
    "    if ACTIVATION_POLICY == 'same':\n",
    "      for i, layer in enumerate(extractor.layers):\n",
    "        if layer.name in layers:\n",
    "          layer.trainable = True\n",
    "    elif ACTIVATION_POLICY == 'prefix':\n",
    "      for i, layer in enumerate(extractor.layers):\n",
    "        block = layer.name[:6]\n",
    "        if block in layers:\n",
    "          layer.trainable = True\n",
    "\n",
    "\t# Print layer indices, names, and trainability status\n",
    "  print('\\n\\nBase model training configuration:')\n",
    "  for i, layer in enumerate(extractor.layers):\n",
    "\t  print(i, layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://github.com/SeanSdahl/RangerOptimizerTensorflow/blob/master/module.py\n",
    "def build_ranger(lr=1e-3, weight_decay=0.0):\n",
    "  try:\n",
    "    import tensorflow_addons as tfa\n",
    "  except:\n",
    "    raise Exception(\"You have to install tensorflow_addons package for Ranger. Please note that this package is available up to tensorflow==2.14\")\n",
    "  def ranger(sync_period=6,\n",
    "           slow_step_size=0.5,\n",
    "           learning_rate=lr,\n",
    "           beta_1=0.9,\n",
    "           beta_2=0.999,\n",
    "           epsilon=1e-7,\n",
    "           weight_decay=weight_decay,\n",
    "           amsgrad=False,\n",
    "           sma_threshold=5.0,\n",
    "           total_steps=0,\n",
    "           warmup_proportion=0.1,\n",
    "           min_lr=0.,\n",
    "           name=\"Ranger\"):\n",
    "    inner = tfa.optimizers.RectifiedAdam(learning_rate, beta_1, beta_2, epsilon, weight_decay, amsgrad, sma_threshold, total_steps, warmup_proportion, min_lr, name)\n",
    "    optim = tfa.optimizers.Lookahead(inner, sync_period, slow_step_size, name)\n",
    "    return optim\n",
    "  return ranger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(is_fine_tuning = False, use_decay_fine_tuning = False, **kwargs):\n",
    "\tdecay = opt_exp_decay_rate\n",
    "\tif is_fine_tuning and not use_decay_fine_tuning:\n",
    "\t\tdecay = None\n",
    "\n",
    "\topt = opt_name if not is_fine_tuning else fine_tuning_opt_name\n",
    "\n",
    "\tif opt == \"SGD\":\n",
    "\t\toptimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9 if 'momentum' not in kwargs else kwargs['momentum'])\n",
    "\t\tif decay is not None:\n",
    "\t\t\tlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "\t\t\t\t\tinitial_learning_rate=fine_tuning_lr if is_fine_tuning else lr,\n",
    "\t\t\t\t\tdecay_steps=opt_decay_epoch_delta * (X_train.shape[0] // batch_size),  # Decay every 7 epochs\n",
    "\t\t\t\t\tdecay_rate=opt_exp_decay_rate,\n",
    "\t\t\t\t\tstaircase=True\n",
    "\t\t\t)\n",
    "\t\t\toptimizer.learning_rate = lr_schedule\n",
    "\t\t\tprint(f'\\n\\n{\"Finetuning: \" if is_fine_tuning else \"NotFinetuning: \"}using {opt} optimizer with exp decay {decay} (momentum = {optimizer.momentum})\\n\\n')\n",
    "\t\t\treturn optimizer\n",
    "\t\telse:\n",
    "\t\t\toptimizer.learning_rate = fine_tuning_lr if is_fine_tuning else lr\n",
    "\t\t\tprint(f'\\n\\n{\"Finetuning: \" if is_fine_tuning else \"NotFinetuning: \"}using {opt} optimizer (momentum = {optimizer.momentum})\\n\\n')\n",
    "\t\t\treturn optimizer\n",
    "\n",
    "\telif opt == \"Adam\":\n",
    "\t\tif 'weight_decay' in kwargs:\n",
    "\t\t\toptimizer = tf.keras.optimizers.Adam(weight_decay=kwargs['weight_decay'])\n",
    "\t\telse:\n",
    "\t\t\toptimizer = tf.keras.optimizers.Adam()\n",
    "\t\tif decay is not None:\n",
    "\t\t\tlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "\t\t\t\t\tinitial_learning_rate=fine_tuning_lr if is_fine_tuning else lr,\n",
    "\t\t\t\t\tdecay_steps=opt_decay_epoch_delta * (X_train.shape[0] // batch_size),  # Decay every 7 epochs\n",
    "\t\t\t\t\tdecay_rate=opt_exp_decay_rate,\n",
    "\t\t\t\t\tstaircase=True\n",
    "\t\t\t)\n",
    "\t\t\toptimizer.learning_rate = lr_schedule\n",
    "\t\t\tprint(f'\\n\\n{\"Finetuning: \" if is_fine_tuning else \"NotFinetuning: \"}using {opt} optimizer with exp decay of {decay} weight decay = {optimizer.weight_decay}\\n\\n')\n",
    "\t\t\treturn optimizer\n",
    "\t\telse:\n",
    "\t\t\toptimizer.learning_rate = fine_tuning_lr if is_fine_tuning else lr\n",
    "\t\t\tprint(f'\\n\\n{\"Finetuning: \" if is_fine_tuning else \"NotFinetuning: \"}using {opt} optimizer (weight decay = {optimizer.weight_decay})\\n\\n')\n",
    "\t\t\treturn optimizer\n",
    "\n",
    "\telif opt == \"AdamW\":\n",
    "\t\tif 'weight_decay' in kwargs:\n",
    "\t\t\toptimizer = tf.keras.optimizers.AdamW(weight_decay=kwargs['weight_decay'])\n",
    "\t\telse:\n",
    "\t\t\toptimizer = tf.keras.optimizers.AdamW()\n",
    "\t\tif decay is not None:\n",
    "\t\t\tlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "\t\t\t\t\tinitial_learning_rate=fine_tuning_lr if is_fine_tuning else lr,\n",
    "\t\t\t\t\tdecay_steps=opt_decay_epoch_delta * (X_train.shape[0] // batch_size),  # Decay every 7 epochs\n",
    "\t\t\t\t\tdecay_rate=opt_exp_decay_rate,\n",
    "\t\t\t\t\tstaircase=True\n",
    "\t\t\t)\n",
    "\t\t\toptimizer.learning_rate = lr_schedule\n",
    "\t\t\tprint(f'\\n\\n{\"Finetuning: \" if is_fine_tuning else \"NotFinetuning: \"}using {opt} optimizer with exp decay of {decay} weight decay = {optimizer.weight_decay}\\n\\n')\n",
    "\t\t\treturn optimizer\n",
    "\t\telse:\n",
    "\t\t\toptimizer.learning_rate = fine_tuning_lr if is_fine_tuning else lr\n",
    "\t\t\tprint(f'\\n\\n{\"Finetuning: \" if is_fine_tuning else \"NotFinetuning: \"}using {opt} optimizer (weight decay = {optimizer.weight_decay})\\n\\n')\n",
    "\t\t\treturn optimizer\n",
    "\n",
    "\telif opt == \"Lion\":\n",
    "\t\tif 'weight_decay' in kwargs:\n",
    "\t\t\toptimizer = tf.keras.optimizers.Lion(weight_decay=kwargs['weight_decay'])\n",
    "\t\telse:\n",
    "\t\t\toptimizer = tf.keras.optimizers.Lion()\n",
    "\t\tif decay is not None:\n",
    "\t\t\tlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "\t\t\t\t\tinitial_learning_rate=fine_tuning_lr if is_fine_tuning else lr,\n",
    "\t\t\t\t\tdecay_steps=opt_decay_epoch_delta * (X_train.shape[0] // batch_size),  # Decay every 7 epochs\n",
    "\t\t\t\t\tdecay_rate=opt_exp_decay_rate,\n",
    "\t\t\t\t\tstaircase=True\n",
    "\t\t\t)\n",
    "\t\t\toptimizer.learning_rate = lr_schedule\n",
    "\t\t\tprint(f'\\n\\n{\"Finetuning: \" if is_fine_tuning else \"NotFinetuning: \"}using {opt} optimizer with exp decay of {decay} weight decay = {optimizer.weight_decay}\\n\\n')\n",
    "\t\t\treturn optimizer\n",
    "\t\telse:\n",
    "\t\t\toptimizer.learning_rate = fine_tuning_lr if is_fine_tuning else lr\n",
    "\t\t\tprint(f'\\n\\n{\"Finetuning: \" if is_fine_tuning else \"NotFinetuning: \"}using {opt} optimizer (weight decay = {optimizer.weight_decay})\\n\\n')\n",
    "\t\t\treturn optimizer\n",
    "\telif opt == \"Ranger\":\n",
    "\t\toptimizer = build_ranger(lr=lr if not is_fine_tuning else fine_tuning_lr, weight_decay=0.0 if 'weight_decay' not in kwargs else kwargs['weight_decay'])\n",
    "\t\tif decay is not None:\n",
    "\t\t\traise RuntimeError(\"Not supported\")\n",
    "\t\telse:\n",
    "\t\t\toptimizer.learning_rate = fine_tuning_lr if is_fine_tuning else lr\n",
    "\t\t\tprint(f'\\n\\n{\"Finetuning: \" if is_fine_tuning else \"NotFinetuning: \"}using {opt} optimizer\\n\\n')\n",
    "\t\t\treturn optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_model(model):\n",
    "\t# Display a summary of the model architecture\n",
    "\tmodel.summary(expand_nested=True)\n",
    "\n",
    "\t# Display model architecture with layer shapes and trainable parameters\n",
    "\ttfk.utils.plot_model(model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Augment images before training. Use this for custom augmentation which cannot fit inside a data loader nor a Keras Sequence\n",
    "'''\n",
    "# Use custom augmentation\n",
    "#X_train, y_train = preprocess_images(X_train, y_train)\n",
    "\n",
    "'''\n",
    "Classifier training\n",
    "'''\n",
    "model = build_model(base_model=get_base_model())\n",
    "base_model = model.get_layer(base_model_name)\n",
    "\n",
    "if not LOAD_TRAINED_CLASSIFIER:\n",
    "  model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=get_optimizer(is_fine_tuning=False), metrics=['accuracy'])\n",
    "  #display_model(model)\n",
    "  \n",
    "  # Fit the initial model\n",
    "  print('\\n\\nFitting classifier\\n\\n')\n",
    "  # Pass a data loader if we want to pass a loader object which applied some aug (e.g. cutmix)\n",
    "  class_fit_history = fit_model(model, data_loader=get_AugmentationGeneratorAugMix(X_train, y_train, batch_size))\n",
    "  \n",
    "  intermediate_val_acc = round(max(class_fit_history['val_accuracy']) * 100, 2)\n",
    "  # Save intermediate model\n",
    "  model_filename = f'{base_model_name}-intermediateDONOTUSE-finetuned{len(based_model_layer_blocks_to_activate) if \"all\" not in based_model_layer_blocks_to_activate else \"all\"}blocks-{str(intermediate_val_acc)}-{datetime.now().strftime(\"%y%m%d_%H%M\")}.keras'\n",
    "  model.save(model_filename)\n",
    "\n",
    "'''\n",
    "Fine tuning\n",
    "'''\n",
    "# Enable fine tuning\n",
    "enable_feature_extractor_layers(base_model, based_model_layer_blocks_to_activate)\n",
    "\n",
    "model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=get_optimizer(is_fine_tuning=True), metrics=['accuracy'])\n",
    "#display_model(model)\n",
    "\n",
    "# Fit the initial finetuned model\n",
    "print('\\n\\nFine tuning\\n\\n')\n",
    "# Pass a data loader if we want to pass a loader object which applied some aug (e.g. cutmix)\n",
    "fine_tuning_fit_history = fit_model(model, data_loader=get_AugmentationGeneratorAugMix(X_train, y_train, batch_size))\n",
    "\n",
    "# Calculate and print the best validation accuracy achieved\n",
    "final_val_accuracy = round(max(fine_tuning_fit_history['val_accuracy']) * 100, 2)\n",
    "print(f'Final validation accuracy: {final_val_accuracy}%')\n",
    "\n",
    "# Save the trained model to a file, including final accuracy in the filename\n",
    "model_filename = f'{base_model_name}-finetuned{len(based_model_layer_blocks_to_activate) if \"all\" not in based_model_layer_blocks_to_activate else \"all\"}blocks-{str(final_val_accuracy)}-{datetime.now().strftime(\"%y%m%d_%H%M\")}.keras'\n",
    "model.save(model_filename)\n",
    "\n",
    "# Free memory by deleting the model instance\n",
    "if FREE_MODEL:\n",
    "  del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trainig(fit):\n",
    "\t# Create figure and subplots for loss and accuracy\n",
    "\tfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 6))\n",
    "\n",
    "\t# Plot loss for both re-trained and transfer learning models\n",
    "\tax1.plot(fit['loss'], alpha=0.3, color='#00008b', label='training loss', linestyle='--')\n",
    "\tax1.plot(fit['val_loss'], label='validation loss', alpha=0.8, color='#ffa500')\n",
    "\tax1.set_title('Categorical Crossentropy')\n",
    "\tax1.legend(loc='upper left')\n",
    "\tax1.grid(alpha=0.3)\n",
    "\n",
    "\t# Plot accuracy for both re-trained and transfer learning models\n",
    "\tax2.plot(fit['accuracy'], alpha=0.3, color='#00008b', label='training accuracy', linestyle='--')\n",
    "\tax2.plot(fit['val_accuracy'], label='validation accuracy', alpha=0.8, color='#ffa500')\n",
    "\tax2.set_title('Accuracy')\n",
    "\tax2.legend(loc='upper left')\n",
    "\tax2.grid(alpha=0.3)\n",
    "\n",
    "\t# Adjust layout to prevent label overlap and display the plots\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()\n",
    "\n",
    "#plot_trainig(class_fit_history)\n",
    "plot_trainig(fine_tuning_fit_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëî Load a trained model (if needed!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = tf.keras.models.load_model('vgg19-finetuned16layers-99.13-241114_1414.kera#s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úçüèø Make evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##loss, acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "##print('Model, accuracy: {:5.2f}%'.format(100 * acc))\n",
    "\n",
    "# Predict labels for the entire test set\n",
    "predictions = model.predict(X_test, verbose=0)\n",
    "\n",
    "# Display the shape of the predictions\n",
    "print(\"Predictions Shape:\", predictions.shape)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "pred_classes = np.argmax(predictions, axis=-1)\n",
    "\n",
    "# Extract ground truth classes\n",
    "true_classes = np.argmax(y_test, axis=-1)\n",
    "\n",
    "# Calculate and display test set accuracy as percentage\n",
    "accuracy = accuracy_score(true_classes, pred_classes)\n",
    "print(f'Accuracy score over the test set: {round(100 * accuracy, 2)}%')\n",
    "\n",
    "# Calculate and display test set precision as percentage\n",
    "precision = precision_score(true_classes, pred_classes, average='weighted')\n",
    "print(f'Precision score over the test set: {round(100 * precision, 2)}%')\n",
    "\n",
    "# Calculate and display test set recall as percentage\n",
    "recall = recall_score(true_classes, pred_classes, average='weighted')\n",
    "print(f'Recall score over the test set: {round(100 * recall, 2)}%')\n",
    "\n",
    "# Calculate and display test set F1 score as percentage\n",
    "f1 = f1_score(true_classes, pred_classes, average='weighted')\n",
    "print(f'F1 score over the test set: {round(100 * f1, 2)}%')\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(true_classes, pred_classes)\n",
    "\n",
    "# Calculate the percentages for each element in the confusion matrix\n",
    "cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "# Combine numbers and percentages into a single string for annotation\n",
    "annot = np.array([f\"{num}\\n({percent:.2f}%)\" for num, percent in zip(cm.flatten(), cm_percentage.flatten())]).reshape(cm.shape)\n",
    "\n",
    "# Plot the confusion matrix with percentages\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_percentage.T, annot=annot, fmt='', xticklabels=list(labels.values()), yticklabels=list(labels.values()), cmap='Blues')\n",
    "plt.xlabel('True labels')\n",
    "plt.ylabel('Predicted labels')\n",
    "plt.title('Confusion Matrix (Percentages)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNp6pUZuddqC"
   },
   "source": [
    "## üìä Prepare Your Submission\n",
    "\n",
    "To prepare your submission, create a `.zip` file that includes all the necessary code to run your model. It **must** include a `model.py` file with the following class:\n",
    "\n",
    "```python\n",
    "# file: model.py\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the internal state of the model.\"\"\"\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return a numpy array with the labels corresponding to the input X.\"\"\"\n",
    "```\n",
    "\n",
    "The next cell shows an example implementation of the `model.py` file, which includes loading model weights from the `weights.keras` file and conducting predictions on provided input data. The `.zip` file is created and downloaded in the last notebook cell.\n",
    "\n",
    "‚ùó Feel free to modify the method implementations to better fit your specific requirements, but please ensure that the class name and method interfaces remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKT4h-9xYwiT"
   },
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the internal state of the model. Note that the __init__\n",
    "        method cannot accept any arguments.\n",
    "\n",
    "        The following is an example loading the weights of a pre-trained\n",
    "        model.\n",
    "        \"\"\"\n",
    "        self.neural_network = tfk.models.load_model('vgg19-finetuned14layers-97.84-241115_0029.keras')\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the labels corresponding to the input X. Note that X is a numpy\n",
    "        array of shape (n_samples, 96, 96, 3) and the output should be a numpy\n",
    "        array of shape (n_samples,). Therefore, outputs must no be one-hot\n",
    "        encoded.\n",
    "\n",
    "        The following is an example of a prediction from the pre-trained model\n",
    "        loaded in the __init__ method.\n",
    "        \"\"\"\n",
    "        preds = self.neural_network.predict(X)\n",
    "        if len(preds.shape) == 2:\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s18kX1uDconq"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "filename = f'submission_{datetime.now().strftime(\"%y%m%d_%H%M%S\")}.zip'\n",
    "\n",
    "# Add files to the zip command if needed\n",
    "!zip {filename} model.py vgg19-finetuned14layers-97.84-241115_0029.keras"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPhN8z97sycURDAjAFsp+EI",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "anndl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
