{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuwVgG3Vbbka"
   },
   "source": [
    "# Artificial Neural Networks and Deep Learning\n",
    "\n",
    "---\n",
    "## Homework 2: Image segmentation of Mars' stones\n",
    "## Team: The Backpropagators\n",
    "Arianna Procaccio, Francesco Buccoliero, Kai-Xi Matteo Chen, Luca Capoferri\n",
    "\n",
    "ariii, frbuccoliero, kaiximatteoc, luke01\n",
    "\n",
    "246843, 245498, 245523, 259617\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7IqZP5Iblna"
   },
   "source": [
    "## ⚙️ Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T20:59:48.162042Z",
     "iopub.status.busy": "2024-11-21T20:59:48.161808Z",
     "iopub.status.idle": "2024-11-21T20:59:48.172447Z",
     "shell.execute_reply": "2024-11-21T20:59:48.171556Z",
     "shell.execute_reply.started": "2024-11-21T20:59:48.162018Z"
    },
    "id": "CO6_Ft_8T56A",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "#from sklearn.utils.class_weight import compute_class_weight\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T20:59:48.175109Z",
     "iopub.status.busy": "2024-11-21T20:59:48.174259Z",
     "iopub.status.idle": "2024-11-21T20:59:48.181658Z",
     "shell.execute_reply": "2024-11-21T20:59:48.180934Z",
     "shell.execute_reply.started": "2024-11-21T20:59:48.175068Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GN_cpHlSboXV"
   },
   "source": [
    "## ⏳ Load, inspect and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T20:59:48.205584Z",
     "iopub.status.busy": "2024-11-21T20:59:48.205315Z",
     "iopub.status.idle": "2024-11-21T20:59:48.214590Z",
     "shell.execute_reply": "2024-11-21T20:59:48.213686Z",
     "shell.execute_reply.started": "2024-11-21T20:59:48.205560Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = \"dataset.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T20:59:48.215707Z",
     "iopub.status.busy": "2024-11-21T20:59:48.215465Z",
     "iopub.status.idle": "2024-11-21T20:59:48.226436Z",
     "shell.execute_reply": "2024-11-21T20:59:48.225658Z",
     "shell.execute_reply.started": "2024-11-21T20:59:48.215684Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Although we have the test set provided, we don't know their true mask. Reserve a small subset for a known test set to make inference later\n",
    "train_ratio = 0.85\n",
    "validation_ratio = 0.10\n",
    "test_ratio = 0.05\n",
    "\n",
    "assert train_ratio + validation_ratio + test_ratio == 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(DATASET_PATH)\n",
    "\n",
    "training_set = data[\"training_set\"]\n",
    "X_train = training_set[:, 0]\n",
    "y_train = training_set[:, 1]\n",
    "\n",
    "hidden_X_test = data[\"test_set\"]\n",
    "\n",
    "print(f\"Training X shape: {X_train.shape}\")\n",
    "print(f\"Training y shape: {y_train.shape}\")\n",
    "print(f\"Test hidden X shape: {hidden_X_test.shape}\")\n",
    "\n",
    "# Add color channel and rescale pixels between 0 and 1\n",
    "X_train = X_train[..., np.newaxis] / 255\n",
    "hidden_X_test = hidden_X_test[..., np.newaxis] / 255\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and validation\n",
    "validation_size = int(X_train.shape[0] * validation_ratio)\n",
    "\n",
    "indices = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "X_train = X_train[indices]\n",
    "y_train = y_train[indices]\n",
    "\n",
    "# Define train and validation indices\n",
    "split_indices = [int(X_train.shape[0] * train_ratio), int(X_train.shape[0] * (train_ratio + validation_ratio))]\n",
    "\n",
    "X_train, X_val, X_test = np.split(X_train, split_indices)\n",
    "y_train, y_val, y_test = np.split(y_train, split_indices)\n",
    "\n",
    "print(f\"Training X shape: {X_train.shape}\")\n",
    "print(f\"Training y shape: {y_train.shape}\")\n",
    "print(f\"Validation X shape: {X_val.shape}\")\n",
    "print(f\"Validation y shape: {y_val.shape}\")\n",
    "print(f\"Test X shape: {X_test.shape}\")\n",
    "print(f\"Test y shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data. The number of images being displayed are rows X cols\n",
    "def plot(data, mask=None, num_images=10, rows=4, cols=8):\n",
    "  # Reshape if needed (e.g., remove channel dimension for grayscale images)\n",
    "  if data.shape[-1] == 1:  # Grayscale case\n",
    "    data = data.squeeze(axis=-1)  # Remove channel dimension\n",
    "  \n",
    "  if mask is None:\n",
    "    # Plot settings\n",
    "    _, axes = plt.subplots(rows, cols, figsize=(12, 6))  # Adjust figure size as needed\n",
    "  \n",
    "    # Display images\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "      if i < len(data):  # Check if there are enough images\n",
    "        ax.imshow(data[i], cmap='gray' if len(data[i].shape) == 2 else None)\n",
    "        ax.axis('off')  # Hide axes\n",
    "      else:\n",
    "        ax.axis('off')  # Hide any empty subplot\n",
    "  \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "  else:\n",
    "    num_samples = num_images  # Number of images to display\n",
    "    if num_samples < 4:\n",
    "      num_samples = 4\n",
    "\n",
    "    # Plot settings\n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(8, num_samples * 2))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "      # Original image\n",
    "      axes[i, 0].imshow(data[i], cmap=\"gray\")\n",
    "      axes[i, 0].set_title(f\"Image {i+1}\")\n",
    "      axes[i, 0].axis(\"off\")\n",
    "\n",
    "      # Corresponding mask\n",
    "      axes[i, 1].imshow(mask[i], cmap=\"viridis\", alpha=0.8)  # Adjust cmap as needed\n",
    "      axes[i, 1].set_title(f\"Mask {i+1}\")\n",
    "      axes[i, 1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers share the mask\n",
    "outlier_mask_template = np.load(\"outlier_mask.npy\") # discovered by hand\n",
    "train_outliers_indices = [i for i, img in enumerate(y_train) if not np.array_equal(img, outlier_mask_template)]\n",
    "val_outliers_indices = [i for i, img in enumerate(y_val) if not np.array_equal(img, outlier_mask_template)]\n",
    "test_outliers_indices = [i for i, img in enumerate(y_test) if not np.array_equal(img, outlier_mask_template)]\n",
    "print(f'Total outliers in train set: {y_train.shape[0] - len(train_outliers_indices)}')\n",
    "print(f'Total outliers in validation set: {y_val.shape[0] - len(val_outliers_indices)}')\n",
    "print(f'Total outliers in test set: {y_test.shape[0] - len(test_outliers_indices)}')\n",
    "\n",
    "# Remove outlier from train and validation set\n",
    "X_train = X_train[train_outliers_indices]\n",
    "y_train = y_train[train_outliers_indices]\n",
    "X_val = X_val[val_outliers_indices]\n",
    "y_val = y_val[val_outliers_indices]\n",
    "X_test = X_test[test_outliers_indices]\n",
    "y_test = y_test[test_outliers_indices]\n",
    "\n",
    "print(f'Updated train dataset size: {X_train.shape}')\n",
    "print(f'Updated validation dataset size: {X_val.shape}')\n",
    "print(f'Updated test dataset size: {X_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(X_train, rows=10, cols=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An additional check: you should not see any outlier\n",
    "plot(X_train, mask=y_train, num_images=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image size for the network (dataset size is 64 X 128) and num of classes\n",
    "IMG_SIZE = (64, 128)\n",
    "NUM_CLASSES = num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `concat_and_shuffle_aug_with_no_aug` will double the X_train size\n",
    "def get_dataset(X, y, batch_size=32, augmentations=None, **kwargs):\n",
    "  def resize_img_and_mask(img, mask):\n",
    "    input_img = tf.image.resize(img, IMG_SIZE)\n",
    "    input_img = tf.cast(input_img, tf.float32)\n",
    "\n",
    "    # Resize needs at least 3 dims, add a dummy one\n",
    "    target_img = tf.expand_dims(mask, axis=-1)\n",
    "    # Nearest-neighbor is essential for resizing segmentation masks because it preserves the discrete class labels (e.g., 0, 1, 2) without introducing unintended values due to interpolation\n",
    "    target_img = tf.image.resize(target_img, IMG_SIZE, method=\"nearest\")\n",
    "    target_img = tf.cast(target_img, tf.int32) # Consider lower integers\n",
    "\n",
    "    return input_img, target_img\n",
    "\n",
    "  # Apply augmentations before converting to dataset (this will be serial I think but we avoid type conversions as A works on np arrays)\n",
    "  if augmentations is not None:\n",
    "    X_a = []\n",
    "    y_a = []\n",
    "    for a, b in zip(X, y):\n",
    "      aug_a, aug_b = augmentations(a, b)\n",
    "      X_a.append(aug_a)  \n",
    "      y_a.append(aug_b)  \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((np.array(X_a), np.array(y_a)))\n",
    "    if kwargs.get('concat_and_shuffle_aug_with_no_aug', False):\n",
    "      dataset = dataset.concatenate(tf.data.Dataset.from_tensor_slices((X, y)))\n",
    "      dataset = dataset.shuffle(seed=seed, buffer_size=X.shape[0] * 2)\n",
    "\n",
    "  else:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "\n",
    "  dataset = dataset.map(resize_img_and_mask, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "  dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model name\n",
    "model_name = 'U_NET'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T20:59:52.969085Z",
     "iopub.status.busy": "2024-11-21T20:59:52.968811Z",
     "iopub.status.idle": "2024-11-21T20:59:52.978445Z",
     "shell.execute_reply": "2024-11-21T20:59:52.977502Z",
     "shell.execute_reply.started": "2024-11-21T20:59:52.969030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define training setup\n",
    "epochs = 1000\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T20:59:52.980424Z",
     "iopub.status.busy": "2024-11-21T20:59:52.980176Z",
     "iopub.status.idle": "2024-11-21T20:59:52.990553Z",
     "shell.execute_reply": "2024-11-21T20:59:52.989845Z",
     "shell.execute_reply.started": "2024-11-21T20:59:52.980400Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define optimizer setup\n",
    "lr = 1e-4\n",
    "fine_tuning_lr = 1e-4\n",
    "# One of:\n",
    "# SGD\n",
    "# Adam\n",
    "# AdamW\n",
    "# Lion\n",
    "# Ranger\n",
    "opt_name = \"AdamW\"\n",
    "fine_tuning_opt_name = \"AdamW\"\n",
    "\n",
    "opt_exp_decay_rate: float | None = None\n",
    "# Decay at how many epochs\n",
    "opt_decay_epoch_delta = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sparse_categorical_crossentropy when your labels are integers representing class indices\n",
    "loss_fn = 'sparse_categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T20:59:53.000513Z",
     "iopub.status.busy": "2024-11-21T20:59:53.000251Z",
     "iopub.status.idle": "2024-11-21T20:59:53.010025Z",
     "shell.execute_reply": "2024-11-21T20:59:53.009423Z",
     "shell.execute_reply.started": "2024-11-21T20:59:53.000489Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define fitting callbacks. Comment out from dict the unwanted ones\n",
    "model_fit_callbacks = {\n",
    "\t#'ReduceLROnPlateau': tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6, verbose=1),\n",
    "\t'EarlyStopping': tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=15, restore_best_weights=True, verbose=1) #https://colab.research.google.com/drive/15h-47mevDv3hFXq5LUBh3XxyTTf9ZDx8#scrollTo=H6J65MMp4pA8&line=4&uniqifier=1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T20:59:53.011207Z",
     "iopub.status.busy": "2024-11-21T20:59:53.010956Z",
     "iopub.status.idle": "2024-11-21T20:59:53.022969Z",
     "shell.execute_reply": "2024-11-21T20:59:53.022102Z",
     "shell.execute_reply.started": "2024-11-21T20:59:53.011183Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# just to free or not the memory\n",
    "FREE_MODEL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ Define model, augmentation and utils builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:00:06.456714Z",
     "iopub.status.busy": "2024-11-21T21:00:06.456469Z",
     "iopub.status.idle": "2024-11-21T21:00:06.461596Z",
     "shell.execute_reply": "2024-11-21T21:00:06.460618Z",
     "shell.execute_reply.started": "2024-11-21T21:00:06.456690Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_augmentation():\n",
    "  transform = A.Compose([\n",
    "      #A.GridElasticDeform(num_grid_xy=(4, 4), magnitude=10, p=0.5),\n",
    "      A.GridElasticDeform(num_grid_xy=(8, 8), magnitude=10),\n",
    "      A.XYMasking(),\n",
    "      A.ShiftScaleRotate()\n",
    "  ])\n",
    "  return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_augmentation(img, mask):\n",
    "  transform = build_augmentation()\n",
    "  transformed = transform(image=img, mask=mask)\n",
    "  return transformed[\"image\"], transformed[\"mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the augmented dataset\n",
    "N = 5\n",
    "ds = get_dataset(X_train[:N], y_train[:N], augmentations=apply_augmentation, concat_and_shuffle_aug_with_no_aug=True)\n",
    "\n",
    "for batch in ds.take(1):\n",
    "  a, b = batch\n",
    "  plot(a.numpy(), b.numpy(), num_images=N * 2) # use N * 2 as `concat_and_shuffle_aug_with_no_aug` is True \n",
    "  break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:00:15.913190Z",
     "iopub.status.busy": "2024-11-21T21:00:15.912842Z",
     "iopub.status.idle": "2024-11-21T21:00:15.923585Z",
     "shell.execute_reply": "2024-11-21T21:00:15.922710Z",
     "shell.execute_reply.started": "2024-11-21T21:00:15.913155Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# taken from https://keras.io/examples/vision/oxford_pets_image_segmentation/\n",
    "def build_U_NET_XCEPTION(img_size: tuple[int, int, int], num_classes):\n",
    "  inputs = tfk.Input(shape=img_size) # One channel input\n",
    "\n",
    "  ### [First half of the network: downsampling inputs] ###\n",
    "\n",
    "  # Entry block\n",
    "  x = tfkl.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
    "  x = tfkl.BatchNormalization()(x)\n",
    "  x = tfkl.Activation(\"relu\")(x)\n",
    "\n",
    "  previous_block_activation = x  # Set aside residual\n",
    "\n",
    "  # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
    "  for filters in [64, 128, 256]:\n",
    "    x = tfkl.Activation(\"relu\")(x)\n",
    "    x = tfkl.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "\n",
    "    x = tfkl.Activation(\"relu\")(x)\n",
    "    x = tfkl.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "\n",
    "    x = tfkl.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    # Project residual\n",
    "    residual = tfkl.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
    "    previous_block_activation\n",
    "    )\n",
    "    x = tfkl.add([x, residual])  # Add back residual\n",
    "    previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "  ### [Second half of the network: upsampling inputs] ###\n",
    "\n",
    "  for filters in [256, 128, 64, 32]:\n",
    "    x = tfkl.Activation(\"relu\")(x)\n",
    "    x = tfkl.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "\n",
    "    x = tfkl.Activation(\"relu\")(x)\n",
    "    x = tfkl.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "\n",
    "    x = tfkl.UpSampling2D(2)(x)\n",
    "\n",
    "    # Project residual\n",
    "    residual = tfkl.UpSampling2D(2)(previous_block_activation)\n",
    "    residual = tfkl.Conv2D(filters, 1, padding=\"same\")(residual)\n",
    "    x = tfkl.add([x, residual])  # Add back residual\n",
    "    previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "  # Add a per-pixel classification layer\n",
    "  outputs = tfkl.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
    "\n",
    "  # Define the model\n",
    "  model = tfk.Model(inputs, outputs, name='UNetXception')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_U_NET(img_size: tuple[int, int, int], num_classes):\n",
    "  def unet_block(input_tensor, filters, kernel_size=3, activation='relu', stack=2, name=''):\n",
    "    # Initialise the input tensor\n",
    "    x = input_tensor\n",
    "\n",
    "    # Apply a sequence of Conv2D, Batch Normalisation, and Activation layers for the specified number of stacks\n",
    "    for i in range(stack):\n",
    "        x = tfkl.Conv2D(filters, kernel_size=kernel_size, padding='same', name=name + 'conv' + str(i + 1))(x)\n",
    "        x = tfkl.BatchNormalization(name=name + 'bn' + str(i + 1))(x)\n",
    "        x = tfkl.Activation(activation, name=name + 'activation' + str(i + 1))(x)\n",
    "\n",
    "    # Return the transformed tensor\n",
    "    return x\n",
    "\n",
    "  input_layer = tfkl.Input(shape=img_size, name='input_layer')\n",
    "\n",
    "  # Downsampling path\n",
    "  down_block_1 = unet_block(input_layer, 32, name='down_block1_')\n",
    "  d1 = tfkl.MaxPooling2D()(down_block_1)\n",
    "\n",
    "  down_block_2 = unet_block(d1, 64, name='down_block2_')\n",
    "  d2 = tfkl.MaxPooling2D()(down_block_2)\n",
    "\n",
    "  # Bottleneck\n",
    "  bottleneck = unet_block(d2, 128, name='bottleneck')\n",
    "\n",
    "  # Upsampling path\n",
    "  u1 = tfkl.UpSampling2D()(bottleneck)\n",
    "  u1 = tfkl.Concatenate()([u1, down_block_2])\n",
    "  u1 = unet_block(u1, 64, name='up_block1_')\n",
    "\n",
    "  u2 = tfkl.UpSampling2D()(u1)\n",
    "  u2 = tfkl.Concatenate()([u2, down_block_1])\n",
    "  u2 = unet_block(u2, 32, name='up_block2_')\n",
    "\n",
    "  # Output Layer\n",
    "  output_layer = tfkl.Conv2D(num_classes, kernel_size=1, padding='same', activation=\"softmax\", name='output_layer')(u2)\n",
    "\n",
    "  model = tf.keras.Model(inputs=input_layer, outputs=output_layer, name='UNet')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "\t'U_NET': build_U_NET,\n",
    "\t'U_NET_XCEPTIO': build_U_NET_XCEPTION\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:00:15.925010Z",
     "iopub.status.busy": "2024-11-21T21:00:15.924678Z",
     "iopub.status.idle": "2024-11-21T21:00:15.939252Z",
     "shell.execute_reply": "2024-11-21T21:00:15.938433Z",
     "shell.execute_reply.started": "2024-11-21T21:00:15.924970Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_callbacks():\n",
    "\treturn [i for i in model_fit_callbacks.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:00:15.940614Z",
     "iopub.status.busy": "2024-11-21T21:00:15.940360Z",
     "iopub.status.idle": "2024-11-21T21:00:15.949708Z",
     "shell.execute_reply": "2024-11-21T21:00:15.948895Z",
     "shell.execute_reply.started": "2024-11-21T21:00:15.940588Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fit_model(model, data_loader=None, validation_data_loader=None):\n",
    "  assert(data_loader is not None)\n",
    "  assert(validation_data_loader is not None)\n",
    "  fit_history = model.fit(\n",
    "        data_loader,\n",
    "\t      epochs=epochs,\n",
    "        validation_data=validation_data_loader,\n",
    "\t      callbacks=get_callbacks()\n",
    "\t    ).history\n",
    "  return fit_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:00:15.964995Z",
     "iopub.status.busy": "2024-11-21T21:00:15.964769Z",
     "iopub.status.idle": "2024-11-21T21:00:15.978997Z",
     "shell.execute_reply": "2024-11-21T21:00:15.978372Z",
     "shell.execute_reply.started": "2024-11-21T21:00:15.964972Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Taken from https://github.com/SeanSdahl/RangerOptimizerTensorflow/blob/master/module.py\n",
    "def build_ranger(lr=1e-3, weight_decay=0.0):\n",
    "  try:\n",
    "    import tensorflow_addons as tfa\n",
    "  except:\n",
    "    raise Exception(\"You have to install tensorflow_addons package for Ranger. Please note that this package is available up to tensorflow==2.14\")\n",
    "  def ranger(sync_period=6,\n",
    "           slow_step_size=0.5,\n",
    "           learning_rate=lr,\n",
    "           beta_1=0.9,\n",
    "           beta_2=0.999,\n",
    "           epsilon=1e-7,\n",
    "           weight_decay=weight_decay,\n",
    "           amsgrad=False,\n",
    "           sma_threshold=5.0,\n",
    "           total_steps=0,\n",
    "           warmup_proportion=0.1,\n",
    "           min_lr=0.,\n",
    "           name=\"Ranger\"):\n",
    "    inner = tfa.optimizers.RectifiedAdam(learning_rate, beta_1, beta_2, epsilon, weight_decay, amsgrad, sma_threshold, total_steps, warmup_proportion, min_lr, name)\n",
    "    optim = tfa.optimizers.Lookahead(inner, sync_period, slow_step_size, name)\n",
    "    return optim\n",
    "  return ranger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:00:15.980359Z",
     "iopub.status.busy": "2024-11-21T21:00:15.980111Z",
     "iopub.status.idle": "2024-11-21T21:00:15.995129Z",
     "shell.execute_reply": "2024-11-21T21:00:15.994358Z",
     "shell.execute_reply.started": "2024-11-21T21:00:15.980335Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_optimizer(is_fine_tuning = False, use_decay_fine_tuning = False, **kwargs):\n",
    "\tdecay = opt_exp_decay_rate\n",
    "\tif is_fine_tuning and not use_decay_fine_tuning:\n",
    "\t\tdecay = None\n",
    "\n",
    "\topt = opt_name if not is_fine_tuning else fine_tuning_opt_name\n",
    "\n",
    "\tif opt == \"SGD\":\n",
    "\t\toptimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9 if 'momentum' not in kwargs else kwargs['momentum'])\n",
    "\t\tif decay is not None:\n",
    "\t\t\tlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "\t\t\t\t\tinitial_learning_rate=fine_tuning_lr if is_fine_tuning else lr,\n",
    "\t\t\t\t\tdecay_steps=opt_decay_epoch_delta * (X_train.shape[0] // batch_size),  # Decay every 7 epochs\n",
    "\t\t\t\t\tdecay_rate=opt_exp_decay_rate,\n",
    "\t\t\t\t\tstaircase=True\n",
    "\t\t\t)\n",
    "\t\t\toptimizer.learning_rate = lr_schedule\n",
    "\t\t\tprint(f'\\n\\n{\"Finetuning: \" if is_fine_tuning else \"NotFinetuning: \"}using {opt} optimizer with exp decay {decay} (momentum = {optimizer.momentum})\\n\\n')\n",
    "\t\t\treturn optimizer\n",
    "\t\telse:\n",
    "\t\t\toptimizer.learning_rate = fine_tuning_lr if is_fine_tuning else lr\n",
    "\t\t\tprint(f'\\n\\n{\"Finetuning: \" if is_fine_tuning else \"NotFinetuning: \"}using {opt} optimizer (momentum = {optimizer.momentum})\\n\\n')\n",
    "\t\t\treturn optimizer\n",
    "\n",
    "\telif opt == \"Adam\":\n",
    "\t\tif 'weight_decay' in kwargs:\n",
    "\t\t\toptimizer = tf.keras.optimizers.Adam(weight_decay=kwargs['weight_decay'])\n",
    "\t\telse:\n",
    "\t\t\toptimizer = tf.keras.optimizers.Adam()\n",
    "\t\tif decay is not None:\n",
    "\t\t\tlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "\t\t\t\t\tinitial_learning_rate=fine_tuning_lr if is_fine_tuning else lr,\n",
    "\t\t\t\t\tdecay_steps=opt_decay_epoch_delta * (X_train.shape[0] // batch_size),  # Decay every 7 epochs\n",
    "\t\t\t\t\tdecay_rate=opt_exp_decay_rate,\n",
    "\t\t\t\t\tstaircase=True\n",
    "\t\t\t)\n",
    "\t\t\toptimizer.learning_rate = lr_schedule\n",
    "\t\t\tprint(f'\\n\\n{\"Finetuning: \" if is_fine_tuning else \"NotFinetuning: \"}using {opt} optimizer with exp decay of {decay} weight decay = {optimizer.weight_decay}\\n\\n')\n",
    "\t\t\treturn optimizer\n",
    "\t\telse:\n",
    "\t\t\toptimizer.learning_rate = fine_tuning_lr if is_fine_tuning else lr\n",
    "\t\t\tprint(f'\\n\\n{\"Finetuning: \" if is_fine_tuning else \"NotFinetuning: \"}using {opt} optimizer (weight decay = {optimizer.weight_decay})\\n\\n')\n",
    "\t\t\treturn optimizer\n",
    "\n",
    "\telif opt == \"AdamW\":\n",
    "\t\tif 'weight_decay' in kwargs:\n",
    "\t\t\toptimizer = tf.keras.optimizers.AdamW(weight_decay=kwargs['weight_decay'])\n",
    "\t\telse:\n",
    "\t\t\toptimizer = tf.keras.optimizers.AdamW()\n",
    "\t\tif decay is not None:\n",
    "\t\t\tlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "\t\t\t\t\tinitial_learning_rate=fine_tuning_lr if is_fine_tuning else lr,\n",
    "\t\t\t\t\tdecay_steps=opt_decay_epoch_delta * (X_train.shape[0] // batch_size),  # Decay every 7 epochs\n",
    "\t\t\t\t\tdecay_rate=opt_exp_decay_rate,\n",
    "\t\t\t\t\tstaircase=True\n",
    "\t\t\t)\n",
    "\t\t\toptimizer.learning_rate = lr_schedule\n",
    "\t\t\tprint(f'\\n\\n{\"Finetuning: \" if is_fine_tuning else \"NotFinetuning: \"}using {opt} optimizer with exp decay of {decay} weight decay = {optimizer.weight_decay}\\n\\n')\n",
    "\t\t\treturn optimizer\n",
    "\t\telse:\n",
    "\t\t\toptimizer.learning_rate = fine_tuning_lr if is_fine_tuning else lr\n",
    "\t\t\tprint(f'\\n\\n{\"Finetuning: \" if is_fine_tuning else \"NotFinetuning: \"}using {opt} optimizer (weight decay = {optimizer.weight_decay})\\n\\n')\n",
    "\t\t\treturn optimizer\n",
    "\n",
    "\telif opt == \"Lion\":\n",
    "\t\tif 'weight_decay' in kwargs:\n",
    "\t\t\toptimizer = tf.keras.optimizers.Lion(weight_decay=kwargs['weight_decay'])\n",
    "\t\telse:\n",
    "\t\t\toptimizer = tf.keras.optimizers.Lion()\n",
    "\t\tif decay is not None:\n",
    "\t\t\tlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "\t\t\t\t\tinitial_learning_rate=fine_tuning_lr if is_fine_tuning else lr,\n",
    "\t\t\t\t\tdecay_steps=opt_decay_epoch_delta * (X_train.shape[0] // batch_size),  # Decay every 7 epochs\n",
    "\t\t\t\t\tdecay_rate=opt_exp_decay_rate,\n",
    "\t\t\t\t\tstaircase=True\n",
    "\t\t\t)\n",
    "\t\t\toptimizer.learning_rate = lr_schedule\n",
    "\t\t\tprint(f'\\n\\n{\"Finetuning: \" if is_fine_tuning else \"NotFinetuning: \"}using {opt} optimizer with exp decay of {decay} weight decay = {optimizer.weight_decay}\\n\\n')\n",
    "\t\t\treturn optimizer\n",
    "\t\telse:\n",
    "\t\t\toptimizer.learning_rate = fine_tuning_lr if is_fine_tuning else lr\n",
    "\t\t\tprint(f'\\n\\n{\"Finetuning: \" if is_fine_tuning else \"NotFinetuning: \"}using {opt} optimizer (weight decay = {optimizer.weight_decay})\\n\\n')\n",
    "\t\t\treturn optimizer\n",
    "\telif opt == \"Ranger\":\n",
    "\t\toptimizer = build_ranger(lr=lr if not is_fine_tuning else fine_tuning_lr, weight_decay=0.0 if 'weight_decay' not in kwargs else kwargs['weight_decay'])\n",
    "\t\tif decay is not None:\n",
    "\t\t\traise RuntimeError(\"Not supported\")\n",
    "\t\telse:\n",
    "\t\t\toptimizer.learning_rate = fine_tuning_lr if is_fine_tuning else lr\n",
    "\t\t\tprint(f'\\n\\n{\"Finetuning: \" if is_fine_tuning else \"NotFinetuning: \"}using {opt} optimizer\\n\\n')\n",
    "\t\t\treturn optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:00:15.996454Z",
     "iopub.status.busy": "2024-11-21T21:00:15.996146Z",
     "iopub.status.idle": "2024-11-21T21:00:16.006383Z",
     "shell.execute_reply": "2024-11-21T21:00:16.005722Z",
     "shell.execute_reply.started": "2024-11-21T21:00:15.996418Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def display_model(model):\n",
    "\t# Display a summary of the model architecture\n",
    "\tmodel.summary(expand_nested=True)\n",
    "\n",
    "\t# Display model architecture with layer shapes and trainable parameters\n",
    "\ttfk.utils.plot_model(model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🍣 Define and display model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom Mean Intersection Over Union metric: the competition excludes the background class\n",
    "class MeanIntersectionOverUnion(tf.keras.metrics.MeanIoU):\n",
    "  def __init__(self, num_classes, labels_to_exclude=None, name=\"mean_iou\", dtype=None):\n",
    "    super(MeanIntersectionOverUnion, self).__init__(num_classes=num_classes, name=name, dtype=dtype)\n",
    "    if labels_to_exclude is None:\n",
    "      labels_to_exclude = [0]  # Default to excluding label 0\n",
    "    self.labels_to_exclude = labels_to_exclude\n",
    "\n",
    "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "    # Convert predictions to class labels\n",
    "    y_pred = tf.math.argmax(y_pred, axis=-1)\n",
    "\n",
    "    # Flatten the tensors\n",
    "    y_true = tf.reshape(y_true, [-1])\n",
    "    y_pred = tf.reshape(y_pred, [-1])\n",
    "\n",
    "    # Apply mask to exclude specified labels\n",
    "    for label in self.labels_to_exclude:\n",
    "      mask = tf.not_equal(y_true, label)\n",
    "      y_true = tf.boolean_mask(y_true, mask)\n",
    "      y_pred = tf.boolean_mask(y_pred, mask)\n",
    "\n",
    "    # Update the state\n",
    "    return super().update_state(y_true, y_pred, sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_dict[model_name](IMG_SIZE + (1,), NUM_CLASSES)\n",
    "\n",
    "model.compile(loss=loss_fn, optimizer=get_optimizer(is_fine_tuning=False), metrics=['accuracy', MeanIntersectionOverUnion(num_classes=NUM_CLASSES, labels_to_exclude=[0])])\n",
    "display_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧗🏻‍♂️ Train and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:00:16.008010Z",
     "iopub.status.busy": "2024-11-21T21:00:16.007418Z",
     "iopub.status.idle": "2024-11-22T01:38:32.003037Z",
     "shell.execute_reply": "2024-11-22T01:38:32.001978Z",
     "shell.execute_reply.started": "2024-11-21T21:00:16.007972Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Fit the initial model\n",
    "print('\\n\\nFitting model\\n\\n')\n",
    "fit_history = fit_model(model, data_loader=get_dataset(X_train, y_train, batch_size=batch_size, augmentations=apply_augmentation, concat_and_shuffle_aug_with_no_aug=True), validation_data_loader=get_dataset(X_val, y_val, batch_size=batch_size))\n",
    "\n",
    "# Calculate and print the final validation accuracy\n",
    "final_val_meanIoU = round(max(fit_history['val_mean_iou'])* 100, 2)\n",
    "print(f'Final validation Mean Intersection Over Union: {final_val_meanIoU}%')\n",
    "\n",
    "# Save intermediate model\n",
    "model_filename = f'{model_name}-{str(final_val_meanIoU)}-{datetime.now().strftime(\"%y%m%d_%H%M\")}.keras'\n",
    "model.save(model_filename)\n",
    "\n",
    "# Free memory by deleting the model instance\n",
    "if FREE_MODEL:\n",
    "  del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T01:38:32.004572Z",
     "iopub.status.busy": "2024-11-22T01:38:32.004292Z",
     "iopub.status.idle": "2024-11-22T01:38:32.539916Z",
     "shell.execute_reply": "2024-11-22T01:38:32.538708Z",
     "shell.execute_reply.started": "2024-11-22T01:38:32.004546Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_trainig(fit):\n",
    "  # Plot and display training and validation loss\n",
    "  plt.figure(figsize=(18, 3))\n",
    "  plt.plot(fit['loss'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=2)\n",
    "  plt.plot(fit['val_loss'], label='Validation', alpha=0.9, color='#5a9aa5', linewidth=2)\n",
    "  plt.title('Cross Entropy')\n",
    "  plt.legend()\n",
    "  plt.grid(alpha=0.3)\n",
    "  plt.show()\n",
    "\n",
    "  # Plot and display training and validation accuracy\n",
    "  plt.figure(figsize=(18, 3))\n",
    "  plt.plot(fit['accuracy'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=2)\n",
    "  plt.plot(fit['val_accuracy'], label='Validation', alpha=0.9, color='#5a9aa5', linewidth=2)\n",
    "  plt.title('Accuracy')\n",
    "  plt.legend()\n",
    "  plt.grid(alpha=0.3)\n",
    "  plt.show()\n",
    "\n",
    "  # Plot and display training and validation mean IoU\n",
    "  plt.figure(figsize=(18, 3))\n",
    "  plt.plot(fit['mean_iou'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=2)\n",
    "  plt.plot(fit['val_mean_iou'], label='Validation', alpha=0.9, color='#5a9aa5', linewidth=2)\n",
    "  plt.title('Mean Intersection over Union')\n",
    "  plt.legend()\n",
    "  plt.grid(alpha=0.3)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trainig(fit_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✍🏿 Make inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = get_dataset(X_test, y_test, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load UNet model without compiling\n",
    "model = tfk.models.load_model('UNet_59.26.keras', compile=False)\n",
    "\n",
    "# Compile the model with specified loss, optimizer, and metrics\n",
    "model.compile(\n",
    "    loss=loss_fn,\n",
    "    optimizer=get_optimizer(),\n",
    "    metrics=[\"accuracy\", MeanIntersectionOverUnion(num_classes=NUM_CLASSES, labels_to_exclude=[0])]\n",
    ")\n",
    "\n",
    "display_model(model)\n",
    "\n",
    "# Evaluate the model on the test set and print the results\n",
    "test_loss, test_accuracy, test_mean_iou = model.evaluate(test_dataset, verbose=1)\n",
    "print(f'Test Accuracy: {round(test_accuracy, 4)}')\n",
    "print(f'Test Mean Intersection over Union: {round(test_mean_iou, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_segmentation_colormap(num_classes):\n",
    "  \"\"\"\n",
    "  Create a linear colormap using a predefined palette.\n",
    "  Uses 'viridis' as default because it is perceptually uniform\n",
    "  and works well for colorblindness.\n",
    "  \"\"\"\n",
    "  return plt.cm.viridis(np.linspace(0, 1, num_classes))\n",
    "\n",
    "def apply_colormap(label, colormap=None):\n",
    "  \"\"\"\n",
    "  Apply the colormap to a label.\n",
    "  \"\"\"\n",
    "  # Ensure label is 2D\n",
    "  label = np.squeeze(label)\n",
    "\n",
    "  if colormap is None:\n",
    "    num_classes = len(np.unique(label))\n",
    "    colormap = create_segmentation_colormap(num_classes)\n",
    "\n",
    "  # Apply the colormap\n",
    "  colored = colormap[label.astype(int)]\n",
    "\n",
    "  return colored\n",
    "\n",
    "def plot_triptychs(dataset, model, num_samples=1):\n",
    "  \"\"\"\n",
    "  Plot triptychs (original image, true mask, predicted mask) for samples from a tf.data.Dataset\n",
    "\n",
    "  Parameters:\n",
    "  dataset: tf.data.Dataset - The dataset containing image-label pairs\n",
    "  model: tf.keras.Model - The trained model to generate predictions\n",
    "  num_samples: int - Number of samples to plot\n",
    "  \"\"\"\n",
    "  # Take samples from the dataset\n",
    "  samples = dataset.take(num_samples)\n",
    "\n",
    "  for images, labels in samples:\n",
    "    # If we have a batch, take the first example\n",
    "    if len(images.shape) == 4:  # Batch of images\n",
    "      images = images[0:1]\n",
    "      labels = labels[0:1]\n",
    "\n",
    "    # Generate predictions\n",
    "    pred = model.predict(images, verbose=0)\n",
    "    pred = tf.math.argmax(pred, axis=-1)\n",
    "\n",
    "    # Create colormap based on number of classes in labels\n",
    "    labels_np = labels.numpy()\n",
    "    num_classes = len(np.unique(labels_np))\n",
    "    colormap = create_segmentation_colormap(num_classes)\n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 4))\n",
    "\n",
    "    # Plot original image\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].imshow(images[0])\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Plot original mask\n",
    "    axes[1].set_title(\"Original Mask\")\n",
    "    colored_label = apply_colormap(labels[0], colormap)\n",
    "    axes[1].imshow(colored_label)\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # Plot predicted mask\n",
    "    axes[2].set_title(\"Predicted Mask\")\n",
    "    colored_pred = apply_colormap(pred[0], colormap)\n",
    "    axes[2].imshow(colored_pred)\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_triptychs(test_dataset, model, num_samples=2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPhN8z97sycURDAjAFsp+EI",
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6047840,
     "sourceId": 9855285,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6047865,
     "sourceId": 9855319,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 159775,
     "modelInstanceId": 137060,
     "sourceId": 161170,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 162416,
     "modelInstanceId": 139795,
     "sourceId": 164339,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 164634,
     "modelInstanceId": 142056,
     "sourceId": 166955,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 165351,
     "modelInstanceId": 142773,
     "sourceId": 167821,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 170852,
     "modelInstanceId": 148341,
     "sourceId": 174251,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "anndl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
