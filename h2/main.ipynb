{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuwVgG3Vbbka"
   },
   "source": [
    "# Artificial Neural Networks and Deep Learning\n",
    "\n",
    "---\n",
    "## Homework 2: Image segmentation of Mars' stones\n",
    "## Team: The Backpropagators\n",
    "Arianna Procaccio, Francesco Buccoliero, Kai-Xi Matteo Chen, Luca Capoferri\n",
    "\n",
    "ariii, frbuccoliero, kaiximatteoc, luke01\n",
    "\n",
    "246843, 245498, 245523, 259617\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7IqZP5Iblna"
   },
   "source": [
    "## âš™ï¸ Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T20:59:48.162042Z",
     "iopub.status.busy": "2024-11-21T20:59:48.161808Z",
     "iopub.status.idle": "2024-11-21T20:59:48.172447Z",
     "shell.execute_reply": "2024-11-21T20:59:48.171556Z",
     "shell.execute_reply.started": "2024-11-21T20:59:48.162018Z"
    },
    "id": "CO6_Ft_8T56A",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.ndimage import distance_transform_edt as distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš™ï¸ Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 888\n",
    "\n",
    "train_ratio = 0.90\n",
    "validation_ratio = 0.08\n",
    "test_ratio = 0.02\n",
    "\n",
    "IMG_SIZE = (64, 128)\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "running_on = \"local\" # local | colab | kaggle\n",
    "\n",
    "quick_run = True # If true skips the early plotting stuff\n",
    "\n",
    "model_name = 'U_NET' # U_NET | U_NET_XCEPTION | UWNet | ASPP | ROCKSEG | TURKEYSEG | GROUP_NORM_UNET | LIGHT4MARS\n",
    "\n",
    "# Training schedule\n",
    "training_schedule = [\n",
    "  # E.g. a first run over non-augmented data\n",
    "  {\n",
    "    \"augmentation\" : True,\n",
    "    \"augmentation_repetition\": 1,\n",
    "    \"enlarge_dataset_with_custom_np_ds\": False,\n",
    "    \"include_non_augmented\": True,\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 32,\n",
    "    \"lr\": 1e-4,\n",
    "    \"opt_name\": \"Adam\", # SGD | Adam | AdamW | Lion | Ranger\n",
    "  },\n",
    "]\n",
    "\n",
    "# Exponential decay\n",
    "opt_exp_decay_rate: float | None = None \n",
    "opt_decay_epoch_delta = 7 # Number of epochs between each decay, if above None is not used\n",
    "\n",
    "USE_CLASS_WEIGHTS: bool | list[int,int,int,int,int] = False # If true will use class weights for loss function, if false will use all ones, if list will use that list\n",
    "\n",
    "loss_fn = 'combined_loss' # sparse_categorical_crossentropy | boundary_loss | dice_loss | combined_loss\n",
    "\n",
    "FREE_MODEL = False # If true the model is deleted from memory after being dumped to file\n",
    "\n",
    "model_filename_override = None # If not None will load the model from this file and perform inference\n",
    "\n",
    "DATASET_PATH_LOCAL = \"dataset.npz\"\n",
    "DATASET_PATH_COLAB = \"/content/drive/MyDrive/Colab Notebooks/dataset.npz\"\n",
    "DATASET_PATH_KAGGLE = \"/kaggle/input/dataset-h2/dataset.npz\"\n",
    "\n",
    "OUTLIER_MASK_LOCAL = \"outlier_mask.npy\"\n",
    "OUTLIER_MASK_COLAB = \"/content/drive/MyDrive/Colab Notebooks/outlier_mask.npy\"\n",
    "OUTLIER_MASK_KAGGLE = \"/kaggle/input/dataset-h2/outlier_mask.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define here the Albumentation pipeline to be used for augmentation\n",
    "\n",
    "def build_augmentation():\n",
    "  # For Matteo, do not delete\n",
    "  transform = A.Compose([\n",
    "    A.RandomRotate90(p=0.5),  # Random 90-degree rotation\n",
    "    A.HorizontalFlip(p=0.5),  # Horizontal flip for diverse texture representation\n",
    "    A.VerticalFlip(p=0.5),  # Vertical flip to simulate different orientations\n",
    "    A.Resize(height=IMG_SIZE[0], width=IMG_SIZE[1], p=1),  # Resize for consistent input size\n",
    "  ])\n",
    "  \n",
    "  return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom augmentation for some specific targets (e.g. images with a lot of background)\n",
    "def build_augmentation_bg():\n",
    "  transform = A.Compose([\n",
    "      A.RandomRotate90(p=0.5),  # Random 90-degree rotation\n",
    "      A.HorizontalFlip(p=0.5),  # Horizontal flip for diverse texture representation\n",
    "      A.VerticalFlip(p=0.5),  # Vertical flip to simulate different orientations\n",
    "      A.Resize(height=IMG_SIZE[0], width=IMG_SIZE[1], p=1),  # Resize for consistent input size\n",
    "    ])\n",
    "  return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading configurations based on the given settings\n",
    "assert train_ratio + validation_ratio + test_ratio == 1\n",
    "\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "DATASET_PATH = DATASET_PATH_LOCAL if running_on == \"local\" else DATASET_PATH_COLAB if running_on == \"colab\" else DATASET_PATH_KAGGLE\n",
    "OUTLIER_MASK = OUTLIER_MASK_LOCAL if running_on == \"local\" else OUTLIER_MASK_COLAB if running_on == \"colab\" else OUTLIER_MASK_KAGGLE\n",
    "\n",
    "data = np.load(DATASET_PATH)\n",
    "outlier_mask_template = np.load(OUTLIER_MASK) # discovered by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GN_cpHlSboXV"
   },
   "source": [
    "## â³ Load, inspect and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = data[\"training_set\"]\n",
    "X_train = training_set[:, 0]\n",
    "y_train = training_set[:, 1]\n",
    "\n",
    "hidden_X_test = data[\"test_set\"]\n",
    "\n",
    "print(f\"Training X shape: {X_train.shape}\")\n",
    "print(f\"Training y shape: {y_train.shape}\")\n",
    "print(f\"Test hidden X shape: {hidden_X_test.shape}\")\n",
    "\n",
    "# Add color channel and rescale pixels between 0 and 1\n",
    "X_train = X_train[..., np.newaxis] / 255\n",
    "X_train = X_train.astype(np.float32)\n",
    "hidden_X_test = hidden_X_test[..., np.newaxis] / 255\n",
    "hidden_X_test = hidden_X_test.astype(np.float32)\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Split train and validation\n",
    "validation_size = int(X_train.shape[0] * validation_ratio)\n",
    "\n",
    "indices = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "X_train = X_train[indices]\n",
    "y_train = y_train[indices]\n",
    "\n",
    "# Define train and validation indices\n",
    "split_indices = [int(X_train.shape[0] * train_ratio), int(X_train.shape[0] * (train_ratio + validation_ratio))]\n",
    "\n",
    "X_train, X_val, X_test = np.split(X_train, split_indices)\n",
    "y_train, y_val, y_test = np.split(y_train, split_indices)\n",
    "\n",
    "print(\"======= BEFORE REMOVING OUTLIERS =======\")\n",
    "print(f\"Training X shape: {X_train.shape}\")\n",
    "print(f\"Training y shape: {y_train.shape}\")\n",
    "print(f\"Validation X shape: {X_val.shape}\")\n",
    "print(f\"Validation y shape: {y_val.shape}\")\n",
    "print(f\"Test X shape: {X_test.shape}\")\n",
    "print(f\"Test y shape: {y_test.shape}\")\n",
    "\n",
    "# Outliers share the mask\n",
    "train_outliers_indices = [i for i, img in enumerate(y_train) if not np.array_equal(img, outlier_mask_template)]\n",
    "val_outliers_indices = [i for i, img in enumerate(y_val) if not np.array_equal(img, outlier_mask_template)]\n",
    "test_outliers_indices = [i for i, img in enumerate(y_test) if not np.array_equal(img, outlier_mask_template)]\n",
    "print(f'Total outliers in train set: {y_train.shape[0] - len(train_outliers_indices)}')\n",
    "print(f'Total outliers in validation set: {y_val.shape[0] - len(val_outliers_indices)}')\n",
    "print(f'Total outliers in test set: {y_test.shape[0] - len(test_outliers_indices)}')\n",
    "\n",
    "# Remove outlier from train and validation set\n",
    "X_train = X_train[train_outliers_indices]\n",
    "y_train = y_train[train_outliers_indices]\n",
    "X_val = X_val[val_outliers_indices]\n",
    "y_val = y_val[val_outliers_indices]\n",
    "X_test = X_test[test_outliers_indices]\n",
    "y_test = y_test[test_outliers_indices]\n",
    "\n",
    "print(\"======= AFTER REMOVING OUTLIERS =======\")\n",
    "print(f'Updated train dataset size: {X_train.shape}')\n",
    "print(f'Updated validation dataset size: {X_val.shape}')\n",
    "print(f'Updated test dataset size: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve images with a lot of label labels\n",
    "def retrieve_imgs_of_label(X, y, bg_label_min_percentage=0.9, label=0):\n",
    "  tot_pixels = IMG_SIZE[0] * IMG_SIZE[1]\n",
    "  imgs = []\n",
    "  labels = []\n",
    "  # Count pixels for each class\n",
    "  for label_image, img in zip(y, X):\n",
    "    unique, counts = np.unique(label_image, return_counts=True)\n",
    "    for u, c in zip(unique, counts):\n",
    "      #print(u, c)\n",
    "      #if (int(u) == 0):\n",
    "      #  print('z', c)\n",
    "      if int(u) == label and c / tot_pixels >= bg_label_min_percentage:\n",
    "        imgs.append(img)\n",
    "        labels.append(label_image)\n",
    "  return np.array(imgs), np.array(labels)\n",
    "\n",
    "bg_imgs, bg_labels = retrieve_imgs_of_label(X_train, y_train, bg_label_min_percentage=0.65)\n",
    "print('Retrieved images:', bg_imgs.shape[0], 'with at least 65% of label 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data. The number of images being displayed are rows X cols\n",
    "def plot(data, mask=None, num_images=10, rows=4, cols=8, num_cls=5, colors=None):\n",
    "  # Reshape if needed (e.g., remove channel dimension for grayscale images)\n",
    "  if data.shape[-1] == 1:  # Grayscale case\n",
    "    data = data.squeeze(axis=-1)  # Remove channel dimension\n",
    "  \n",
    "  if mask is None:\n",
    "    # Plot settings\n",
    "    _, axes = plt.subplots(rows, cols, figsize=(12, 6))  # Adjust figure size as needed\n",
    "  \n",
    "    # Display images\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "      if i < len(data):  # Check if there are enough images\n",
    "        ax.imshow(data[i], cmap='gray' if len(data[i].shape) == 2 else None)\n",
    "        ax.axis('off')  # Hide axes\n",
    "      else:\n",
    "        ax.axis('off')  # Hide any empty subplot\n",
    "  \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "  else:\n",
    "\n",
    "    # Define custom colors for each class\n",
    "    import matplotlib.colors as mcolors\n",
    "    if num_cls == 2:\n",
    "      print(1)\n",
    "      class_colors = [\"blue\", \"purple\"] if colors is None else colors\n",
    "    elif num_classes == 5:\n",
    "      print(2)\n",
    "      class_colors = [\"purple\", \"blue\", \"green\", \"orange\", \"yellow\"] if colors is None else colors\n",
    "    else:\n",
    "      raise RuntimeError('Not impl')\n",
    "    cmap = mcolors.ListedColormap(class_colors, N=num_cls)\n",
    "\n",
    "    # Define normalization to map class values to the color map\n",
    "    bounds = np.arange(num_cls + 1) - 0.5  # Create boundaries for each class\n",
    "    norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "    num_samples = num_images  # Number of images to display\n",
    "    if num_samples < 4:\n",
    "      num_samples = 4\n",
    "\n",
    "    # Plot settings\n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(8, num_samples * 2))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "      # Original image\n",
    "      axes[i, 0].imshow(data[i], cmap=\"gray\")\n",
    "      axes[i, 0].set_title(f\"Image {i+1}\")\n",
    "      axes[i, 0].axis(\"off\")\n",
    "\n",
    "      # Corresponding mask\n",
    "      axes[i, 1].imshow(mask[i], cmap=cmap, alpha=0.8)  # Adjust cmap as needed\n",
    "      axes[i, 1].set_title(f\"Mask {i+1}\")\n",
    "      axes[i, 1].axis(\"off\")\n",
    "    # Add a colorbar\n",
    "    cbar = fig.colorbar(\n",
    "      plt.cm.ScalarMappable(cmap=cmap, norm=norm),\n",
    "      ax=axes[:, 1],  # Align the colorbar with the mask columns\n",
    "      orientation=\"vertical\",\n",
    "      fraction=0.02,\n",
    "      pad=0.04\n",
    "    )\n",
    "    cbar.set_ticks(np.arange(num_cls))  # Set tick locations\n",
    "    cbar.set_ticklabels([f\"Class {i}\" for i in range(num_cls)])  # Set tick labels\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not quick_run:\n",
    "  plot(X_train, rows=10, cols=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An additional check: you should not see any outlier\n",
    "if not quick_run:\n",
    "  plot(X_train, mask=y_train, num_images=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_class_distribution(label_dataset, num_classes):\n",
    "  # Initialize counts for each class\n",
    "  class_counts = np.zeros(num_classes)\n",
    "\n",
    "  # Count pixels for each class\n",
    "  for label_image in label_dataset:\n",
    "      unique, counts = np.unique(label_image, return_counts=True)\n",
    "      for u, c in zip(unique, counts):\n",
    "          class_counts[u] += c\n",
    "\n",
    "  # Normalize counts (percentage)\n",
    "  total_pixels = np.sum(class_counts)\n",
    "  class_distribution = class_counts / total_pixels * 100\n",
    "\n",
    "  # Print and visualize\n",
    "  print(\"Class Distribution (% of pixels):\")\n",
    "  for i in range(num_classes):\n",
    "      print(f\"Class {i}: {class_distribution[i]:.2f}%\")\n",
    "  \n",
    "  if not quick_run:\n",
    "    # Plot class distribution\n",
    "    plt.bar(range(num_classes), class_distribution, tick_label=[f\"Class {i}\" for i in range(num_classes)])\n",
    "    plt.xlabel(\"Classes\")\n",
    "    plt.ylabel(\"Percentage of Pixels\")\n",
    "    plt.title(\"Class Distribution\")\n",
    "    plt.show()\n",
    "  \n",
    "  return class_distribution\n",
    "\n",
    "def get_class_weights(class_distribution):\n",
    "  # Convert percentage to class probabilities\n",
    "  class_probabilities = np.array(class_distribution) / 100.0\n",
    "\n",
    "  # Calculate class weights (inverse of class probability)\n",
    "  class_weights = 1.0 / class_probabilities\n",
    "\n",
    "  # Normalize weights (optional, you can skip normalization if desired)\n",
    "  max_weight = np.max(class_weights)\n",
    "  class_weights = class_weights / max_weight  # Normalize to have the maximum weight = 1\n",
    "\n",
    "  return {i:w for i,w in enumerate(class_weights)}\n",
    "\n",
    "# Check for 5 classes (class IDs: 0-4)\n",
    "class_distribution = check_class_distribution([e.astype(np.int8) for e in y_train], num_classes=num_classes)\n",
    "\n",
    "if type(USE_CLASS_WEIGHTS) == bool:\n",
    "  if USE_CLASS_WEIGHTS:\n",
    "    class_weights = get_class_weights(class_distribution)\n",
    "  else:\n",
    "    class_weights = {i: 1.0/num_classes for i in range(num_classes)}\n",
    "else:\n",
    "  class_weights = {i: w for i, w in enumerate(USE_CLASS_WEIGHTS)}\n",
    "\n",
    "print('Class weights:', class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `concat_and_shuffle_aug_with_no_aug` will double the X_train size\n",
    "# `remove_bg` will set all the bg pixels to dark\n",
    "# `augmentation_repetition` will concatenate n times the augmented dataset by applying the same `augmentations` fn. Useful for augmentation pipeline with probability activations\n",
    "def get_dataset(X, y, batch_size=32, augmentations=None, augmentation_repetition=1, **kwargs):\n",
    "\n",
    "  print(X[0].shape, y[0].shape)\n",
    "\n",
    "  def resize_img_and_mask(img, mask):\n",
    "    input_img = tf.image.resize(img, IMG_SIZE)\n",
    "    input_img = tf.cast(input_img, tf.float32)\n",
    "\n",
    "    # Resize needs at least 3 dims, add a dummy one\n",
    "    if kwargs.get('one_hot', False):\n",
    "      mask = tf.cast(mask, tf.int32)\n",
    "      target_img = tf.one_hot(mask, depth=num_classes, axis=-1)\n",
    "    else:\n",
    "      target_img = tf.expand_dims(mask, axis=-1)\n",
    "    # Nearest-neighbor is essential for resizing segmentation masks because it preserves the discrete class labels (e.g., 0, 1, 2) without introducing unintended values due to interpolation\n",
    "    target_img = tf.image.resize(target_img, IMG_SIZE, method=\"nearest\")\n",
    "    target_img = tf.cast(target_img, tf.int32) # Consider lower integers\n",
    "\n",
    "    return input_img, target_img\n",
    "\n",
    "  def remove_background(image, mask, background_label=0):\n",
    "    background_mask = (mask == background_label)\n",
    "    image[background_mask] = 0  # Set to black\n",
    "    return image, mask\n",
    "\n",
    "  def apply_augmentation_np():\n",
    "    X_a = []\n",
    "    y_a = []\n",
    "    for i, m in zip(X, y):\n",
    "      aug_img, aug_mask = augmentations(i, m)\n",
    "      if kwargs.get('remove_bg', False):\n",
    "        aug_img, aug_mask = remove_background(aug_img, aug_mask)\n",
    "      X_a.append(aug_img)  \n",
    "      y_a.append(aug_mask)  \n",
    "    return np.array(X_a), np.array(y_a)\n",
    "  \n",
    "  if kwargs.get('remove_bg', False):\n",
    "    X_a = []\n",
    "    y_a = []\n",
    "    for i, m in zip(X, y):\n",
    "      aug_img, aug_mask = remove_background(i, m)\n",
    "      X_a.append(aug_img)\n",
    "      y_a.append(aug_mask)\n",
    "    X = np.array(X_a)\n",
    "    y = np.array(y_a)\n",
    "\n",
    "  # Apply augmentations before converting to dataset (this will be serial I think but we avoid type conversions as A works on np arrays)\n",
    "  if augmentations is not None:\n",
    "    X_a, y_a = apply_augmentation_np()\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_a, y_a))\n",
    "    if augmentation_repetition > 1:\n",
    "      for i in range(augmentation_repetition-1):\n",
    "        X_a, y_a = apply_augmentation_np()\n",
    "        dataset = dataset.concatenate(tf.data.Dataset.from_tensor_slices((X_a, y_a)))\n",
    "    add_len = 0\n",
    "    if kwargs.get('additional_ds_concat', None):\n",
    "      # Optimistic\n",
    "      add_len = len(kwargs['additional_ds_concat'])\n",
    "      for pair in kwargs['additional_ds_concat']:\n",
    "        print('concatenating additional ds')\n",
    "        images, labels = pair\n",
    "        images = images.astype(np.float32)\n",
    "        dataset = dataset.concatenate(tf.data.Dataset.from_tensor_slices((images, labels)))\n",
    "    if kwargs.get('concat_and_shuffle_aug_with_no_aug', False):\n",
    "      dataset = dataset.concatenate(tf.data.Dataset.from_tensor_slices((X, y)))\n",
    "      dataset = dataset.shuffle(seed=seed, buffer_size=X.shape[0] * (augmentation_repetition+1+add_len))\n",
    "    else:\n",
    "      dataset = dataset.shuffle(seed=seed, buffer_size=X.shape[0] * (augmentation_repetition+add_len))\n",
    "\n",
    "  else:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    add_len = 0\n",
    "    if kwargs.get('additional_ds_concat', None):\n",
    "      # Optimistic\n",
    "      add_len = len(kwargs['additional_ds_concat'])\n",
    "      for pair in kwargs['additional_ds_concat']:\n",
    "        print('concatenating additional ds')\n",
    "        images, labels = pair\n",
    "        images = images.astype(np.float32)\n",
    "        dataset = dataset.concatenate(tf.data.Dataset.from_tensor_slices((images, labels)))\n",
    "    dataset = dataset.shuffle(seed=seed, buffer_size=X.shape[0] * (add_len+1))\n",
    "\n",
    "  dataset = dataset.map(resize_img_and_mask, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "  dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enlarge dataset containing images provided in the inputs\n",
    "def get_enlarged_dataset(imgs, labels, aug_fn, repetitions=2, unbatch=True, np_ds=True):\n",
    "  def apply_aug(img, mask):\n",
    "    transform = aug_fn()\n",
    "    transformed = transform(image=img, mask=mask)\n",
    "    return transformed[\"image\"], transformed[\"mask\"]\n",
    "  if np_ds:\n",
    "    X_a = []\n",
    "    y_a = []\n",
    "    for _ in range(repetitions):\n",
    "      for i, m in zip(imgs, labels):\n",
    "        aug_img, aug_mask = apply_aug(i, m)\n",
    "        X_a.append(aug_img)  \n",
    "        y_a.append(aug_mask)  \n",
    "    return np.array(X_a), np.array(y_a)\n",
    "  else:\n",
    "    enlarged_bg_dataset = get_dataset(imgs, labels, augmentations=apply_aug, augmentation_repetition=repetitions)\n",
    "    # We unbatch as this will be concatenated with other ds\n",
    "    if unbatch:\n",
    "      ds = enlarged_bg_dataset.unbatch()\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the enlarged bg images dataset as example\n",
    "if not quick_run:\n",
    "  N = 10\n",
    "  a, b =  get_enlarged_dataset(bg_imgs[:N], bg_labels[:N], build_augmentation_bg, np_ds=True)\n",
    "  print(a.shape, b.shape)\n",
    "  plot(a, mask=b, num_images=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸŽ² Define training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_signed_distance_map(mask):\n",
    "  \n",
    "  # Calcola la distanza esterna (distanza dai bordi del foreground)\n",
    "  inv_mask = 1 - mask\n",
    "  dist_out = tf.cast(tf.nn.relu(tf.image.sobel_edges(inv_mask)), tf.float32)\n",
    "\n",
    "  # Calcola la distanza interna (distanza dai bordi del background)\n",
    "  dist_in = tf.cast(tf.nn.relu(tf.image.sobel_edges(mask)), tf.float32)\n",
    "\n",
    "  # Combina le distanze in un unico gradiente\n",
    "  dx_out, dy_out = dist_out[..., 0], dist_out[..., 1]\n",
    "  dx_in, dy_in = dist_in[..., 0], dist_in[..., 1]\n",
    "\n",
    "  dist_out_combined = tf.sqrt(tf.square(dx_out) + tf.square(dy_out))\n",
    "  dist_in_combined = tf.sqrt(tf.square(dx_in) + tf.square(dy_in))\n",
    "\n",
    "  # Signed Distance Map: negativo all'interno, positivo all'esterno\n",
    "  sdm = dist_out_combined - dist_in_combined\n",
    "  return sdm\n",
    "\n",
    "def boundary_loss(y_true, y_pred):\n",
    "  # Calcola la Signed Distance Map\n",
    "  sdm = compute_signed_distance_map(y_true)\n",
    "  \n",
    "  # Normalizza le previsioni\n",
    "  y_pred = tf.nn.softmax(y_pred, axis=-1)\n",
    "\n",
    "  # Calcola la boundary loss\n",
    "  loss = tf.reduce_mean(tf.abs(sdm * (y_true - y_pred)))\n",
    "  \n",
    "  return loss\n",
    "\n",
    "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
    "  # Convert y_true to one-hot if needed\n",
    "  # TODO: should we retrieve the argmax and use 1 channels instead of 5?\n",
    "  if y_true.shape[-1] != y_pred.shape[-1]:\n",
    "    y_true = tf.one_hot(tf.cast(y_true[..., 0], tf.int32), depth=y_pred.shape[-1])\n",
    "  \n",
    "  # Compute Dice Loss per class\n",
    "  intersection = tf.reduce_sum(y_true * y_pred, axis=(1, 2))\n",
    "  union = tf.reduce_sum(y_true + y_pred, axis=(1, 2))\n",
    "  dice = (2. * intersection + smooth) / (union + smooth)\n",
    "  \n",
    "  # Average Dice Loss over all classes and batch\n",
    "  dice_loss = 1 - tf.reduce_mean(dice)\n",
    "  \n",
    "  return dice_loss\n",
    "\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    alpha = 0.3\n",
    "    beta = 0.3\n",
    "    gamma = 1 - alpha - beta\n",
    "\n",
    "    loss_sparse = tfk.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    loss_dice = dice_loss(y_true, y_pred)\n",
    "  \n",
    "    if y_true.shape[-1] != y_pred.shape[-1]:\n",
    "      y_true = tf.one_hot(tf.cast(y_true[..., 0], tf.int32), depth=y_pred.shape[-1])\n",
    "    loss_boundary = boundary_loss(y_true, y_pred)\n",
    "    focal_loss = tfk.losses.CategoricalFocalCrossentropy() \n",
    "    loss_focal = focal_loss(y_true, y_pred)\n",
    "\n",
    "    return (\n",
    "      alpha * loss_dice +\n",
    "      beta * loss_focal +\n",
    "      gamma * loss_boundary\n",
    "    )\n",
    "\n",
    "def get_loss():\n",
    "  if loss_fn == 'sparse_categorical_crossentropy':\n",
    "    return tfk.losses.SparseCategoricalCrossentropy()\n",
    "  elif loss_fn == 'boundary_loss':\n",
    "    return boundary_loss\n",
    "  elif loss_fn == 'dice_loss':\n",
    "    return dice_loss\n",
    "  elif loss_fn == 'combined_loss':\n",
    "    return combined_loss\n",
    "  else:\n",
    "    raise ValueError(f\"Loss function {loss_fn} not recognized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization callback\n",
    "category_map = {\n",
    "  0: 0, # Background,\n",
    "  1: 1, # Soil,\n",
    "  2: 2, # Bedrock,\n",
    "  3: 3, # Sand,\n",
    "  4: 4, # Big Rock,\n",
    "}\n",
    "\n",
    "def apply_category_mapping(label):\n",
    "  \"\"\"\n",
    "  Apply category mapping to labels.\n",
    "  \"\"\"\n",
    "  print(\"Label dtype before mapping:\", label.dtype)\n",
    "  keys_tensor = tf.constant(list(category_map.keys()), dtype=tf.int32)\n",
    "  vals_tensor = tf.constant(list(category_map.values()), dtype=tf.int32)\n",
    "  table = tf.lookup.StaticHashTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor),\n",
    "    default_value=0\n",
    "  )\n",
    "  return table.lookup(label)\n",
    "\n",
    "def create_segmentation_colormap(num_classes):\n",
    "  \"\"\"\n",
    "  Create a linear colormap using a predefined palette.\n",
    "  Uses 'viridis' as default because it is perceptually uniform\n",
    "  and works well for colorblindness.\n",
    "  \"\"\"\n",
    "  return plt.cm.viridis(np.linspace(0, 1, num_classes))\n",
    "\n",
    "def apply_colormap(label, colormap=None):\n",
    "  \"\"\"\n",
    "  Apply the colormap to a label.\n",
    "  \"\"\"\n",
    "  # Ensure label is 2D\n",
    "  label = np.squeeze(label)\n",
    "\n",
    "  if colormap is None:\n",
    "    num_classes = len(np.unique(label))\n",
    "    colormap = create_segmentation_colormap(num_classes)\n",
    "\n",
    "  # Apply the colormap\n",
    "  colored = colormap[label.astype(int)]\n",
    "\n",
    "  return colored\n",
    "  \n",
    "class VizCallback(tf.keras.callbacks.Callback):\n",
    "  def __init__(self, image, label, frequency=5):\n",
    "    super().__init__()\n",
    "    self.image = image\n",
    "    self.label = tf.cast(tf.convert_to_tensor(label), tf.int32) \n",
    "    self.frequency = frequency\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    if epoch % self.frequency == 0:  # Visualize only every \"frequency\" epochs\n",
    "      image, label = self.image, self.label\n",
    "      label = apply_category_mapping(label)\n",
    "      image = tf.expand_dims(image, 0)\n",
    "      pred = self.model.predict(image, verbose=0)\n",
    "      y_pred = tf.math.argmax(pred, axis=-1)\n",
    "      y_pred = y_pred.numpy()\n",
    "\n",
    "      # Create colormap\n",
    "      num_classes = NUM_CLASSES\n",
    "      colormap = create_segmentation_colormap(num_classes)\n",
    "\n",
    "      plt.figure(figsize=(16, 4))\n",
    "\n",
    "      # Input image\n",
    "      plt.subplot(1, 3, 1)\n",
    "      plt.imshow(image[0],cmap='gray')\n",
    "      plt.title(\"Input Image\")\n",
    "      plt.axis('off')\n",
    "\n",
    "      # Ground truth\n",
    "      plt.subplot(1, 3, 2)\n",
    "      colored_label = apply_colormap(label.numpy(), colormap)\n",
    "      plt.imshow(colored_label)\n",
    "      plt.title(\"Ground Truth Mask\")\n",
    "      plt.axis('off')\n",
    "\n",
    "      # Prediction\n",
    "      plt.subplot(1, 3, 3)\n",
    "      colored_pred = apply_colormap(y_pred[0], colormap)\n",
    "      plt.imshow(colored_pred)\n",
    "      plt.title(\"Predicted Mask\")\n",
    "      plt.axis('off')\n",
    "\n",
    "      plt.tight_layout()\n",
    "      plt.show()\n",
    "      plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T20:59:53.000513Z",
     "iopub.status.busy": "2024-11-21T20:59:53.000251Z",
     "iopub.status.idle": "2024-11-21T20:59:53.010025Z",
     "shell.execute_reply": "2024-11-21T20:59:53.009423Z",
     "shell.execute_reply.started": "2024-11-21T20:59:53.000489Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define fitting callbacks. Comment out from dict the unwanted ones\n",
    "val_mask = tf.expand_dims(y_val[0], axis=-1)\n",
    "val_mask = tf.image.resize(val_mask, [IMG_SIZE[0], IMG_SIZE[1]], method=\"nearest\")\n",
    "val_img = tf.image.resize(X_val[0], [IMG_SIZE[0], IMG_SIZE[1]])\n",
    "viz_callback = VizCallback(val_img, val_mask)\n",
    "model_fit_callbacks = {\n",
    "  #'ReduceLROnPlateau': tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=25, min_lr=1e-6, verbose=1),\n",
    "  'EarlyStopping': tfk.callbacks.EarlyStopping(monitor='val_mean_iou', mode='max', patience=50, restore_best_weights=True, verbose=1),\n",
    "  'Viz_callback' : viz_callback\n",
    "}\n",
    "\n",
    "def get_callbacks():\n",
    "  return [i for i in model_fit_callbacks.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Define model, augmentation and utils builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_augmentation(img, mask):\n",
    "  transform = build_augmentation()\n",
    "  transformed = transform(image=img, mask=mask)\n",
    "  return transformed[\"image\"], transformed[\"mask\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the augmented dataset\n",
    "if not quick_run:\n",
    "  N = 2\n",
    "  ds = get_dataset(X_train[:N], y_train[:N], augmentations=apply_augmentation, augmentation_repetition=4, concat_and_shuffle_aug_with_no_aug=True)\n",
    "\n",
    "  for batch in ds.take(1):\n",
    "    a, b = batch\n",
    "    plot(a.numpy(), b.numpy(), num_images=N * 5) # use N * (augmentation_repetition+1) as `concat_and_shuffle_aug_with_no_aug` is True \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:00:15.913190Z",
     "iopub.status.busy": "2024-11-21T21:00:15.912842Z",
     "iopub.status.idle": "2024-11-21T21:00:15.923585Z",
     "shell.execute_reply": "2024-11-21T21:00:15.922710Z",
     "shell.execute_reply.started": "2024-11-21T21:00:15.913155Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# taken from https://keras.io/examples/vision/oxford_pets_image_segmentation/\n",
    "def build_U_NET_XCEPTION(img_size: tuple[int, int, int], num_classes):\n",
    "  inputs = tfk.Input(shape=img_size) # One channel input\n",
    "\n",
    "  ### [First half of the network: downsampling inputs] ###\n",
    "\n",
    "  # Entry block\n",
    "  x = tfkl.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
    "  x = tfkl.BatchNormalization()(x)\n",
    "  x = tfkl.Activation(\"relu\")(x)\n",
    "\n",
    "  previous_block_activation = x  # Set aside residual\n",
    "\n",
    "  # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
    "  for filters in [64, 128, 256]:\n",
    "    x = tfkl.Activation(\"relu\")(x)\n",
    "    x = tfkl.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "\n",
    "    x = tfkl.Activation(\"relu\")(x)\n",
    "    x = tfkl.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "\n",
    "    x = tfkl.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    # Project residual\n",
    "    residual = tfkl.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
    "    previous_block_activation\n",
    "    )\n",
    "    x = tfkl.add([x, residual])  # Add back residual\n",
    "    previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "  ### [Second half of the network: upsampling inputs] ###\n",
    "\n",
    "  for filters in [256, 128, 64, 32]:\n",
    "    x = tfkl.Activation(\"relu\")(x)\n",
    "    x = tfkl.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "\n",
    "    x = tfkl.Activation(\"relu\")(x)\n",
    "    x = tfkl.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "\n",
    "    x = tfkl.UpSampling2D(2)(x)\n",
    "\n",
    "    # Project residual\n",
    "    residual = tfkl.UpSampling2D(2)(previous_block_activation)\n",
    "    residual = tfkl.Conv2D(filters, 1, padding=\"same\")(residual)\n",
    "    x = tfkl.add([x, residual])  # Add back residual\n",
    "    previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "  # Add a per-pixel classification layer\n",
    "  outputs = tfkl.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
    "\n",
    "  # Define the model\n",
    "  model = tfk.Model(inputs, outputs, name='UNetXception')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_U_NET(img_size: tuple[int, int, int], num_classes, binary=False):\n",
    "  def unet_block(input_tensor, filters, kernel_size=3, activation='relu', stack=2, name=''):\n",
    "    # Initialise the input tensor\n",
    "    x = input_tensor\n",
    "\n",
    "    # Apply a sequence of Conv2D, Batch Normalisation, and Activation layers for the specified number of stacks\n",
    "    for i in range(stack):\n",
    "        x = tfkl.Conv2D(filters, kernel_size=kernel_size, padding='same', name=name + 'conv' + str(i + 1))(x)\n",
    "        x = tfkl.BatchNormalization(name=name + 'bn' + str(i + 1))(x)\n",
    "        x = tfkl.Activation(activation, name=name + 'activation' + str(i + 1))(x)\n",
    "\n",
    "    # Return the transformed tensor\n",
    "    return x\n",
    "\n",
    "  input_layer = tfkl.Input(shape=img_size, name='input_layer')\n",
    "\n",
    "  # Downsampling path\n",
    "  down_block_1 = unet_block(input_layer, 32, name='down_block1_')\n",
    "  d1 = tfkl.MaxPooling2D()(down_block_1)\n",
    "\n",
    "  down_block_2 = unet_block(d1, 64, name='down_block2_')\n",
    "  d2 = tfkl.MaxPooling2D()(down_block_2)\n",
    "\n",
    "  # Bottleneck\n",
    "  bottleneck = unet_block(d2, 128, name='bottleneck')\n",
    "\n",
    "  # Upsampling path\n",
    "  u1 = tfkl.UpSampling2D()(bottleneck)\n",
    "  u1 = tfkl.Concatenate()([u1, down_block_2])\n",
    "  u1 = unet_block(u1, 64, name='up_block1_')\n",
    "\n",
    "  u2 = tfkl.UpSampling2D()(u1)\n",
    "  u2 = tfkl.Concatenate()([u2, down_block_1])\n",
    "  u2 = unet_block(u2, 32, name='up_block2_')\n",
    "\n",
    "  # Output Layer\n",
    "  if binary:\n",
    "    output_layer = tfkl.Conv2D(1, kernel_size=1, padding='same', activation=\"softmax\", name='output_layer')(u2)\n",
    "  else:\n",
    "    output_layer = tfkl.Conv2D(num_classes, kernel_size=1, padding='same', activation=\"softmax\", name='output_layer')(u2)\n",
    "\n",
    "  model = tf.keras.Model(inputs=input_layer, outputs=output_layer, name='UNet')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_GROUP_NORM_UNET(img_size: tuple[int, int, int], num_classes: int, groups=8):\n",
    "  \"\"\"U-NET architecture using Group Normalization instead of Batch Normalization\"\"\"\n",
    "  def conv_group_norm_block(input_tensor, filters, kernel_size=3, groups=8, activation='relu', stack=2, name=''):\n",
    "    x = input_tensor\n",
    "    \n",
    "    for i in range(stack):\n",
    "      x = tfkl.Conv2D(\n",
    "        filters, \n",
    "        kernel_size=kernel_size, \n",
    "        padding='same', \n",
    "        name=name + f'conv{i + 1}'\n",
    "      )(x)\n",
    "      x = tfkl.GroupNormalization(\n",
    "        groups=min(groups, filters), # Cannot have more groups than channels\n",
    "        name=name + f'gn{i + 1}'\n",
    "      )(x)\n",
    "      x = tfkl.Activation(activation, name=name + f'activation{i + 1}')(x)\n",
    "      \n",
    "    return x\n",
    "\n",
    "  input_layer = tfkl.Input(shape=img_size, name='input_layer')\n",
    "\n",
    "  # Downsampling path\n",
    "  down_block_1 = conv_group_norm_block(input_layer, 64, groups=groups, name='down_block1_')\n",
    "  d1 = tfkl.MaxPooling2D()(down_block_1)\n",
    "\n",
    "  down_block_2 = conv_group_norm_block(d1, 128, groups=groups, name='down_block2_')\n",
    "  d2 = tfkl.MaxPooling2D()(down_block_2)\n",
    "\n",
    "  down_block_3 = conv_group_norm_block(d2, 256, groups=groups, name='down_block3_')\n",
    "  d3 = tfkl.MaxPooling2D()(down_block_3)\n",
    "\n",
    "  # Bottleneck\n",
    "  bottleneck = conv_group_norm_block(d3, 512, groups=groups, name='bottleneck')\n",
    "  bottleneck = tfkl.Dropout(0.3)(bottleneck)\n",
    "\n",
    "  # Upsampling path\n",
    "  u1 = tfkl.UpSampling2D()(bottleneck)\n",
    "  u1 = tfkl.Concatenate()([u1, down_block_3])\n",
    "  u1 = conv_group_norm_block(u1, 256, groups=groups, name='up_block1_')\n",
    "  u1 = tfkl.Dropout(0.2)(u1)\n",
    "\n",
    "  u2 = tfkl.UpSampling2D()(u1)\n",
    "  u2 = tfkl.Concatenate()([u2, down_block_2])\n",
    "  u2 = conv_group_norm_block(u2, 128, groups=groups, name='up_block2_')\n",
    "  u2 = tfkl.Dropout(0.2)(u2)\n",
    "\n",
    "  u3 = tfkl.UpSampling2D()(u2)\n",
    "  u3 = tfkl.Concatenate()([u3, down_block_1])\n",
    "  u3 = conv_group_norm_block(u3, 64, groups=groups, name='up_block3_')\n",
    "  u3 = tfkl.Dropout(0.1)(u3)\n",
    "\n",
    "  # Output Layer\n",
    "  output_layer = tfkl.Conv2D(num_classes, kernel_size=1, padding='same', activation=\"softmax\", name='output_layer')(u3)\n",
    "\n",
    "  model = tf.keras.Model(inputs=input_layer, outputs=output_layer, name='GroupNormUNet')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ATTENTION_UW_NET(img_size: tuple[int, int, int], num_classes):\n",
    "  def attention_block(x, g, inter_channel):\n",
    "    # theta_x (bs, h, w, inter_channel)\n",
    "    theta_x = tfkl.Conv2D(inter_channel, [1, 1], strides=[1, 1])(x)\n",
    "    \n",
    "    # phi_g (bs, h, w, inter_channel)\n",
    "    phi_g = tfkl.Conv2D(inter_channel, [1, 1], strides=[1, 1])(g)\n",
    "    \n",
    "    # f (bs, h, w, 1)\n",
    "    f = tfkl.Activation('relu')(tfkl.Add()([theta_x, phi_g]))\n",
    "    psi_f = tfkl.Conv2D(1, [1, 1], strides=[1, 1])(f)\n",
    "    \n",
    "    # sigmoid_psi_f (bs, h, w, 1)\n",
    "    sigmoid_psi_f = tfkl.Activation('sigmoid')(psi_f)\n",
    "    \n",
    "    # rate (bs, h, w, 1)\n",
    "    rate = tfkl.multiply([x, sigmoid_psi_f])\n",
    "    \n",
    "    return rate\n",
    "  \n",
    "  # Input\n",
    "  inputs = tfkl.Input(shape=img_size)\n",
    "  \n",
    "  # Encoder Path\n",
    "  # Block 1\n",
    "  conv1 = tfkl.Conv2D(64, 3, padding='same')(inputs)\n",
    "  conv1 = tfkl.BatchNormalization()(conv1)\n",
    "  conv1 = tfkl.Activation('relu')(conv1)\n",
    "  conv1 = tfkl.Conv2D(64, 3, padding='same')(conv1)\n",
    "  conv1 = tfkl.BatchNormalization()(conv1)\n",
    "  conv1 = tfkl.Activation('relu')(conv1)\n",
    "  pool1 = tfkl.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "  \n",
    "  # Block 2\n",
    "  conv2 = tfkl.Conv2D(128, 3, padding='same')(pool1)\n",
    "  conv2 = tfkl.BatchNormalization()(conv2)\n",
    "  conv2 = tfkl.Activation('relu')(conv2)\n",
    "  conv2 = tfkl.Conv2D(128, 3, padding='same')(conv2)\n",
    "  conv2 = tfkl.BatchNormalization()(conv2)\n",
    "  conv2 = tfkl.Activation('relu')(conv2)\n",
    "  pool2 = tfkl.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "  \n",
    "  # Block 3\n",
    "  conv3 = tfkl.Conv2D(256, 3, padding='same')(pool2)\n",
    "  conv3 = tfkl.BatchNormalization()(conv3)\n",
    "  conv3 = tfkl.Activation('relu')(conv3)\n",
    "  conv3 = tfkl.Conv2D(256, 3, padding='same')(conv3)\n",
    "  conv3 = tfkl.BatchNormalization()(conv3)\n",
    "  conv3 = tfkl.Activation('relu')(conv3)\n",
    "  pool3 = tfkl.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "  \n",
    "  # Block 4\n",
    "  conv4 = tfkl.Conv2D(512, 3, padding='same')(pool3)\n",
    "  conv4 = tfkl.BatchNormalization()(conv4)\n",
    "  conv4 = tfkl.Activation('relu')(conv4)\n",
    "  conv4 = tfkl.Conv2D(512, 3, padding='same')(conv4)\n",
    "  conv4 = tfkl.BatchNormalization()(conv4)\n",
    "  conv4 = tfkl.Activation('relu')(conv4)\n",
    "  pool4 = tfkl.MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "  \n",
    "  # Bridge\n",
    "  conv5 = tfkl.Conv2D(1024, 3, padding='same')(pool4)\n",
    "  conv5 = tfkl.BatchNormalization()(conv5)\n",
    "  conv5 = tfkl.Activation('relu')(conv5)\n",
    "  conv5 = tfkl.Conv2D(1024, 3, padding='same')(conv5)\n",
    "  conv5 = tfkl.BatchNormalization()(conv5)\n",
    "  conv5 = tfkl.Activation('relu')(conv5)\n",
    "  \n",
    "  # Decoder Path with Attention\n",
    "  # Block 6\n",
    "  up6 = tfkl.Conv2D(512, 2, padding='same')(tfkl.UpSampling2D(size=(2, 2))(conv5))\n",
    "  up6 = tfkl.BatchNormalization()(up6)\n",
    "  up6 = tfkl.Activation('relu')(up6)\n",
    "  \n",
    "  att6 = attention_block(conv4, up6, inter_channel=256)\n",
    "  merge6 = tfkl.concatenate([att6, up6], axis=3)\n",
    "  \n",
    "  conv6 = tfkl.Conv2D(512, 3, padding='same')(merge6)\n",
    "  conv6 = tfkl.BatchNormalization()(conv6)\n",
    "  conv6 = tfkl.Activation('relu')(conv6)\n",
    "  conv6 = tfkl.Conv2D(512, 3, padding='same')(conv6)\n",
    "  conv6 = tfkl.BatchNormalization()(conv6)\n",
    "  conv6 = tfkl.Activation('relu')(conv6)\n",
    "  \n",
    "  # Block 7\n",
    "  up7 = tfkl.Conv2D(256, 2, padding='same')(tfkl.UpSampling2D(size=(2, 2))(conv6))\n",
    "  up7 = tfkl.BatchNormalization()(up7)\n",
    "  up7 = tfkl.Activation('relu')(up7)\n",
    "  \n",
    "  att7 = attention_block(conv3, up7, inter_channel=128)\n",
    "  merge7 = tfkl.concatenate([att7, up7], axis=3)\n",
    "  \n",
    "  conv7 = tfkl.Conv2D(256, 3, padding='same')(merge7)\n",
    "  conv7 = tfkl.BatchNormalization()(conv7)\n",
    "  conv7 = tfkl.Activation('relu')(conv7)\n",
    "  conv7 = tfkl.Conv2D(256, 3, padding='same')(conv7)\n",
    "  conv7 = tfkl.BatchNormalization()(conv7)\n",
    "  conv7 = tfkl.Activation('relu')(conv7)\n",
    "  \n",
    "  # Block 8\n",
    "  up8 = tfkl.Conv2D(128, 2, padding='same')(tfkl.UpSampling2D(size=(2, 2))(conv7))\n",
    "  up8 = tfkl.BatchNormalization()(up8)\n",
    "  up8 = tfkl.Activation('relu')(up8)\n",
    "  \n",
    "  att8 = attention_block(conv2, up8, inter_channel=64)\n",
    "  merge8 = tfkl.concatenate([att8, up8], axis=3)\n",
    "  \n",
    "  conv8 = tfkl.Conv2D(128, 3, padding='same')(merge8)\n",
    "  conv8 = tfkl.BatchNormalization()(conv8)\n",
    "  conv8 = tfkl.Activation('relu')(conv8)\n",
    "  conv8 = tfkl.Conv2D(128, 3, padding='same')(conv8)\n",
    "  conv8 = tfkl.BatchNormalization()(conv8)\n",
    "  conv8 = tfkl.Activation('relu')(conv8)\n",
    "  \n",
    "  # Block 9\n",
    "  up9 = tfkl.Conv2D(64, 2, padding='same')(tfkl.UpSampling2D(size=(2, 2))(conv8))\n",
    "  up9 = tfkl.BatchNormalization()(up9)\n",
    "  up9 = tfkl.Activation('relu')(up9)\n",
    "  \n",
    "  att9 = attention_block(conv1, up9, inter_channel=32)\n",
    "  merge9 = tfkl.concatenate([att9, up9], axis=3)\n",
    "  \n",
    "  conv9 = tfkl.Conv2D(64, 3, padding='same')(merge9)\n",
    "  conv9 = tfkl.BatchNormalization()(conv9)\n",
    "  conv9 = tfkl.Activation('relu')(conv9)\n",
    "  conv9 = tfkl.Conv2D(64, 3, padding='same')(conv9)\n",
    "  conv9 = tfkl.BatchNormalization()(conv9)\n",
    "  conv9 = tfkl.Activation('relu')(conv9)\n",
    "  \n",
    "  # Output\n",
    "  outputs = tfkl.Conv2D(num_classes, 1, activation='softmax')(conv9)\n",
    "  \n",
    "  model = tfk.Model(inputs=inputs, outputs=outputs, name='AttentionUWNet')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ASPP_model(img_size: tuple[int, int, int], num_classes: int):\n",
    "  \n",
    "  initializer = tf.keras.initializers.HeNormal()\n",
    "  regularizer = tf.keras.regularizers.l2(1e-4)\n",
    "\n",
    "  inputs = tfkl.Input(shape=img_size)\n",
    "\n",
    "  def conv_block(x, filters, kernel_size=(3, 3), activation=\"relu\", batch_norm=True, dropout_rate=0.2):\n",
    "    x = tfkl.Conv2D(\n",
    "      filters,\n",
    "      kernel_size,\n",
    "      padding=\"same\",\n",
    "      kernel_initializer=initializer,\n",
    "      kernel_regularizer=regularizer,\n",
    "    )(x)\n",
    "    if batch_norm:\n",
    "      x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Activation(activation)(x)\n",
    "    if dropout_rate > 0:\n",
    "      x = tfkl.SpatialDropout2D(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "  def encoder_block(x, filters, dropout_rate=0.2):\n",
    "    x = conv_block(x, filters, dropout_rate=dropout_rate)\n",
    "    x = conv_block(x, filters, dropout_rate=dropout_rate)\n",
    "    p = tfkl.MaxPooling2D(2)(x)\n",
    "    return x, p\n",
    "\n",
    "  def atrous_spatial_pyramid_pooling(x, dropout_rate=0.3):\n",
    "    dims = x.shape[1:3]\n",
    "    pool = tfkl.GlobalAveragePooling2D()(x)\n",
    "    pool = tfkl.Reshape((1, 1, x.shape[-1]))(pool)\n",
    "    pool = tfkl.Conv2D(\n",
    "      256, \n",
    "      1, \n",
    "      padding=\"same\", \n",
    "      kernel_initializer=initializer, \n",
    "      kernel_regularizer=regularizer,\n",
    "    )(pool)\n",
    "    pool = tfkl.UpSampling2D(size=dims, interpolation=\"bilinear\")(pool)\n",
    "    pool = tfkl.SpatialDropout2D(dropout_rate)(pool)\n",
    "\n",
    "    conv_1x1 = tfkl.Conv2D(\n",
    "      256,\n",
    "      1,\n",
    "      padding=\"same\",\n",
    "      kernel_initializer=initializer,\n",
    "      kernel_regularizer=regularizer,\n",
    "    )(x)\n",
    "    atrous_6 = tfkl.Conv2D(\n",
    "      256,\n",
    "      3,\n",
    "      dilation_rate=6,\n",
    "      padding=\"same\",\n",
    "      kernel_initializer=initializer,\n",
    "      kernel_regularizer=regularizer,\n",
    "    )(x)\n",
    "    atrous_12 = tfkl.Conv2D(\n",
    "      256,\n",
    "      3,\n",
    "      dilation_rate=12,\n",
    "      padding=\"same\",\n",
    "      kernel_initializer=initializer,\n",
    "      kernel_regularizer=regularizer,\n",
    "    )(x)\n",
    "    atrous_18 = tfkl.Conv2D(\n",
    "      256,\n",
    "      3,\n",
    "      dilation_rate=18,\n",
    "      padding=\"same\",\n",
    "      kernel_initializer=initializer,\n",
    "      kernel_regularizer=regularizer,\n",
    "    )(x)\n",
    "\n",
    "    x = tfkl.Concatenate()([pool, conv_1x1, atrous_6, atrous_12, atrous_18])\n",
    "    x = tfkl.Conv2D(\n",
    "      256,\n",
    "      1,\n",
    "      padding=\"same\",\n",
    "      kernel_initializer=initializer,\n",
    "      kernel_regularizer=regularizer,\n",
    "    )(x)\n",
    "    x = tfkl.SpatialDropout2D(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "  def decoder_block(x, skip, filters, dropout_rate=0.2):\n",
    "    x = tfkl.Conv2DTranspose(\n",
    "      filters,\n",
    "      2,\n",
    "      strides=2,\n",
    "      padding=\"same\",\n",
    "      kernel_initializer=initializer,\n",
    "      kernel_regularizer=regularizer,\n",
    "    )(x)\n",
    "    x = tfkl.Concatenate()([x, skip])\n",
    "    x = conv_block(x, filters, dropout_rate=dropout_rate)\n",
    "    return x\n",
    "\n",
    "  # Encoder\n",
    "  filters = [64, 128, 256, 512]\n",
    "  skips = []\n",
    "  x = inputs\n",
    "  for f in filters:\n",
    "    skip, x = encoder_block(x, f, dropout_rate=0.2)\n",
    "    skips.append(skip)\n",
    "\n",
    "  # Bottleneck with ASPP\n",
    "  x = conv_block(x, 1024, dropout_rate=0.3)\n",
    "  x = atrous_spatial_pyramid_pooling(x, dropout_rate=0.3)\n",
    "\n",
    "  # Decoder\n",
    "  skips = skips[::-1]\n",
    "  decoder_filters = [512, 256, 128, 64]\n",
    "  for skip, f in zip(skips, decoder_filters):\n",
    "    x = decoder_block(x, skip, f, dropout_rate=0.2)\n",
    "\n",
    "  # Final convolutional layer\n",
    "  outputs = tfkl.Conv2D(\n",
    "    num_classes, \n",
    "    1, \n",
    "    activation=\"softmax\", \n",
    "    kernel_initializer=initializer, \n",
    "    kernel_regularizer=regularizer,\n",
    "  )(x)\n",
    "\n",
    "  model = tf.keras.Model(inputs, outputs)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_RockSeg(img_size: tuple[int, int, int], num_classes: int):\n",
    "  \"\"\"\n",
    "  Builds the RockSeg model, adjusted for input image size 64x128.\n",
    "\n",
    "  Parameters:\n",
    "    img_size (tuple): Input image dimensions (height, width, channels).\n",
    "    num_classes (int): Number of output classes.\n",
    "\n",
    "  Returns:\n",
    "    tf.keras.Model: RockSeg model instance.\n",
    "  \"\"\"\n",
    "\n",
    "  initializer = tf.keras.initializers.HeNormal()\n",
    "  regularizer = tf.keras.regularizers.l2(1e-4)  # L2 regularization with strength 1e-4\n",
    "\n",
    "  def resnet_block(input_tensor, filters, kernel_size=3, activation='relu', stack=2, name=''):\n",
    "    x = input_tensor\n",
    "    for i in range(stack):\n",
    "      x = tfkl.Conv2D(filters, kernel_size=kernel_size, padding='same', kernel_initializer=initializer,kernel_regularizer=regularizer, name=name + f'conv{i + 1}')(x)\n",
    "      x = tfkl.BatchNormalization(name=name + f'bn{i + 1}')(x)\n",
    "      x = tfkl.Activation(activation, name=name + f'activation{i + 1}')(x)\n",
    "    return x\n",
    "\n",
    "  def transformer_block(input_tensor, embed_dim, num_heads, name=''):\n",
    "    x = tfkl.Conv2D(filters=256, kernel_size=(1, 1), padding='same', kernel_initializer=initializer,kernel_regularizer=regularizer, name=name + 'transformer_11')(input_tensor)\n",
    "    x = tfkl.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"same\", name=name + 'avg_pool')(x)\n",
    "    x = tf.keras.tfkl.LayerNormalization(name=name + 'ln')(x)\n",
    "    attention_output = tf.keras.tfkl.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name=name + 'mha')(x, x)\n",
    "    x = tf.keras.tfkl.Add(name=name + 'skip1')([x, attention_output])\n",
    "    feed_forward = tfkl.Dense(embed_dim, activation='relu', kernel_initializer=initializer,kernel_regularizer=regularizer, name=name + 'dense')(x)\n",
    "    x = tf.keras.tfkl.Add(name=name + 'skip2')([x, feed_forward])\n",
    "    return x\n",
    "\n",
    "  def multiscale_feature_fusion(feature_maps, name=''):\n",
    "    base_channels = feature_maps[len(feature_maps) // 2].shape[-1]\n",
    "    consistent_features = [\n",
    "      tfkl.Conv2D(base_channels, kernel_size=(1, 1), padding='same', kernel_initializer=initializer,kernel_regularizer=regularizer, name=name + f'conv_{i}')(fm) if fm.shape[-1] != base_channels else fm\n",
    "      for i, fm in enumerate(feature_maps)\n",
    "    ]\n",
    "    base_height, base_width = consistent_features[len(consistent_features) // 2].shape[1:3]\n",
    "    resized_features = [\n",
    "      tfkl.UpSampling2D(size=(base_height // fm.shape[1], base_width // fm.shape[2]), interpolation='bilinear', name=name + f'up_{i}')(fm)\n",
    "      if fm.shape[1] < base_height else\n",
    "      tfkl.MaxPooling2D(pool_size=(fm.shape[1] // base_height, fm.shape[2] // base_width), name=name + f'pool_{i}')(fm)\n",
    "      for i, fm in enumerate(consistent_features)\n",
    "    ]\n",
    "    fused = tfkl.Concatenate(name=name + 'concat')(resized_features)\n",
    "    return tfkl.Conv2D(base_channels, kernel_size=1, padding='same', activation='relu', kernel_initializer=initializer,kernel_regularizer=regularizer, name=name + 'conv_fused')(fused)\n",
    "\n",
    "  input_layer = tfkl.Input(shape=img_size, name='input_layer')\n",
    "\n",
    "  conv1 = tfkl.Conv2D(filters=64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializer,kernel_regularizer=regularizer, name='conv1')(input_layer)\n",
    "  conv1_upsampled = tfkl.UpSampling2D(size=(2, 2), interpolation='bilinear', name='conv1_upsampled')(conv1)\n",
    "  maxpool1 = tfkl.MaxPooling2D(pool_size=(2, 2), name='pool1')(conv1)\n",
    "\n",
    "  resnet1 = resnet_block(maxpool1, filters=64, stack=2, name='resnet1_')\n",
    "  resnet2 = resnet_block(resnet1, filters=128, stack=2, name='resnet2_')\n",
    "\n",
    "  transformer = transformer_block(resnet2, embed_dim=256, num_heads=4, name='transformer_')\n",
    "\n",
    "  bottleneck = tfkl.Conv2D(filters=512, kernel_size=3, padding='same', activation='relu', kernel_initializer=initializer, kernel_regularizer=regularizer, name='bottleneck')(transformer)\n",
    "\n",
    "  msf1 = multiscale_feature_fusion([resnet1, resnet2, bottleneck], name='msf1_')\n",
    "  msf1_upsampled = tfkl.UpSampling2D(size=(2, 2), interpolation='bilinear', name='msf1_upsample')(msf1)\n",
    "\n",
    "  resnet1_upsampled = tfkl.UpSampling2D(size=(2, 2), interpolation='bilinear', name='resnet1_upsample')(resnet1)\n",
    "\n",
    "  concat1 = tfkl.Concatenate(name='concat1')([msf1_upsampled, resnet1_upsampled])\n",
    "  decoder1 = tfkl.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', kernel_initializer=initializer, kernel_regularizer=regularizer, name='decoder1')(concat1)\n",
    "\n",
    "  upsample2 = tfkl.UpSampling2D(size=(2, 2), interpolation='bilinear', name='upsample2')(decoder1)\n",
    "\n",
    "  concat2 = tfkl.Concatenate(name='concat2')([upsample2, conv1_upsampled])\n",
    "  decoder2 = tfkl.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', kernel_initializer=initializer,kernel_regularizer=regularizer, name='decoder2')(concat2)\n",
    "\n",
    "  upsample3 = tfkl.UpSampling2D(size=(1, 1), interpolation='bilinear', name='upsample3')(decoder2)\n",
    "\n",
    "  output_layer = tfkl.Conv2D(num_classes, kernel_size=1, padding='same', activation=\"softmax\", kernel_initializer=initializer, name='output_layer')(upsample3)\n",
    "\n",
    "  model = tf.keras.Model(inputs=input_layer, outputs=output_layer, name='RockSeg')\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_TURKEYSEG(img_size: tuple[int, int, int], num_classes: int):\n",
    "  def conv_block(inputs, filters, kernel_size=3, dilation_rate=1, use_se=True, name=\"conv_block\"):\n",
    "    \"\"\"A convolutional block with optional squeeze-and-excitation.\"\"\"\n",
    "    x = tfkl.Conv2D(filters, kernel_size, padding='same', dilation_rate=dilation_rate, activation='relu', name=f\"{name}_conv1\")(inputs)\n",
    "    x = tfkl.Conv2D(filters, kernel_size, padding='same', activation='relu', name=f\"{name}_conv2\")(x)\n",
    "    if use_se:\n",
    "      se = tfkl.GlobalAveragePooling2D(name=f\"{name}_se_pool\")(x)\n",
    "      se = tfkl.Dense(filters // 16, activation='relu', name=f\"{name}_se_dense1\")(se)\n",
    "      se = tfkl.Dense(filters, activation='sigmoid', name=f\"{name}_se_dense2\")(se)\n",
    "      se = tfkl.Reshape((1, 1, filters), name=f\"{name}_se_reshape\")(se)\n",
    "      x = tfkl.multiply([x, se], name=f\"{name}_se_mult\")\n",
    "    return x\n",
    "\n",
    "  def down_block(inputs, filters, name=\"down_block\"):\n",
    "    \"\"\"Downsampling block with max pooling.\"\"\"\n",
    "    x = conv_block(inputs, filters, name=f\"{name}_conv\")\n",
    "    p = tfkl.MaxPooling2D(pool_size=(2, 2), name=f\"{name}_pool\")(x)\n",
    "    return x, p\n",
    "\n",
    "  def up_block(inputs, skip, filters, name=\"up_block\"):\n",
    "    \"\"\"Upsampling block with skip connections.\"\"\"\n",
    "    x = tfkl.Conv2DTranspose(filters, kernel_size=3, strides=2, padding='same', name=f\"{name}_upsample\")(inputs)\n",
    "    x = tfkl.concatenate([x, skip], name=f\"{name}_concat\")\n",
    "    x = conv_block(x, filters, name=f\"{name}_conv\")\n",
    "    return x\n",
    "\n",
    "  def bottleneck_with_attention(inputs, filters, name=\"bottleneck\"):\n",
    "    \"\"\"Bottleneck with parallel dilated convolutions and MultiHeadAttention.\"\"\"\n",
    "    # Parallel dilated convolutions\n",
    "    d1 = tfkl.Conv2D(filters, kernel_size=3, dilation_rate=1, padding='same', activation='relu', name=f\"{name}_dil1\")(inputs)\n",
    "    d2 = tfkl.Conv2D(filters, kernel_size=3, dilation_rate=2, padding='same', activation='relu', name=f\"{name}_dil2\")(inputs)\n",
    "    d3 = tfkl.Conv2D(filters, kernel_size=3, dilation_rate=4, padding='same', activation='relu', name=f\"{name}_dil3\")(inputs)\n",
    "    fused = tfkl.Add(name=f\"{name}_fused\")([d1, d2, d3])\n",
    "    \n",
    "    # Reshape for attention\n",
    "    b, h, w, c = fused.shape  # Batch, Height, Width, Channels\n",
    "    reshaped = tfkl.Reshape((h * w, c), name=f\"{name}_reshape\")(fused)\n",
    "    \n",
    "    # MultiHeadAttention\n",
    "    attention_output = tfkl.MultiHeadAttention(num_heads=4, key_dim=c, name=f\"{name}_mha\")(reshaped, reshaped)\n",
    "    attention_output = tfkl.Reshape((h, w, c), name=f\"{name}_attention_reshape\")(attention_output)\n",
    "    \n",
    "    # Merge attention with fused features\n",
    "    output = tfkl.Add(name=f\"{name}_attention_fusion\")([fused, attention_output])\n",
    "    \n",
    "    return output\n",
    "\n",
    "  inputs = tf.keras.Input(shape=img_size, name=\"input_image\")\n",
    "\n",
    "  # Encoder (global context path)\n",
    "  g1, p1 = down_block(inputs, 64, name=\"global_down1\")\n",
    "  g2, p2 = down_block(p1, 128, name=\"global_down2\")\n",
    "  g3, p3 = down_block(p2, 256, name=\"global_down3\")\n",
    "\n",
    "  # Bottleneck (global context)\n",
    "  g_bottleneck = bottleneck_with_attention(p3, 512, name=\"global_bottleneck\")\n",
    "\n",
    "  # Decoder (global context path)\n",
    "  g_up3 = up_block(g_bottleneck, g3, 256, name=\"global_up3\")\n",
    "  g_up2 = up_block(g_up3, g2, 128, name=\"global_up2\")\n",
    "  g_up1 = up_block(g_up2, g1, 64, name=\"global_up1\")\n",
    "\n",
    "  # Local path (for fine details)\n",
    "  l1, lp1 = down_block(inputs, 32, name=\"local_down1\")\n",
    "  l2, lp2 = down_block(lp1, 64, name=\"local_down2\")\n",
    "  l_bottleneck = bottleneck_with_attention(lp2, 128, name=\"local_bottleneck\")\n",
    "  l_up2 = up_block(l_bottleneck, l2, 64, name=\"local_up2\")\n",
    "  l_up1 = up_block(l_up2, l1, 32, name=\"local_up1\")\n",
    "\n",
    "  # Fusion of global and local paths\n",
    "  fusion = tfkl.concatenate([g_up1, l_up1], name=\"fusion_concat\")\n",
    "  fusion = conv_block(fusion, 64, name=\"fusion_conv\")\n",
    "\n",
    "  # Output layer\n",
    "  outputs = tfkl.Conv2D(num_classes, kernel_size=1, activation='softmax', name=\"output_layer\")(fusion)\n",
    "\n",
    "  return tfk.Model(inputs, outputs, name=\"MarsSegmentationModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light4Mars: https://doi.org/10.1016/j.isprsjprs.2024.06.008\n",
    "\n",
    "class SqueezeWindowAttention(tfkl.Layer):\n",
    "  def __init__(self, num_heads=8, window_size=(8, 8), **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.num_heads = num_heads\n",
    "    self.Ph, self.Pw = window_size\n",
    "    self.window_size = window_size\n",
    "    \n",
    "  def build(self, input_shape):\n",
    "    _, H, W, C = input_shape\n",
    "    self.channels = C\n",
    "    self.d_head = self.channels // self.num_heads\n",
    "    \n",
    "    # Linear projections for Q, K, V\n",
    "    self.query = tfkl.Dense(self.channels)\n",
    "    self.key = tfkl.Dense(self.channels)\n",
    "    self.value = tfkl.Dense(self.channels)\n",
    "    \n",
    "    self.layer_norm = tfkl.LayerNormalization()\n",
    "    \n",
    "  def window_partition(self, x):\n",
    "    \"\"\"Partition the input into windows\"\"\"\n",
    "    B = tf.shape(x)[0]  # Get batch size dynamically\n",
    "    H, W, C = x.shape[1:]  # Get other dimensions from static shape\n",
    "    \n",
    "    # Ensure input dimensions are divisible by window size\n",
    "    assert H % self.Ph == 0, f\"Input height {H} not divisible by window height {self.Ph}\"\n",
    "    assert W % self.Pw == 0, f\"Input width {W} not divisible by window width {self.Pw}\"\n",
    "    \n",
    "    # Reshape to have window dimensions\n",
    "    x = tf.reshape(x, [\n",
    "      B,  # Batch dimension\n",
    "      H // self.Ph,  # Number of windows in height\n",
    "      self.Ph,  # Window height\n",
    "      W // self.Pw,  # Number of windows in width\n",
    "      self.Pw,  # Window width\n",
    "      C  # Channels\n",
    "    ])\n",
    "    \n",
    "    # Permute to get windows as batch dimension\n",
    "    x = tf.transpose(x, [0, 1, 3, 2, 4, 5])\n",
    "    \n",
    "    # Merge batch and window number dimensions\n",
    "    x = tf.reshape(x, [B * (H // self.Ph) * (W // self.Pw), self.Ph * self.Pw, C])\n",
    "    \n",
    "    return x, B\n",
    "    \n",
    "  def window_reverse(self, x, B, H, W):\n",
    "    \"\"\"Reverse window partitioning\"\"\"\n",
    "    x = tf.reshape(x, [\n",
    "      B,  # Batch\n",
    "      H // self.Ph,  # Windows in height\n",
    "      W // self.Pw,  # Windows in width\n",
    "      self.Ph,  # Window height\n",
    "      self.Pw,  # Window width\n",
    "      self.channels  # Channels\n",
    "    ])\n",
    "    x = tf.transpose(x, [0, 1, 3, 2, 4, 5])\n",
    "    x = tf.reshape(x, [B, H, W, self.channels])\n",
    "    return x\n",
    "    \n",
    "  def squeeze_attention(self, q, k):\n",
    "    \"\"\"Apply squeeze attention as per paper\"\"\"\n",
    "    # Global average pooling within windows\n",
    "    q_squeezed = tf.reduce_mean(q, axis=1, keepdims=True)\n",
    "    k_squeezed = tf.reduce_mean(k, axis=1, keepdims=True)\n",
    "    \n",
    "    # Layer normalization\n",
    "    q_squeezed = self.layer_norm(q_squeezed)\n",
    "    k_squeezed = self.layer_norm(k_squeezed)\n",
    "    \n",
    "    # Element-wise multiplication with original vectors\n",
    "    q = q * q_squeezed\n",
    "    k = k * k_squeezed\n",
    "    \n",
    "    return q, k\n",
    "  \n",
    "  def compute_output_shape(self, input_shape):\n",
    "    return input_shape\n",
    "  \n",
    "  def call(self, inputs):\n",
    "    # Window partitioning\n",
    "    x, B = self.window_partition(inputs)\n",
    "    H, W = inputs.shape[1:3]  # Get spatial dimensions\n",
    "    \n",
    "    # Linear projections\n",
    "    q = self.query(x)\n",
    "    k = self.key(x)\n",
    "    v = self.value(x)\n",
    "    \n",
    "    # Reshape for multi-head attention\n",
    "    q = tf.reshape(q, [-1, self.Ph*self.Pw, self.num_heads, self.d_head])\n",
    "    k = tf.reshape(k, [-1, self.Ph*self.Pw, self.num_heads, self.d_head])\n",
    "    v = tf.reshape(v, [-1, self.Ph*self.Pw, self.num_heads, self.d_head])\n",
    "    \n",
    "    # Apply squeeze attention\n",
    "    q, k = self.squeeze_attention(q, k)\n",
    "    \n",
    "    # Compute attention scores\n",
    "    attention_scores = tf.matmul(q, k, transpose_b=True)\n",
    "    attention_scores = attention_scores / tf.math.sqrt(tf.cast(self.d_head, tf.float32))\n",
    "    attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
    "    \n",
    "    # Apply attention to values\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    \n",
    "    # Reshape back to original dimensions\n",
    "    output = tf.reshape(output, [-1, self.Ph*self.Pw, self.channels])\n",
    "    \n",
    "    # Reverse window partitioning\n",
    "    output = self.window_reverse(output, B, H, W)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def squeeze_window_attention(input_tensor, num_heads=8, window_size=(8, 8)):\n",
    "  \"\"\"Implements the full Squeeze Window Transformer block.\"\"\"\n",
    "  # Layer Norm\n",
    "  x = tfkl.LayerNormalization()(input_tensor)\n",
    "  \n",
    "  # Squeeze Window Multi-Head Self-Attention\n",
    "  attention_output = SqueezeWindowAttention(num_heads=num_heads, window_size=window_size)(x)\n",
    "  \n",
    "  # Skip connection\n",
    "  x = tfkl.Add()([input_tensor, attention_output])\n",
    "  \n",
    "  # Layer Norm + Upgraded FeedForward Network\n",
    "  x = tfkl.LayerNormalization()(x)\n",
    "  ff_output = tfkl.Conv2D(x.shape[-1], kernel_size=1, strides=1, padding=\"same\", activation='relu')(x)\n",
    "  x = tfkl.Add()([x, ff_output])\n",
    "  \n",
    "  return x\n",
    "\n",
    "def over_patch_embedding(input_tensor, patch_size=4, embedding_dim=64):\n",
    "  \"\"\"Simulates Over Patch Embedding.\"\"\"\n",
    "  return tfkl.Conv2D(embedding_dim, kernel_size=patch_size, strides=patch_size, padding=\"same\")(input_tensor)\n",
    "\n",
    "def aggregate_local_attention(skip_tensor, upsample_tensor):\n",
    "  \"\"\"Implements the Aggregate Local Attention module following paper specifications.\"\"\"\n",
    "  # Match spatial dimensions\n",
    "  if upsample_tensor.shape[1:3] != skip_tensor.shape[1:3]:\n",
    "    upsample_tensor = tfkl.Resizing(\n",
    "      skip_tensor.shape[1], \n",
    "      skip_tensor.shape[2]\n",
    "    )(upsample_tensor)\n",
    "  \n",
    "  # Initial concatenation of Ti and Di\n",
    "  x = tfkl.Concatenate()([skip_tensor, upsample_tensor])\n",
    "  \n",
    "  # Initial DWC3Ã—3\n",
    "  x = tfkl.DepthwiseConv2D(kernel_size=3, padding=\"same\")(x)\n",
    "  x = tfkl.BatchNormalization()(x)\n",
    "  \n",
    "  # Multi-branch structure after BN\n",
    "  branch1 = tfkl.DepthwiseConv2D(kernel_size=1, padding=\"same\")(x)\n",
    "  branch2 = tfkl.DepthwiseConv2D(kernel_size=3, padding=\"same\")(x)\n",
    "  branch3 = tfkl.DepthwiseConv2D(kernel_size=5, padding=\"same\")(x)\n",
    "  \n",
    "  # Sum of branches\n",
    "  x = tfkl.Add()([branch1, branch2, branch3])\n",
    "  \n",
    "  # Convolutional attention weight (CA)\n",
    "  ca = tfkl.Conv2D(\n",
    "    x.shape[-1], \n",
    "    kernel_size=1,\n",
    "    padding=\"same\"\n",
    "  )(x)\n",
    "  ca = tfkl.BatchNormalization()(ca)\n",
    "  \n",
    "  # Element-wise multiplication instead of addition\n",
    "  output = tfkl.Multiply()([ca, x])\n",
    "  \n",
    "  return output\n",
    "\n",
    "# Building the overall model\n",
    "\n",
    "def build_LIGHT4MARS(img_size: tuple[int, int, int], num_classes: int):\n",
    "  inputs = tfk.Input(shape=img_size)\n",
    "  \n",
    "  # Stage 1 - H/2 Ã— W/2 (32x64 -> 16x32)\n",
    "  x1 = tfkl.Conv2D(64, kernel_size=3, strides=2, padding='same')(inputs)\n",
    "  # Use smaller window size for larger feature maps\n",
    "  t1 = squeeze_window_attention(x1, num_heads=8, window_size=(4, 4))\n",
    "  t1_up = tfkl.UpSampling2D(size=(2, 2))(t1)\n",
    "  t1_up = tfkl.Conv2D(t1.shape[-1], kernel_size=1, padding='same')(t1_up)\n",
    "  d1 = aggregate_local_attention(t1, t1_up)\n",
    "  \n",
    "  # Stage 2 - H/4 Ã— W/4 (16x32 -> 8x16)\n",
    "  x2 = tfkl.Conv2D(128, kernel_size=3, strides=2, padding='same')(d1)\n",
    "  # Adjust window size as dimensions get smaller\n",
    "  t2 = squeeze_window_attention(x2, num_heads=8, window_size=(4, 4))\n",
    "  t2_up = tfkl.UpSampling2D(size=(2, 2))(t2)\n",
    "  t2_up = tfkl.Conv2D(t2.shape[-1], kernel_size=1, padding='same')(t2_up)\n",
    "  d2 = aggregate_local_attention(t2, t2_up)\n",
    "  \n",
    "  # Stage 3 - H/8 Ã— W/8 (8x16 -> 4x8)\n",
    "  x3 = tfkl.Conv2D(256, kernel_size=3, strides=2, padding='same')(d2)\n",
    "  # Use even smaller window size\n",
    "  t3 = squeeze_window_attention(x3, num_heads=8, window_size=(2, 2))\n",
    "  t3_up = tfkl.UpSampling2D(size=(2, 2))(t3)\n",
    "  t3_up = tfkl.Conv2D(t3.shape[-1], kernel_size=1, padding='same')(t3_up)\n",
    "  d3 = aggregate_local_attention(t3, t3_up)\n",
    "  \n",
    "  # Stage 4 (Bottleneck) - H/16 Ã— W/16 (4x8 -> 2x4)\n",
    "  x4 = tfkl.Conv2D(512, kernel_size=3, strides=2, padding='same')(d3)\n",
    "  # Use smallest window size at bottleneck\n",
    "  bottleneck = squeeze_window_attention(x4, num_heads=8, window_size=(2, 2))\n",
    "  \n",
    "  up1 = tfkl.UpSampling2D(size=(2, 2))(bottleneck)\n",
    "  up1 = tfkl.Conv2D(512, kernel_size=3, padding='same', activation='relu')(up1)\n",
    "  up1 = tfkl.Conv2D(512, kernel_size=1, padding='same')(up1)\n",
    "  up1 = tfkl.Add()([up1, d3])\n",
    "  \n",
    "  up2 = tfkl.UpSampling2D(size=(2, 2))(up1)\n",
    "  up2 = tfkl.Conv2D(256, kernel_size=3, padding='same', activation='relu')(up2)\n",
    "  up2 = tfkl.Conv2D(256, kernel_size=1, padding='same')(up2)\n",
    "  up2 = tfkl.Add()([up2, d2])\n",
    "  \n",
    "  up3 = tfkl.UpSampling2D(size=(2, 2))(up2)\n",
    "  up3 = tfkl.Conv2D(128, kernel_size=3, padding='same', activation='relu')(up3)\n",
    "  up3 = tfkl.Conv2D(128, kernel_size=1, padding='same')(up3)\n",
    "  up3 = tfkl.Add()([up3, d1])\n",
    "  \n",
    "  up4 = tfkl.UpSampling2D(size=(2, 2))(up3)\n",
    "  up4 = tfkl.Conv2D(32, kernel_size=3, padding='same', activation='relu')(up4)\n",
    "  \n",
    "  outputs = tfkl.Conv2D(num_classes, kernel_size=1, activation='softmax')(up4)\n",
    "  \n",
    "  return tfk.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "  'U_NET': build_U_NET,\n",
    "  'U_NET_XCEPTION': build_U_NET_XCEPTION,\n",
    "  'UWNet': build_ATTENTION_UW_NET,\n",
    "  'ASPP' : build_ASPP_model,\n",
    "  'ROCKSEG' : build_RockSeg,\n",
    "  'TURKEYSEG' : build_TURKEYSEG,\n",
    "  'GROUP_NORM_UNET': build_GROUP_NORM_UNET,\n",
    "  'LIGHT4MARS' : build_LIGHT4MARS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:00:15.940614Z",
     "iopub.status.busy": "2024-11-21T21:00:15.940360Z",
     "iopub.status.idle": "2024-11-21T21:00:15.949708Z",
     "shell.execute_reply": "2024-11-21T21:00:15.948895Z",
     "shell.execute_reply.started": "2024-11-21T21:00:15.940588Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fit_model(model, cw=class_weights, data_loader=None, epochs=100, validation_data_loader=None):\n",
    "  assert(data_loader is not None)\n",
    "  assert(validation_data_loader is not None)\n",
    "  fit_history = model.fit(\n",
    "        data_loader,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_data_loader,\n",
    "        class_weight=cw,\n",
    "        callbacks=get_callbacks()\n",
    "      ).history\n",
    "  return fit_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:00:15.980359Z",
     "iopub.status.busy": "2024-11-21T21:00:15.980111Z",
     "iopub.status.idle": "2024-11-21T21:00:15.995129Z",
     "shell.execute_reply": "2024-11-21T21:00:15.994358Z",
     "shell.execute_reply.started": "2024-11-21T21:00:15.980335Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Taken from https://github.com/SeanSdahl/RangerOptimizerTensorflow/blob/master/module.py\n",
    "def build_ranger(lr=1e-3, weight_decay=0.0):\n",
    "  try:\n",
    "    import tensorflow_addons as tfa\n",
    "  except:\n",
    "    raise Exception(\"You have to install tensorflow_addons package for Ranger. Please note that this package is available up to tensorflow==2.14\")\n",
    "  def ranger(sync_period=6,\n",
    "      slow_step_size=0.5,\n",
    "      learning_rate=lr,\n",
    "      beta_1=0.9,\n",
    "      beta_2=0.999,\n",
    "      epsilon=1e-7,\n",
    "      weight_decay=weight_decay,\n",
    "      amsgrad=False,\n",
    "      sma_threshold=5.0,\n",
    "      total_steps=0,\n",
    "      warmup_proportion=0.1,\n",
    "      min_lr=0.,\n",
    "      name=\"Ranger\"):\n",
    "    inner = tfa.optimizers.RectifiedAdam(learning_rate, beta_1, beta_2, epsilon, weight_decay, amsgrad, sma_threshold, total_steps, warmup_proportion, min_lr, name)\n",
    "    optim = tfa.optimizers.Lookahead(inner, sync_period, slow_step_size, name)\n",
    "    return optim\n",
    "  return ranger()\n",
    "\n",
    "def get_optimizer(opt, batch_size, lr, **kwargs):\n",
    "  decay = opt_exp_decay_rate\n",
    "  if opt == \"SGD\":\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9 if 'momentum' not in kwargs else kwargs['momentum'])\n",
    "    if decay is not None:\n",
    "      lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "          initial_learning_rate= lr,\n",
    "          decay_steps=opt_decay_epoch_delta * (X_train.shape[0] // batch_size),  # Decay every 7 epochs\n",
    "          decay_rate=opt_exp_decay_rate,\n",
    "          staircase=True\n",
    "      )\n",
    "      optimizer.learning_rate = lr_schedule\n",
    "      print(f'Using {opt} optimizer with exp decay {decay} (momentum = {optimizer.momentum})')\n",
    "      return optimizer\n",
    "    else:\n",
    "      optimizer.learning_rate = lr\n",
    "      print(f'Using {opt} optimizer (momentum = {optimizer.momentum})')\n",
    "      return optimizer\n",
    "\n",
    "  elif opt == \"Adam\":\n",
    "    if 'weight_decay' in kwargs:\n",
    "      optimizer = tf.keras.optimizers.Adam(weight_decay=kwargs['weight_decay'])\n",
    "    else:\n",
    "      optimizer = tf.keras.optimizers.Adam()\n",
    "    if decay is not None:\n",
    "      lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "          initial_learning_rate=lr,\n",
    "          decay_steps=opt_decay_epoch_delta * (X_train.shape[0] // batch_size),  # Decay every 7 epochs\n",
    "          decay_rate=opt_exp_decay_rate,\n",
    "          staircase=True\n",
    "      )\n",
    "      optimizer.learning_rate = lr_schedule\n",
    "      print(f'Using {opt} optimizer with exp decay of {decay} weight decay = {optimizer.weight_decay}')\n",
    "      return optimizer\n",
    "    else:\n",
    "      optimizer.learning_rate = lr\n",
    "      print(f'Using {opt} optimizer (weight decay = {optimizer.weight_decay})')\n",
    "      return optimizer\n",
    "\n",
    "  elif opt == \"AdamW\":\n",
    "    if 'weight_decay' in kwargs:\n",
    "      optimizer = tf.keras.optimizers.AdamW(weight_decay=kwargs['weight_decay'])\n",
    "    else:\n",
    "      optimizer = tf.keras.optimizers.AdamW()\n",
    "    if decay is not None:\n",
    "      lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "          initial_learning_rate= lr,\n",
    "          decay_steps=opt_decay_epoch_delta * (X_train.shape[0] // batch_size),  # Decay every 7 epochs\n",
    "          decay_rate=opt_exp_decay_rate,\n",
    "          staircase=True\n",
    "      )\n",
    "      optimizer.learning_rate = lr_schedule\n",
    "      print(f'Using {opt} optimizer with exp decay of {decay} weight decay = {optimizer.weight_decay}')\n",
    "      return optimizer\n",
    "    else:\n",
    "      optimizer.learning_rate = lr\n",
    "      print(f'Using {opt} optimizer (weight decay = {optimizer.weight_decay})')\n",
    "      return optimizer\n",
    "\n",
    "  elif opt == \"Lion\":\n",
    "    if 'weight_decay' in kwargs:\n",
    "      optimizer = tf.keras.optimizers.Lion(weight_decay=kwargs['weight_decay'])\n",
    "    else:\n",
    "      optimizer = tf.keras.optimizers.Lion()\n",
    "    if decay is not None:\n",
    "      lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "          initial_learning_rate= lr,\n",
    "          decay_steps=opt_decay_epoch_delta * (X_train.shape[0] // batch_size),  # Decay every 7 epochs\n",
    "          decay_rate=opt_exp_decay_rate,\n",
    "          staircase=True\n",
    "      )\n",
    "      optimizer.learning_rate = lr_schedule\n",
    "      print(f'Using {opt} optimizer with exp decay of {decay} weight decay = {optimizer.weight_decay}')\n",
    "      return optimizer\n",
    "    else:\n",
    "      optimizer.learning_rate = lr\n",
    "      print(f'Using {opt} optimizer (weight decay = {optimizer.weight_decay})')\n",
    "      return optimizer\n",
    "  elif opt == \"Ranger\":\n",
    "    optimizer = build_ranger(lr=lr, weight_decay=0.0 if 'weight_decay' not in kwargs else kwargs['weight_decay'])\n",
    "    if decay is not None:\n",
    "      raise RuntimeError(\"Not supported\")\n",
    "    else:\n",
    "      optimizer.learning_rate = lr\n",
    "      print(f'Uusing {opt} optimizer')\n",
    "      return optimizer\n",
    "  print(f\"Starting learning rate: {lr} and batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:00:15.996454Z",
     "iopub.status.busy": "2024-11-21T21:00:15.996146Z",
     "iopub.status.idle": "2024-11-21T21:00:16.006383Z",
     "shell.execute_reply": "2024-11-21T21:00:16.005722Z",
     "shell.execute_reply.started": "2024-11-21T21:00:15.996418Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def display_model(model):\n",
    "  if not quick_run:\n",
    "    # Display a summary of the model architecture\n",
    "    model.summary(expand_nested=True)\n",
    "    # Display model architecture with layer shapes and trainable parameters\n",
    "    tfk.utils.plot_model(model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)\n",
    "  else:\n",
    "    # Just print the total parameters\n",
    "    print(f\"Total parameters: {model.count_params()/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§—ðŸ»â€â™‚ï¸ Train and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom Mean Intersection Over Union metric: the competition excludes the background class\n",
    "class MeanIntersectionOverUnion(tf.keras.metrics.MeanIoU):\n",
    "  def __init__(self, num_classes, labels_to_exclude=None, name=\"mean_iou\", dtype=None):\n",
    "    super(MeanIntersectionOverUnion, self).__init__(num_classes=num_classes, name=name, dtype=dtype)\n",
    "    if labels_to_exclude is None:\n",
    "      labels_to_exclude = [0]  # Default to excluding label 0\n",
    "    self.labels_to_exclude = labels_to_exclude\n",
    "\n",
    "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "    # Convert predictions to class labels\n",
    "    y_pred = tf.math.argmax(y_pred, axis=-1)\n",
    "\n",
    "    # Flatten the tensors\n",
    "    y_true = tf.reshape(y_true, [-1])\n",
    "    y_pred = tf.reshape(y_pred, [-1])\n",
    "\n",
    "    # Apply mask to exclude specified labels\n",
    "    for label in self.labels_to_exclude:\n",
    "      mask = tf.not_equal(y_true, label)\n",
    "      y_true = tf.boolean_mask(y_true, mask)\n",
    "      y_pred = tf.boolean_mask(y_pred, mask)\n",
    "\n",
    "    # Update the state\n",
    "    return super().update_state(y_true, y_pred, sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(fit):\n",
    "  # Plot and display training and validation loss\n",
    "  plt.figure(figsize=(18, 3))\n",
    "  plt.plot(fit['loss'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=2)\n",
    "  plt.plot(fit['val_loss'], label='Validation', alpha=0.9, color='#5a9aa5', linewidth=2)\n",
    "  plt.title('Cross Entropy')\n",
    "  plt.legend()\n",
    "  plt.grid(alpha=0.3)\n",
    "  plt.show()\n",
    "\n",
    "  # Plot and display training and validation accuracy\n",
    "  plt.figure(figsize=(18, 3))\n",
    "  plt.plot(fit['accuracy'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=2)\n",
    "  plt.plot(fit['val_accuracy'], label='Validation', alpha=0.9, color='#5a9aa5', linewidth=2)\n",
    "  plt.title('Accuracy')\n",
    "  plt.legend()\n",
    "  plt.grid(alpha=0.3)\n",
    "  plt.show()\n",
    "\n",
    "  # Plot and display training and validation mean IoU\n",
    "  plt.figure(figsize=(18, 3))\n",
    "  plt.plot(fit['mean_iou'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=2)\n",
    "  plt.plot(fit['val_mean_iou'], label='Validation', alpha=0.9, color='#5a9aa5', linewidth=2)\n",
    "  plt.title('Mean Intersection over Union')\n",
    "  plt.legend()\n",
    "  plt.grid(alpha=0.3)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_triptychs(dataset, model, num_batches=1):\n",
    "  \"\"\"\n",
    "  Plot triptychs (original image, true mask, predicted mask) for samples from a tf.data.Dataset\n",
    "\n",
    "  Parameters:\n",
    "  dataset: tf.data.Dataset - The dataset containing image-label pairs\n",
    "  model: tf.keras.Model - The trained model to generate predictions\n",
    "  num_samples: int - Number of samples to plot\n",
    "  \"\"\"\n",
    "  # Take samples from the dataset\n",
    "  samples = dataset.take(num_batches)\n",
    "\n",
    "  for images, labels in samples:\n",
    "    # Generate predictions\n",
    "    preds = model.predict(images, verbose=0)\n",
    "    preds = tf.math.argmax(preds, axis=-1)\n",
    "\n",
    "    # Create figure with subplots\n",
    "    batch_size = images.shape[0]\n",
    "    fig, axes = plt.subplots(batch_size, 3, figsize=(15, 5 * batch_size))\n",
    "\n",
    "    if batch_size == 1:\n",
    "      axes = [axes]  # Handle case where batch size is 1\n",
    "\n",
    "    # Define custom colors for each class\n",
    "    import matplotlib.colors as mcolors\n",
    "    class_colors = [\"purple\", \"blue\", \"green\", \"orange\", \"yellow\"]  # Define your colors\n",
    "    cmap = mcolors.ListedColormap(class_colors)\n",
    "\n",
    "    # Define normalization to map class values to the color map\n",
    "    bounds = np.arange(num_classes + 1) - 0.5  # Create boundaries for each class\n",
    "    norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "      # Plot original image\n",
    "      axes[i][0].set_title(\"Original Image\")\n",
    "      axes[i][0].imshow(images[i])\n",
    "      axes[i][0].axis('off')\n",
    "\n",
    "      # Plot original mask\n",
    "      axes[i][1].set_title(\"Original Mask\")\n",
    "      axes[i][1].imshow(labels[i], cmap=cmap, alpha=0.8)\n",
    "      axes[i][1].axis('off')\n",
    "\n",
    "      # Plot predicted mask\n",
    "      axes[i][2].set_title(\"Predicted Mask\")\n",
    "      axes[i][2].imshow(preds[i], cmap=cmap, alpha=0.8)\n",
    "      axes[i][2].axis('off')\n",
    "\n",
    "    # Add a colorbar\n",
    "    cbar = fig.colorbar(\n",
    "      plt.cm.ScalarMappable(cmap=cmap, norm=norm),\n",
    "      ax=axes[:, 1],  # Align the colorbar with the mask columns\n",
    "      orientation=\"vertical\",\n",
    "      fraction=0.02,\n",
    "      pad=0.04\n",
    "    )\n",
    "    cbar.set_ticks(np.arange(num_classes))  # Set tick locations\n",
    "    cbar.set_ticklabels([f\"Class {i}\" for i in range(num_classes)])  # Set tick labels\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_combined_model(X_train, y_train, X_val, y_val, bg_label=0):\n",
    "  # Convert masks to binary (1: Background, 0: Others)\n",
    "  binary_masks_train = (y_train == bg_label).astype(np.float32)\n",
    "  binary_masks_val = (y_val == bg_label).astype(np.float32)\n",
    "\n",
    "  return (X_train, binary_masks_train, y_train), (X_val, binary_masks_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_models(binary_model, multi_model, input_shape):\n",
    "  inputs = tfkl.Input(shape=input_shape)\n",
    "\n",
    "  # Binary segmentation output\n",
    "  binary_output = binary_model(inputs)\n",
    "  binary_mask = tf.round(binary_output)  # Convert probabilities to binary mask\n",
    "\n",
    "  # Multi-class segmentation output\n",
    "  multi_output = multi_model(inputs)\n",
    "\n",
    "  # Mask the main output using the binary mask\n",
    "  background_class = tf.zeros_like(multi_output[..., :1])  # Background as zeros\n",
    "  combined_output = tf.where(tf.expand_dims(binary_mask, axis=-1) > 0.5, background_class, multi_output)\n",
    "\n",
    "  return tf.keras.Model(inputs, combined_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_X, train_y, val_X, val_y, override_loss = None):\n",
    "  miou = MeanIntersectionOverUnion(num_classes=NUM_CLASSES, labels_to_exclude=[0])\n",
    "  histories = []\n",
    "\n",
    "  for _, ts in enumerate(training_schedule):\n",
    "    model.compile(loss=get_loss() if override_loss is None else override_loss,\n",
    "                  optimizer=get_optimizer(batch_size=ts['batch_size'], lr = ts['lr'], opt=ts['opt_name']), \n",
    "                  metrics=['accuracy', miou])\n",
    "    display_model(model)\n",
    "    val_data_loader = get_dataset(val_X, val_y, batch_size=ts['batch_size'])\n",
    "    # Get dataset with aug\n",
    "    if ts['augmentation']:\n",
    "      if ts.get('enlarge_dataset_with_custom_np_ds', False):\n",
    "        # This example enlarges the images with a lot of bg labels (>=65%)\n",
    "        bg_augmented_data = get_enlarged_dataset(bg_imgs, bg_labels, build_augmentation_bg, repetitions=4)\n",
    "        # You can add additional data and then using additional_ds_concat=[bg_augmented_data, ...]\n",
    "        data_loader = get_dataset(train_X, train_y, \n",
    "                                  batch_size=ts['batch_size'], \n",
    "                                  augmentations=apply_augmentation,\n",
    "                                  augmentation_repetition=ts['augmentation_repetition'], \n",
    "                                  concat_and_shuffle_aug_with_no_aug=ts['include_non_augmented'], additional_ds_concat=[bg_augmented_data])\n",
    "        print(f\"Training with augmentation x{ts['augmentation_repetition']}, enlarge_dataset_with_custom_np_ds and {'none ' if not ts['include_non_augmented'] else ''}non-augmented data.\")\n",
    "      else:\n",
    "        data_loader = get_dataset(train_X, train_y, \n",
    "                                  batch_size=ts['batch_size'], \n",
    "                                  augmentations=apply_augmentation,\n",
    "                                  augmentation_repetition=ts['augmentation_repetition'], \n",
    "                                  concat_and_shuffle_aug_with_no_aug=ts['include_non_augmented'])\n",
    "        print(f\"Training with augmentation x{ts['augmentation_repetition']} and {'none ' if not ts['include_non_augmented'] else ''}non-augmented data.\")\n",
    "    # Get dataset with no aug\n",
    "    else:\n",
    "      data_loader = get_dataset(train_X, train_y, batch_size=ts['batch_size'])\n",
    "      print(f'Fitting model without augmentation')\n",
    "\n",
    "    fit_history = fit_model(model, \n",
    "                            data_loader=data_loader, \n",
    "                            validation_data_loader=val_data_loader,\n",
    "                            epochs = ts['epochs'])\n",
    "    histories.append(fit_history)\n",
    "\n",
    "  # Calculate and print the final validation accuracy\n",
    "  final_val_meanIoU = round(max(histories[-1]['val_mean_iou']) * 100, 2)\n",
    "  print(f'Final validation Mean Intersection Over Union: {final_val_meanIoU}%')\n",
    "\n",
    "  # Save intermediate model\n",
    "  model_filename = f'{model_name}-{str(final_val_meanIoU)}-{datetime.now().strftime(\"%y%m%d_%H%M\")}.keras'\n",
    "  model.save(model_filename)\n",
    "\n",
    "  # Free memory by deleting the model instance\n",
    "  if FREE_MODEL:\n",
    "    del model\n",
    "  return histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model = model_dict[model_name](IMG_SIZE + (1,), NUM_CLASSES, binary=True)\n",
    "multi_model = model_dict[model_name](IMG_SIZE + (1,), NUM_CLASSES, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = prepare_dataset_combined_model(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot(X_train[:10], mask=y_train[:10], num_images=4, num_cls=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here bg will have label 1! (inverted colors)\n",
    "plot(train_data[0][:10], mask=train_data[1][:10], num_images=4, num_cls=2, colors=['blue', 'purple'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0].shape, y_train[0].shape, train_data[0][0].shape, train_data[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_binary = train_model(binary_model, train_data[0], train_data[1], val_data[0], val_data[1], override_loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on binary test\n",
    "binary_X_test = X_test\n",
    "binary_y_test = (y_test == 0).astype(np.float32)\n",
    "test_dataset = get_dataset(binary_X_test, binary_y_test, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the test set and print the results\n",
    "test_loss, test_accuracy, test_mean_iou = binary_model.evaluate(test_dataset, verbose=1)\n",
    "print(f'Test Accuracy: {round(test_accuracy, 4)}')\n",
    "print(f'Test Mean Intersection over Union: {round(test_mean_iou, 4)}')\n",
    "\n",
    "# Call the function with your dataset and model\n",
    "plot_triptychs(test_dataset, binary_model, num_batches=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hist_multi = train_model(multi_model, train_data[0][0], train_data[0][2], train_data[1][0], train_data[1][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœðŸ¿ Make prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = get_dataset(X_test, y_test, batch_size=8)\n",
    "\n",
    "if model_filename_override:\n",
    "  model = tfk.models.load_model(model_filename_override, compile=False)\n",
    "  model.compile(\n",
    "    loss=get_loss(),\n",
    "    optimizer=get_optimizer(),\n",
    "    metrics=[\"accuracy\", miou]\n",
    "  )\n",
    "  display_model(model)\n",
    "\n",
    "# Evaluate the model on the test set and print the results\n",
    "test_loss, test_accuracy, test_mean_iou = model.evaluate(test_dataset, verbose=1)\n",
    "print(f'Test Accuracy: {round(test_accuracy, 4)}')\n",
    "print(f'Test Mean Intersection over Union: {round(test_mean_iou, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸŽ° Make prediction on competition test set and create csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not loading the model but using the python env model as there is a current error on the `MeanIntersectionOverUnion` class which is not serializable making the model not loadable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(hidden_X_test.shape)\n",
    "preds = model.predict(hidden_X_test)\n",
    "preds = np.argmax(preds, axis=-1)\n",
    "print(f\"Predictions shape: {preds.shape}\")\n",
    "\n",
    "def y_to_df(y) -> pd.DataFrame:\n",
    "  \"\"\"Converts segmentation predictions into a DataFrame format for Kaggle.\"\"\"\n",
    "  n_samples = len(y)\n",
    "  y_flat = y.reshape(n_samples, -1)\n",
    "  df = pd.DataFrame(y_flat)\n",
    "  df[\"id\"] = np.arange(n_samples)\n",
    "  cols = [\"id\"] + [col for col in df.columns if col != \"id\"]\n",
    "  return df[cols]\n",
    "\n",
    "submission_filename = f'submission_{datetime.now().strftime(\"%y%m%d_%H%M\")}.csv'\n",
    "submission_df = y_to_df(preds)\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "print('Submission saved in', submission_filename)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPhN8z97sycURDAjAFsp+EI",
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6047840,
     "sourceId": 9855285,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6047865,
     "sourceId": 9855319,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 159775,
     "modelInstanceId": 137060,
     "sourceId": 161170,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 162416,
     "modelInstanceId": 139795,
     "sourceId": 164339,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 164634,
     "modelInstanceId": 142056,
     "sourceId": 166955,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 165351,
     "modelInstanceId": 142773,
     "sourceId": 167821,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 170852,
     "modelInstanceId": 148341,
     "sourceId": 174251,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "anndl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
