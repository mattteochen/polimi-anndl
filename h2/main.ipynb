{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuwVgG3Vbbka"
   },
   "source": [
    "# Artificial Neural Networks and Deep Learning\n",
    "\n",
    "---\n",
    "## Homework 2: Image segmentation of Mars' stones\n",
    "## Team: The Backpropagators\n",
    "Arianna Procaccio, Francesco Buccoliero, Kai-Xi Matteo Chen, Luca Capoferri\n",
    "\n",
    "ariii, frbuccoliero, kaiximatteoc, luke01\n",
    "\n",
    "246843, 245498, 245523, 259617\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7IqZP5Iblna"
   },
   "source": [
    "## âš™ï¸ Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T20:59:48.162042Z",
     "iopub.status.busy": "2024-11-21T20:59:48.161808Z",
     "iopub.status.idle": "2024-11-21T20:59:48.172447Z",
     "shell.execute_reply": "2024-11-21T20:59:48.171556Z",
     "shell.execute_reply.started": "2024-11-21T20:59:48.162018Z"
    },
    "id": "CO6_Ft_8T56A",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.ndimage import distance_transform_edt as distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš™ï¸ Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 666\n",
    "\n",
    "train_ratio = 0.85\n",
    "validation_ratio = 0.10\n",
    "test_ratio = 0.05\n",
    "\n",
    "IMG_SIZE = (64, 128)\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "running_on = \"local\" # local | colab | kaggle\n",
    "\n",
    "quick_run = True # If true skips the early plotting stuff\n",
    "\n",
    "model_name = 'TURKEYSEG' # U_NET | U_NET_XCEPTION | UWNet | ASPP | ROCKSEG | TURKEYSEG\n",
    "\n",
    "# Training schedule\n",
    "training_schedule = [\n",
    "\t# E.g. a first run over non-augmented data\n",
    "\t{\n",
    "\t\t\"augmentation\" : False,\n",
    "\t\t\"epochs\": 100,\n",
    "\t\t\"batch_size\": 16,\n",
    "\t\t\"lr\": 1e-4,\n",
    "\t\t\"opt_name\": \"AdamW\", # SGD | Adam | AdamW | Lion | Ranger\n",
    "\t},\n",
    "\t# Second run over augmented data\n",
    "\t{\n",
    "\t\t\"augmentation\" : True,\n",
    "\t\t\"augmentation_repetition\": 3,\n",
    "\t\t#\"enlarge_dataset_with_custom_np_ds\": True,\n",
    "\t\t\"include_non_augmented\": False,\n",
    "\t\t\"epochs\": 1000,\n",
    "\t\t\"batch_size\": 32,\n",
    "\t\t\"lr\": 3e-4,\n",
    "\t\t\"opt_name\": \"AdamW\", # SGD | Adam | AdamW | Lion | Ranger\n",
    "\t}\n",
    "]\n",
    "\n",
    "# Exponential decay\n",
    "opt_exp_decay_rate: float | None = None \n",
    "opt_decay_epoch_delta = 7 # Number of epochs between each decay, if above None is not used\n",
    "\n",
    "USE_CLASS_WEIGHTS = False # If true the model will use class weights to balance the dataset\n",
    "\n",
    "loss_fn = 'sparse_categorical_crossentropy'\n",
    "\n",
    "FREE_MODEL = False # If true the model is deleted from memory after being dumped to file\n",
    "\n",
    "model_filename_override = None # If not None will load the model from this file and perform inference\n",
    "\n",
    "DATASET_PATH_LOCAL = \"dataset.npz\"\n",
    "DATASET_PATH_COLAB = \"/content/drive/MyDrive/Colab Notebooks/dataset.npz\"\n",
    "DATASET_PATH_KAGGLE = \"/kaggle/input/dataset-h2/dataset.npz\"\n",
    "\n",
    "OUTLIER_MASK_LOCAL = \"outlier_mask.npy\"\n",
    "OUTLIER_MASK_COLAB = \"/content/drive/MyDrive/Colab Notebooks/outlier_mask.npy\"\n",
    "OUTLIER_MASK_KAGGLE = \"/kaggle/input/dataset-h2/outlier_mask.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define here the Albumentation pipeline to be used for augmentation\n",
    "\n",
    "def build_augmentation():\n",
    "\ttransform = A.Compose([\n",
    "\t\t\t\t\tA.RandomRotate90(p=0.7),  # Random 90-degree rotation\n",
    "\t\t\t\t\tA.HorizontalFlip(p=0.7),  # Horizontal flip for diverse texture representation\n",
    "\t\t\t\t\tA.VerticalFlip(p=0.7),  # Vertical flip to simulate different orientations\n",
    "\t\t\t\t\tA.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.7),  # Adjust brightness and contrast\n",
    "\t\t\t\t\tA.GaussianBlur(blur_limit=3, p=0.7),  # Add blur to simulate camera effects\n",
    "\t\t\t\t\tA.CoarseDropout(max_holes=4, max_height=16, max_width=16, p=0.7),  # Randomly occlude parts of the image\n",
    "\t\t\t\t\tA.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=45, p=0.7),  # Random shifts, scales, and rotations\n",
    "\t\t\t\t\tA.ElasticTransform(alpha=1, sigma=50, p=0.7),\n",
    "\t\t\t\t\tA.GridDistortion(num_steps=5, distort_limit=0.3, p=0.7),\n",
    "\t\t\t\t\tA.OpticalDistortion(distort_limit=0.2, shift_limit=0.2, p=0.7),\n",
    "\t\t\t\t\tA.Resize(height=IMG_SIZE[0], width=IMG_SIZE[1], p=1),  # Resize for consistent input size\n",
    "\t\t\t])\n",
    "\n",
    "\t# For Matteo, do not delete\n",
    "\t#transform = A.Compose([\n",
    "\t#  A.RandomRotate90(p=0.7),  # Random 90-degree rotation\n",
    "\t#  A.HorizontalFlip(p=0.7),  # Horizontal flip for diverse texture representation\n",
    "\t#  A.VerticalFlip(p=0.7),  # Vertical flip to simulate different orientations\n",
    "\t#  A.ElasticTransform(alpha=50, sigma=50, p=0.7),\n",
    "\t#  A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.7),\n",
    "\t#  A.Resize(height=IMG_SIZE[0], width=IMG_SIZE[1], p=1),  # Resize for consistent input size\n",
    "\t#])\n",
    "\t\n",
    "\treturn transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom augmentation for some specific targets (e.g. images with a lot of background)\n",
    "def build_augmentation_bg():\n",
    "\ttransform = A.Compose([\n",
    "\t\t  A.RandomRotate90(p=0.8),  # Random 90-degree rotation\n",
    "\t\t  A.HorizontalFlip(p=0.8),  # Horizontal flip for diverse texture representation\n",
    "\t\t  A.VerticalFlip(p=0.8),  # Vertical flip to simulate different orientations\n",
    "\t\t  A.ElasticTransform(alpha=10, sigma=50, p=0.5),\n",
    "\t\t  A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
    "\t\t  A.Resize(height=IMG_SIZE[0], width=IMG_SIZE[1], p=1),  # Resize for consistent input size\n",
    "\t  ])\n",
    "\treturn transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading configurations based on the given settings\n",
    "assert train_ratio + validation_ratio + test_ratio == 1\n",
    "\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "DATASET_PATH = DATASET_PATH_LOCAL if running_on == \"local\" else DATASET_PATH_COLAB if running_on == \"colab\" else DATASET_PATH_KAGGLE\n",
    "OUTLIER_MASK = OUTLIER_MASK_LOCAL if running_on == \"local\" else OUTLIER_MASK_COLAB if running_on == \"colab\" else OUTLIER_MASK_KAGGLE\n",
    "\n",
    "data = np.load(DATASET_PATH)\n",
    "outlier_mask_template = np.load(OUTLIER_MASK) # discovered by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GN_cpHlSboXV"
   },
   "source": [
    "## â³ Load, inspect and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = data[\"training_set\"]\n",
    "X_train = training_set[:, 0]\n",
    "y_train = training_set[:, 1]\n",
    "\n",
    "hidden_X_test = data[\"test_set\"]\n",
    "\n",
    "print(f\"Training X shape: {X_train.shape}\")\n",
    "print(f\"Training y shape: {y_train.shape}\")\n",
    "print(f\"Test hidden X shape: {hidden_X_test.shape}\")\n",
    "\n",
    "# Add color channel and rescale pixels between 0 and 1\n",
    "X_train = X_train[..., np.newaxis] / 255\n",
    "X_train = X_train.astype(np.float32)\n",
    "hidden_X_test = hidden_X_test[..., np.newaxis] / 255\n",
    "hidden_X_test = hidden_X_test.astype(np.float32)\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Split train and validation\n",
    "validation_size = int(X_train.shape[0] * validation_ratio)\n",
    "\n",
    "indices = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "X_train = X_train[indices]\n",
    "y_train = y_train[indices]\n",
    "\n",
    "# Define train and validation indices\n",
    "split_indices = [int(X_train.shape[0] * train_ratio), int(X_train.shape[0] * (train_ratio + validation_ratio))]\n",
    "\n",
    "X_train, X_val, X_test = np.split(X_train, split_indices)\n",
    "y_train, y_val, y_test = np.split(y_train, split_indices)\n",
    "\n",
    "print(\"======= BEFORE REMOVING OUTLIERS =======\")\n",
    "print(f\"Training X shape: {X_train.shape}\")\n",
    "print(f\"Training y shape: {y_train.shape}\")\n",
    "print(f\"Validation X shape: {X_val.shape}\")\n",
    "print(f\"Validation y shape: {y_val.shape}\")\n",
    "print(f\"Test X shape: {X_test.shape}\")\n",
    "print(f\"Test y shape: {y_test.shape}\")\n",
    "\n",
    "# Outliers share the mask\n",
    "train_outliers_indices = [i for i, img in enumerate(y_train) if not np.array_equal(img, outlier_mask_template)]\n",
    "val_outliers_indices = [i for i, img in enumerate(y_val) if not np.array_equal(img, outlier_mask_template)]\n",
    "test_outliers_indices = [i for i, img in enumerate(y_test) if not np.array_equal(img, outlier_mask_template)]\n",
    "print(f'Total outliers in train set: {y_train.shape[0] - len(train_outliers_indices)}')\n",
    "print(f'Total outliers in validation set: {y_val.shape[0] - len(val_outliers_indices)}')\n",
    "print(f'Total outliers in test set: {y_test.shape[0] - len(test_outliers_indices)}')\n",
    "\n",
    "# Remove outlier from train and validation set\n",
    "X_train = X_train[train_outliers_indices]\n",
    "y_train = y_train[train_outliers_indices]\n",
    "X_val = X_val[val_outliers_indices]\n",
    "y_val = y_val[val_outliers_indices]\n",
    "X_test = X_test[test_outliers_indices]\n",
    "y_test = y_test[test_outliers_indices]\n",
    "\n",
    "print(\"======= AFTER REMOVING OUTLIERS =======\")\n",
    "print(f'Updated train dataset size: {X_train.shape}')\n",
    "print(f'Updated validation dataset size: {X_val.shape}')\n",
    "print(f'Updated test dataset size: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve images with a lot of label labels\n",
    "def retrieve_imgs_of_label(X, y, bg_label_min_percentage=0.9, label=0):\n",
    "\ttot_pixels = IMG_SIZE[0] * IMG_SIZE[1]\n",
    "\timgs = []\n",
    "\tlabels = []\n",
    "\t# Count pixels for each class\n",
    "\tfor label_image, img in zip(y, X):\n",
    "\t\tunique, counts = np.unique(label_image, return_counts=True)\n",
    "\t\tfor u, c in zip(unique, counts):\n",
    "\t\t\t#print(u, c)\n",
    "\t\t\t#if (int(u) == 0):\n",
    "\t\t\t#  print('z', c)\n",
    "\t\t\tif int(u) == label and c / tot_pixels >= bg_label_min_percentage:\n",
    "\t\t\t\timgs.append(img)\n",
    "\t\t\t\tlabels.append(label_image)\n",
    "\treturn np.array(imgs), np.array(labels)\n",
    "\n",
    "bg_imgs, bg_labels = retrieve_imgs_of_label(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data. The number of images being displayed are rows X cols\n",
    "def plot(data, mask=None, num_images=10, rows=4, cols=8):\n",
    "\t# Reshape if needed (e.g., remove channel dimension for grayscale images)\n",
    "\tif data.shape[-1] == 1:  # Grayscale case\n",
    "\t\tdata = data.squeeze(axis=-1)  # Remove channel dimension\n",
    "\t\n",
    "\tif mask is None:\n",
    "\t\t# Plot settings\n",
    "\t\t_, axes = plt.subplots(rows, cols, figsize=(12, 6))  # Adjust figure size as needed\n",
    "\t\n",
    "\t\t# Display images\n",
    "\t\tfor i, ax in enumerate(axes.flat):\n",
    "\t\t\tif i < len(data):  # Check if there are enough images\n",
    "\t\t\t\tax.imshow(data[i], cmap='gray' if len(data[i].shape) == 2 else None)\n",
    "\t\t\t\tax.axis('off')  # Hide axes\n",
    "\t\t\telse:\n",
    "\t\t\t\tax.axis('off')  # Hide any empty subplot\n",
    "\t\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.show()\n",
    "\telse:\n",
    "\t\tnum_samples = num_images  # Number of images to display\n",
    "\t\tif num_samples < 4:\n",
    "\t\t\tnum_samples = 4\n",
    "\n",
    "\t\t# Plot settings\n",
    "\t\tfig, axes = plt.subplots(num_samples, 2, figsize=(8, num_samples * 2))\n",
    "\n",
    "\t\tfor i in range(num_samples):\n",
    "\t\t\t# Original image\n",
    "\t\t\taxes[i, 0].imshow(data[i], cmap=\"gray\")\n",
    "\t\t\taxes[i, 0].set_title(f\"Image {i+1}\")\n",
    "\t\t\taxes[i, 0].axis(\"off\")\n",
    "\n",
    "\t\t\t# Corresponding mask\n",
    "\t\t\taxes[i, 1].imshow(mask[i], cmap=\"viridis\", alpha=0.8)  # Adjust cmap as needed\n",
    "\t\t\taxes[i, 1].set_title(f\"Mask {i+1}\")\n",
    "\t\t\taxes[i, 1].axis(\"off\")\n",
    "\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not quick_run:\n",
    "\tplot(X_train, rows=10, cols=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An additional check: you should not see any outlier\n",
    "if not quick_run:\n",
    "\tplot(X_train, mask=y_train, num_images=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_class_distribution(label_dataset, num_classes):\n",
    "\t# Initialize counts for each class\n",
    "\tclass_counts = np.zeros(num_classes)\n",
    "\n",
    "\t# Count pixels for each class\n",
    "\tfor label_image in label_dataset:\n",
    "\t\t\tunique, counts = np.unique(label_image, return_counts=True)\n",
    "\t\t\tfor u, c in zip(unique, counts):\n",
    "\t\t\t\t\tclass_counts[u] += c\n",
    "\n",
    "\t# Normalize counts (percentage)\n",
    "\ttotal_pixels = np.sum(class_counts)\n",
    "\tclass_distribution = class_counts / total_pixels * 100\n",
    "\n",
    "\t# Print and visualize\n",
    "\tprint(\"Class Distribution (% of pixels):\")\n",
    "\tfor i in range(num_classes):\n",
    "\t\t\tprint(f\"Class {i}: {class_distribution[i]:.2f}%\")\n",
    "\t\n",
    "\tif not quick_run:\n",
    "\t\t# Plot class distribution\n",
    "\t\tplt.bar(range(num_classes), class_distribution, tick_label=[f\"Class {i}\" for i in range(num_classes)])\n",
    "\t\tplt.xlabel(\"Classes\")\n",
    "\t\tplt.ylabel(\"Percentage of Pixels\")\n",
    "\t\tplt.title(\"Class Distribution\")\n",
    "\t\tplt.show()\n",
    "\t\n",
    "\treturn class_distribution\n",
    "\n",
    "def get_class_weights(class_distribution):\n",
    "\t# Convert percentage to class probabilities\n",
    "\tclass_probabilities = np.array(class_distribution) / 100.0\n",
    "\n",
    "\t# Calculate class weights (inverse of class probability)\n",
    "\tclass_weights = 1.0 / class_probabilities\n",
    "\n",
    "\t# Normalize weights (optional, you can skip normalization if desired)\n",
    "\tmax_weight = np.max(class_weights)\n",
    "\tclass_weights = class_weights / max_weight  # Normalize to have the maximum weight = 1\n",
    "\n",
    "\treturn {i:w for i,w in enumerate(class_weights)}\n",
    "\n",
    "# Check for 5 classes (class IDs: 0-4)\n",
    "class_distribution = check_class_distribution([e.astype(np.int8) for e in y_train], num_classes=num_classes)\n",
    "class_weights = get_class_weights(class_distribution) if USE_CLASS_WEIGHTS else {i:1.0 for i in range(NUM_CLASSES)}\n",
    "print('Class weights:', class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `concat_and_shuffle_aug_with_no_aug` will double the X_train size\n",
    "# `remove_bg` will set all the bg pixels to dark\n",
    "# `augmentation_repetition` will concatenate n times the augmented dataset by applying the same `augmentations` fn. Useful for augmentation pipeline with probability activations\n",
    "def get_dataset(X, y, batch_size=32, augmentations=None, augmentation_repetition=1, **kwargs):\n",
    "\n",
    "\tdef resize_img_and_mask(img, mask):\n",
    "\t\tinput_img = tf.image.resize(img, IMG_SIZE)\n",
    "\t\tinput_img = tf.cast(input_img, tf.float32)\n",
    "\n",
    "\t\t# Resize needs at least 3 dims, add a dummy one\n",
    "\t\tif kwargs.get('one_hot', False):\n",
    "\t\t\tmask = tf.cast(mask, tf.int32)\n",
    "\t\t\ttarget_img = tf.one_hot(mask, depth=num_classes, axis=-1)\n",
    "\t\telse:\n",
    "\t\t\ttarget_img = tf.expand_dims(mask, axis=-1)\n",
    "\t\t# Nearest-neighbor is essential for resizing segmentation masks because it preserves the discrete class labels (e.g., 0, 1, 2) without introducing unintended values due to interpolation\n",
    "\t\ttarget_img = tf.image.resize(target_img, IMG_SIZE, method=\"nearest\")\n",
    "\t\ttarget_img = tf.cast(target_img, tf.int32) # Consider lower integers\n",
    "\n",
    "\t\treturn input_img, target_img\n",
    "\n",
    "\tdef remove_background(image, mask, background_label=0):\n",
    "\t\tbackground_mask = (mask == background_label)\n",
    "\t\timage[background_mask] = 0  # Set to black\n",
    "\t\treturn image, mask\n",
    "\n",
    "\tdef apply_augmentation_np():\n",
    "\t\tX_a = []\n",
    "\t\ty_a = []\n",
    "\t\tfor i, m in zip(X, y):\n",
    "\t\t\taug_img, aug_mask = augmentations(i, m)\n",
    "\t\t\tif kwargs.get('remove_bg', False):\n",
    "\t\t\t\taug_img, aug_mask = remove_background(aug_img, aug_mask)\n",
    "\t\t\tX_a.append(aug_img)  \n",
    "\t\t\ty_a.append(aug_mask)  \n",
    "\t\treturn np.array(X_a), np.array(y_a)\n",
    "\t\n",
    "\tdef to_one_hot(x, y):\n",
    "\t\ty_one_hot = tf.one_hot(y, depth=num_classes, axis=-1)\n",
    "\t\treturn x, y_one_hot\n",
    "\n",
    "\tif kwargs.get('remove_bg', False):\n",
    "\t\tX_a = []\n",
    "\t\ty_a = []\n",
    "\t\tfor i, m in zip(X, y):\n",
    "\t\t\taug_img, aug_mask = remove_background(i, m)\n",
    "\t\t\tX_a.append(aug_img)\n",
    "\t\t\ty_a.append(aug_mask)\n",
    "\t\tX = np.array(X_a)\n",
    "\t\ty = np.array(y_a)\n",
    "\n",
    "\t# Apply augmentations before converting to dataset (this will be serial I think but we avoid type conversions as A works on np arrays)\n",
    "\tif augmentations is not None:\n",
    "\t\tX_a, y_a = apply_augmentation_np()\n",
    "\t\tdataset = tf.data.Dataset.from_tensor_slices((X_a, y_a))\n",
    "\t\tif augmentation_repetition > 1:\n",
    "\t\t\tfor i in range(augmentation_repetition-1):\n",
    "\t\t\t\tX_a, y_a = apply_augmentation_np()\n",
    "\t\t\t\tdataset = dataset.concatenate(tf.data.Dataset.from_tensor_slices((X_a, y_a)))\n",
    "\t\tadd_len = 0\n",
    "\t\tif kwargs.get('additional_ds_concat', None):\n",
    "\t\t\t# Optimistic\n",
    "\t\t\tadd_len = len(kwargs['additional_ds_concat'])\n",
    "\t\t\tfor pair in kwargs['additional_ds_concat']:\n",
    "\t\t\t\tprint('concatenating additional ds')\n",
    "\t\t\t\timages, labels = pair\n",
    "\t\t\t\timages = images.astype(np.float32)\n",
    "\t\t\t\tdataset = dataset.concatenate(tf.data.Dataset.from_tensor_slices((images, labels)))\n",
    "\t\tif kwargs.get('concat_and_shuffle_aug_with_no_aug', False):\n",
    "\t\t\tdataset = dataset.concatenate(tf.data.Dataset.from_tensor_slices((X, y)))\n",
    "\t\t\tdataset = dataset.shuffle(seed=seed, buffer_size=X.shape[0] * (augmentation_repetition+1+add_len))\n",
    "\t\telse:\n",
    "\t\t\tdataset = dataset.shuffle(seed=seed, buffer_size=X.shape[0] * (augmentation_repetition+add_len))\n",
    "\n",
    "\telse:\n",
    "\t\tdataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "\n",
    "\tdataset = dataset.map(resize_img_and_mask, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\tdataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\treturn dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enlarge dataset containing images provided in the inputs\n",
    "def get_enlarged_dataset(imgs, labels, aug_fn, repetitions=2, unbatch=True, np_ds=True):\n",
    "\tdef apply_aug(img, mask):\n",
    "\t\ttransform = aug_fn()\n",
    "\t\ttransformed = transform(image=img, mask=mask)\n",
    "\t\treturn transformed[\"image\"], transformed[\"mask\"]\n",
    "\tif np_ds:\n",
    "\t\tX_a = []\n",
    "\t\ty_a = []\n",
    "\t\tfor i, m in zip(imgs, labels):\n",
    "\t\t\taug_img, aug_mask = apply_aug(i, m)\n",
    "\t\t\tX_a.append(aug_img)  \n",
    "\t\t\ty_a.append(aug_mask)  \n",
    "\t\treturn np.array(X_a), np.array(y_a)\n",
    "\telse:\n",
    "\t\tenlarged_bg_dataset = get_dataset(imgs, labels, augmentations=apply_aug, augmentation_repetition=repetitions)\n",
    "\t\t# We unbatch as this will be concatenated with other ds\n",
    "\t\tif unbatch:\n",
    "\t\t\tds = enlarged_bg_dataset.unbatch()\n",
    "\t\treturn ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the enlarged bg images dataset as example\n",
    "if not quick_run:\n",
    "\tN = 10\n",
    "\ta, b =  get_enlarged_dataset(bg_imgs[:N], bg_labels[:N], build_augmentation_bg, np_ds=True)\n",
    "\tprint(a.shape, b.shape)\n",
    "\tplot(a, mask=b, num_images=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸŽ² Define training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_signed_distance_map(mask):\n",
    "    \n",
    "    # Calcola la distanza esterna (distanza dai bordi del foreground)\n",
    "    inv_mask = 1 - mask\n",
    "    dist_out = tf.cast(tf.nn.relu(tf.image.sobel_edges(inv_mask)), tf.float32)\n",
    "\n",
    "    # Calcola la distanza interna (distanza dai bordi del background)\n",
    "    dist_in = tf.cast(tf.nn.relu(tf.image.sobel_edges(mask)), tf.float32)\n",
    "\n",
    "    # Combina le distanze in un unico gradiente\n",
    "    dx_out, dy_out = dist_out[..., 0], dist_out[..., 1]\n",
    "    dx_in, dy_in = dist_in[..., 0], dist_in[..., 1]\n",
    "\n",
    "    dist_out_combined = tf.sqrt(tf.square(dx_out) + tf.square(dy_out))\n",
    "    dist_in_combined = tf.sqrt(tf.square(dx_in) + tf.square(dy_in))\n",
    "\n",
    "    # Signed Distance Map: negativo all'interno, positivo all'esterno\n",
    "    sdm = dist_out_combined - dist_in_combined\n",
    "    return sdm\n",
    "\n",
    "def boundary_loss(y_true, y_pred):\n",
    "    # Calcola la Signed Distance Map\n",
    "    sdm = compute_signed_distance_map(y_true)\n",
    "    \n",
    "    # Normalizza le previsioni\n",
    "    y_pred = tf.nn.softmax(y_pred, axis=-1)\n",
    "\n",
    "    # Calcola la boundary loss\n",
    "    loss = tf.reduce_mean(tf.abs(sdm * (y_true - y_pred)))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
    "  # Convert y_true to one-hot if needed\n",
    "  # TODO: should we retrieve the argmax and use 1 channels instead of 5?\n",
    "  if y_true.shape[-1] != y_pred.shape[-1]:\n",
    "      y_true = tf.one_hot(tf.cast(y_true[..., 0], tf.int32), depth=y_pred.shape[-1])\n",
    "  \n",
    "  # Compute Dice Loss per class\n",
    "  intersection = tf.reduce_sum(y_true * y_pred, axis=(1, 2))\n",
    "  union = tf.reduce_sum(y_true + y_pred, axis=(1, 2))\n",
    "  dice = (2. * intersection + smooth) / (union + smooth)\n",
    "  \n",
    "  # Average Dice Loss over all classes and batch\n",
    "  dice_loss = 1 - tf.reduce_mean(dice)\n",
    "  \n",
    "  return dice_loss\n",
    "\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "        alpha = 0.3\n",
    "        beta = 0.3\n",
    "        gamma = 1 - alpha - beta\n",
    "\n",
    "        loss_sparse = tfk.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "        loss_dice = dice_loss(y_true, y_pred)\n",
    "    \n",
    "        if y_true.shape[-1] != y_pred.shape[-1]:\n",
    "            y_true = tf.one_hot(tf.cast(y_true[..., 0], tf.int32), depth=y_pred.shape[-1])\n",
    "        loss_boundary = boundary_loss(y_true, y_pred)\n",
    "        focal_loss = tfk.losses.CategoricalFocalCrossentropy() \n",
    "        loss_focal = focal_loss(y_true, y_pred)\n",
    "\n",
    "        return (\n",
    "            alpha * loss_dice +\n",
    "            beta * loss_focal +\n",
    "            gamma * loss_boundary\n",
    "        )\n",
    "#if you want to use combined_loss        \n",
    "#loss_fn = combined_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization callback\n",
    "category_map = {\n",
    "\t0: 0, # Background,\n",
    "\t1: 1, # Soil,\n",
    "\t2: 2, # Bedrock,\n",
    "\t3: 3, # Sand,\n",
    "\t4: 4, # Big Rock,\n",
    "}\n",
    "\n",
    "def apply_category_mapping(label):\n",
    "\t\"\"\"\n",
    "\tApply category mapping to labels.\n",
    "\t\"\"\"\n",
    "\tprint(\"Label dtype before mapping:\", label.dtype)\n",
    "\tkeys_tensor = tf.constant(list(category_map.keys()), dtype=tf.int32)\n",
    "\tvals_tensor = tf.constant(list(category_map.values()), dtype=tf.int32)\n",
    "\ttable = tf.lookup.StaticHashTable(\n",
    "\t\ttf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor),\n",
    "\t\tdefault_value=0\n",
    "\t)\n",
    "\treturn table.lookup(label)\n",
    "\n",
    "def create_segmentation_colormap(num_classes):\n",
    "\t\"\"\"\n",
    "\tCreate a linear colormap using a predefined palette.\n",
    "\tUses 'viridis' as default because it is perceptually uniform\n",
    "\tand works well for colorblindness.\n",
    "\t\"\"\"\n",
    "\treturn plt.cm.viridis(np.linspace(0, 1, num_classes))\n",
    "\n",
    "def apply_colormap(label, colormap=None):\n",
    "\t\"\"\"\n",
    "\tApply the colormap to a label.\n",
    "\t\"\"\"\n",
    "\t# Ensure label is 2D\n",
    "\tlabel = np.squeeze(label)\n",
    "\n",
    "\tif colormap is None:\n",
    "\t\tnum_classes = len(np.unique(label))\n",
    "\t\tcolormap = create_segmentation_colormap(num_classes)\n",
    "\n",
    "\t# Apply the colormap\n",
    "\tcolored = colormap[label.astype(int)]\n",
    "\n",
    "\treturn colored\n",
    "\t\n",
    "class VizCallback(tf.keras.callbacks.Callback):\n",
    "\tdef __init__(self, image, label, frequency=5):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.image = image\n",
    "\t\tself.label = tf.cast(tf.convert_to_tensor(label), tf.int32) \n",
    "\t\tself.frequency = frequency\n",
    "\n",
    "\tdef on_epoch_end(self, epoch, logs=None):\n",
    "\t\tif epoch % self.frequency == 0:  # Visualize only every \"frequency\" epochs\n",
    "\t\t\timage, label = self.image, self.label\n",
    "\t\t\tlabel = apply_category_mapping(label)\n",
    "\t\t\timage = tf.expand_dims(image, 0)\n",
    "\t\t\tpred = self.model.predict(image, verbose=0)\n",
    "\t\t\ty_pred = tf.math.argmax(pred, axis=-1)\n",
    "\t\t\ty_pred = y_pred.numpy()\n",
    "\n",
    "\t\t\t# Create colormap\n",
    "\t\t\tnum_classes = NUM_CLASSES\n",
    "\t\t\tcolormap = create_segmentation_colormap(num_classes)\n",
    "\n",
    "\t\t\tplt.figure(figsize=(16, 4))\n",
    "\n",
    "\t\t\t# Input image\n",
    "\t\t\tplt.subplot(1, 3, 1)\n",
    "\t\t\tplt.imshow(image[0],cmap='gray')\n",
    "\t\t\tplt.title(\"Input Image\")\n",
    "\t\t\tplt.axis('off')\n",
    "\n",
    "\t\t\t# Ground truth\n",
    "\t\t\tplt.subplot(1, 3, 2)\n",
    "\t\t\tcolored_label = apply_colormap(label.numpy(), colormap)\n",
    "\t\t\tplt.imshow(colored_label)\n",
    "\t\t\tplt.title(\"Ground Truth Mask\")\n",
    "\t\t\tplt.axis('off')\n",
    "\n",
    "\t\t\t# Prediction\n",
    "\t\t\tplt.subplot(1, 3, 3)\n",
    "\t\t\tcolored_pred = apply_colormap(y_pred[0], colormap)\n",
    "\t\t\tplt.imshow(colored_pred)\n",
    "\t\t\tplt.title(\"Predicted Mask\")\n",
    "\t\t\tplt.axis('off')\n",
    "\n",
    "\t\t\tplt.tight_layout()\n",
    "\t\t\tplt.show()\n",
    "\t\t\tplt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T20:59:53.000513Z",
     "iopub.status.busy": "2024-11-21T20:59:53.000251Z",
     "iopub.status.idle": "2024-11-21T20:59:53.010025Z",
     "shell.execute_reply": "2024-11-21T20:59:53.009423Z",
     "shell.execute_reply.started": "2024-11-21T20:59:53.000489Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define fitting callbacks. Comment out from dict the unwanted ones\n",
    "val_mask = tf.expand_dims(y_val[0], axis=-1)\n",
    "val_mask = tf.image.resize(val_mask, [IMG_SIZE[0], IMG_SIZE[1]], method=\"nearest\")\n",
    "val_img = tf.image.resize(X_val[0], [IMG_SIZE[0], IMG_SIZE[1]])\n",
    "viz_callback = VizCallback(val_img, val_mask)\n",
    "model_fit_callbacks = {\n",
    "\t'ReduceLROnPlateau': tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=25, min_lr=1e-6, verbose=1),\n",
    "\t'EarlyStopping': tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=50, restore_best_weights=True, verbose=1),\n",
    "\t#'Viz_callback' : viz_callback\n",
    "}\n",
    "\n",
    "def get_callbacks():\n",
    "\treturn [i for i in model_fit_callbacks.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Define model, augmentation and utils builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_augmentation(img, mask):\n",
    "\ttransform = build_augmentation()\n",
    "\ttransformed = transform(image=img, mask=mask)\n",
    "\treturn transformed[\"image\"], transformed[\"mask\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the augmented dataset\n",
    "if not quick_run:\n",
    "\tN = 2\n",
    "\tds = get_dataset(X_train[:N], y_train[:N], augmentations=apply_augmentation, augmentation_repetition=4, concat_and_shuffle_aug_with_no_aug=True)\n",
    "\n",
    "\tfor batch in ds.take(1):\n",
    "\t\ta, b = batch\n",
    "\t\tplot(a.numpy(), b.numpy(), num_images=N * 5) # use N * (augmentation_repetition+1) as `concat_and_shuffle_aug_with_no_aug` is True \n",
    "\t\tbreak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:00:15.913190Z",
     "iopub.status.busy": "2024-11-21T21:00:15.912842Z",
     "iopub.status.idle": "2024-11-21T21:00:15.923585Z",
     "shell.execute_reply": "2024-11-21T21:00:15.922710Z",
     "shell.execute_reply.started": "2024-11-21T21:00:15.913155Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# taken from https://keras.io/examples/vision/oxford_pets_image_segmentation/\n",
    "def build_U_NET_XCEPTION(img_size: tuple[int, int, int], num_classes):\n",
    "\tinputs = tfk.Input(shape=img_size) # One channel input\n",
    "\n",
    "\t### [First half of the network: downsampling inputs] ###\n",
    "\n",
    "\t# Entry block\n",
    "\tx = tfkl.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
    "\tx = tfkl.BatchNormalization()(x)\n",
    "\tx = tfkl.Activation(\"relu\")(x)\n",
    "\n",
    "\tprevious_block_activation = x  # Set aside residual\n",
    "\n",
    "\t# Blocks 1, 2, 3 are identical apart from the feature depth.\n",
    "\tfor filters in [64, 128, 256]:\n",
    "\t\tx = tfkl.Activation(\"relu\")(x)\n",
    "\t\tx = tfkl.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "\t\tx = tfkl.BatchNormalization()(x)\n",
    "\n",
    "\t\tx = tfkl.Activation(\"relu\")(x)\n",
    "\t\tx = tfkl.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "\t\tx = tfkl.BatchNormalization()(x)\n",
    "\n",
    "\t\tx = tfkl.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "\t\t# Project residual\n",
    "\t\tresidual = tfkl.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
    "\t\tprevious_block_activation\n",
    "\t\t)\n",
    "\t\tx = tfkl.add([x, residual])  # Add back residual\n",
    "\t\tprevious_block_activation = x  # Set aside next residual\n",
    "\n",
    "\t### [Second half of the network: upsampling inputs] ###\n",
    "\n",
    "\tfor filters in [256, 128, 64, 32]:\n",
    "\t\tx = tfkl.Activation(\"relu\")(x)\n",
    "\t\tx = tfkl.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "\t\tx = tfkl.BatchNormalization()(x)\n",
    "\n",
    "\t\tx = tfkl.Activation(\"relu\")(x)\n",
    "\t\tx = tfkl.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "\t\tx = tfkl.BatchNormalization()(x)\n",
    "\n",
    "\t\tx = tfkl.UpSampling2D(2)(x)\n",
    "\n",
    "\t\t# Project residual\n",
    "\t\tresidual = tfkl.UpSampling2D(2)(previous_block_activation)\n",
    "\t\tresidual = tfkl.Conv2D(filters, 1, padding=\"same\")(residual)\n",
    "\t\tx = tfkl.add([x, residual])  # Add back residual\n",
    "\t\tprevious_block_activation = x  # Set aside next residual\n",
    "\n",
    "\t# Add a per-pixel classification layer\n",
    "\toutputs = tfkl.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
    "\n",
    "\t# Define the model\n",
    "\tmodel = tfk.Model(inputs, outputs, name='UNetXception')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_U_NET(img_size: tuple[int, int, int], num_classes):\n",
    "\tdef unet_block(input_tensor, filters, kernel_size=3, activation='relu', stack=2, name=''):\n",
    "\t\t# Initialise the input tensor\n",
    "\t\tx = input_tensor\n",
    "\n",
    "\t\t# Apply a sequence of Conv2D, Batch Normalisation, and Activation layers for the specified number of stacks\n",
    "\t\tfor i in range(stack):\n",
    "\t\t\t\tx = tfkl.Conv2D(filters, kernel_size=kernel_size, padding='same', name=name + 'conv' + str(i + 1))(x)\n",
    "\t\t\t\tx = tfkl.BatchNormalization(name=name + 'bn' + str(i + 1))(x)\n",
    "\t\t\t\tx = tfkl.Activation(activation, name=name + 'activation' + str(i + 1))(x)\n",
    "\n",
    "\t\t# Return the transformed tensor\n",
    "\t\treturn x\n",
    "\n",
    "\tinput_layer = tfkl.Input(shape=img_size, name='input_layer')\n",
    "\n",
    "\t# Downsampling path\n",
    "\tdown_block_1 = unet_block(input_layer, 32, name='down_block1_')\n",
    "\td1 = tfkl.MaxPooling2D()(down_block_1)\n",
    "\n",
    "\tdown_block_2 = unet_block(d1, 64, name='down_block2_')\n",
    "\td2 = tfkl.MaxPooling2D()(down_block_2)\n",
    "\n",
    "\t# Bottleneck\n",
    "\tbottleneck = unet_block(d2, 128, name='bottleneck')\n",
    "\n",
    "\t# Upsampling path\n",
    "\tu1 = tfkl.UpSampling2D()(bottleneck)\n",
    "\tu1 = tfkl.Concatenate()([u1, down_block_2])\n",
    "\tu1 = unet_block(u1, 64, name='up_block1_')\n",
    "\n",
    "\tu2 = tfkl.UpSampling2D()(u1)\n",
    "\tu2 = tfkl.Concatenate()([u2, down_block_1])\n",
    "\tu2 = unet_block(u2, 32, name='up_block2_')\n",
    "\n",
    "\t# Output Layer\n",
    "\toutput_layer = tfkl.Conv2D(num_classes, kernel_size=1, padding='same', activation=\"softmax\", name='output_layer')(u2)\n",
    "\n",
    "\tmodel = tf.keras.Model(inputs=input_layer, outputs=output_layer, name='UNet')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ATTENTION_UW_NET(img_size: tuple[int, int, int], num_classes):\n",
    "\tdef attention_block(x, g, inter_channel):\n",
    "\t\t# theta_x (bs, h, w, inter_channel)\n",
    "\t\ttheta_x = tfkl.Conv2D(inter_channel, [1, 1], strides=[1, 1])(x)\n",
    "\t\t\n",
    "\t\t# phi_g (bs, h, w, inter_channel)\n",
    "\t\tphi_g = tfkl.Conv2D(inter_channel, [1, 1], strides=[1, 1])(g)\n",
    "\t\t\n",
    "\t\t# f (bs, h, w, 1)\n",
    "\t\tf = tfkl.Activation('relu')(tfkl.Add()([theta_x, phi_g]))\n",
    "\t\tpsi_f = tfkl.Conv2D(1, [1, 1], strides=[1, 1])(f)\n",
    "\t\t\n",
    "\t\t# sigmoid_psi_f (bs, h, w, 1)\n",
    "\t\tsigmoid_psi_f = tfkl.Activation('sigmoid')(psi_f)\n",
    "\t\t\n",
    "\t\t# rate (bs, h, w, 1)\n",
    "\t\trate = tfkl.multiply([x, sigmoid_psi_f])\n",
    "\t\t\n",
    "\t\treturn rate\n",
    "\t\n",
    "\t# Input\n",
    "\tinputs = tfkl.Input(shape=img_size)\n",
    "\t\n",
    "\t# Encoder Path\n",
    "\t# Block 1\n",
    "\tconv1 = tfkl.Conv2D(64, 3, padding='same')(inputs)\n",
    "\tconv1 = tfkl.BatchNormalization()(conv1)\n",
    "\tconv1 = tfkl.Activation('relu')(conv1)\n",
    "\tconv1 = tfkl.Conv2D(64, 3, padding='same')(conv1)\n",
    "\tconv1 = tfkl.BatchNormalization()(conv1)\n",
    "\tconv1 = tfkl.Activation('relu')(conv1)\n",
    "\tpool1 = tfkl.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\t\n",
    "\t# Block 2\n",
    "\tconv2 = tfkl.Conv2D(128, 3, padding='same')(pool1)\n",
    "\tconv2 = tfkl.BatchNormalization()(conv2)\n",
    "\tconv2 = tfkl.Activation('relu')(conv2)\n",
    "\tconv2 = tfkl.Conv2D(128, 3, padding='same')(conv2)\n",
    "\tconv2 = tfkl.BatchNormalization()(conv2)\n",
    "\tconv2 = tfkl.Activation('relu')(conv2)\n",
    "\tpool2 = tfkl.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\t\n",
    "\t# Block 3\n",
    "\tconv3 = tfkl.Conv2D(256, 3, padding='same')(pool2)\n",
    "\tconv3 = tfkl.BatchNormalization()(conv3)\n",
    "\tconv3 = tfkl.Activation('relu')(conv3)\n",
    "\tconv3 = tfkl.Conv2D(256, 3, padding='same')(conv3)\n",
    "\tconv3 = tfkl.BatchNormalization()(conv3)\n",
    "\tconv3 = tfkl.Activation('relu')(conv3)\n",
    "\tpool3 = tfkl.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\t\n",
    "\t# Block 4\n",
    "\tconv4 = tfkl.Conv2D(512, 3, padding='same')(pool3)\n",
    "\tconv4 = tfkl.BatchNormalization()(conv4)\n",
    "\tconv4 = tfkl.Activation('relu')(conv4)\n",
    "\tconv4 = tfkl.Conv2D(512, 3, padding='same')(conv4)\n",
    "\tconv4 = tfkl.BatchNormalization()(conv4)\n",
    "\tconv4 = tfkl.Activation('relu')(conv4)\n",
    "\tpool4 = tfkl.MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\t\n",
    "\t# Bridge\n",
    "\tconv5 = tfkl.Conv2D(1024, 3, padding='same')(pool4)\n",
    "\tconv5 = tfkl.BatchNormalization()(conv5)\n",
    "\tconv5 = tfkl.Activation('relu')(conv5)\n",
    "\tconv5 = tfkl.Conv2D(1024, 3, padding='same')(conv5)\n",
    "\tconv5 = tfkl.BatchNormalization()(conv5)\n",
    "\tconv5 = tfkl.Activation('relu')(conv5)\n",
    "\t\n",
    "\t# Decoder Path with Attention\n",
    "\t# Block 6\n",
    "\tup6 = tfkl.Conv2D(512, 2, padding='same')(tfkl.UpSampling2D(size=(2, 2))(conv5))\n",
    "\tup6 = tfkl.BatchNormalization()(up6)\n",
    "\tup6 = tfkl.Activation('relu')(up6)\n",
    "\t\n",
    "\tatt6 = attention_block(conv4, up6, inter_channel=256)\n",
    "\tmerge6 = tfkl.concatenate([att6, up6], axis=3)\n",
    "\t\n",
    "\tconv6 = tfkl.Conv2D(512, 3, padding='same')(merge6)\n",
    "\tconv6 = tfkl.BatchNormalization()(conv6)\n",
    "\tconv6 = tfkl.Activation('relu')(conv6)\n",
    "\tconv6 = tfkl.Conv2D(512, 3, padding='same')(conv6)\n",
    "\tconv6 = tfkl.BatchNormalization()(conv6)\n",
    "\tconv6 = tfkl.Activation('relu')(conv6)\n",
    "\t\n",
    "\t# Block 7\n",
    "\tup7 = tfkl.Conv2D(256, 2, padding='same')(tfkl.UpSampling2D(size=(2, 2))(conv6))\n",
    "\tup7 = tfkl.BatchNormalization()(up7)\n",
    "\tup7 = tfkl.Activation('relu')(up7)\n",
    "\t\n",
    "\tatt7 = attention_block(conv3, up7, inter_channel=128)\n",
    "\tmerge7 = tfkl.concatenate([att7, up7], axis=3)\n",
    "\t\n",
    "\tconv7 = tfkl.Conv2D(256, 3, padding='same')(merge7)\n",
    "\tconv7 = tfkl.BatchNormalization()(conv7)\n",
    "\tconv7 = tfkl.Activation('relu')(conv7)\n",
    "\tconv7 = tfkl.Conv2D(256, 3, padding='same')(conv7)\n",
    "\tconv7 = tfkl.BatchNormalization()(conv7)\n",
    "\tconv7 = tfkl.Activation('relu')(conv7)\n",
    "\t\n",
    "\t# Block 8\n",
    "\tup8 = tfkl.Conv2D(128, 2, padding='same')(tfkl.UpSampling2D(size=(2, 2))(conv7))\n",
    "\tup8 = tfkl.BatchNormalization()(up8)\n",
    "\tup8 = tfkl.Activation('relu')(up8)\n",
    "\t\n",
    "\tatt8 = attention_block(conv2, up8, inter_channel=64)\n",
    "\tmerge8 = tfkl.concatenate([att8, up8], axis=3)\n",
    "\t\n",
    "\tconv8 = tfkl.Conv2D(128, 3, padding='same')(merge8)\n",
    "\tconv8 = tfkl.BatchNormalization()(conv8)\n",
    "\tconv8 = tfkl.Activation('relu')(conv8)\n",
    "\tconv8 = tfkl.Conv2D(128, 3, padding='same')(conv8)\n",
    "\tconv8 = tfkl.BatchNormalization()(conv8)\n",
    "\tconv8 = tfkl.Activation('relu')(conv8)\n",
    "\t\n",
    "\t# Block 9\n",
    "\tup9 = tfkl.Conv2D(64, 2, padding='same')(tfkl.UpSampling2D(size=(2, 2))(conv8))\n",
    "\tup9 = tfkl.BatchNormalization()(up9)\n",
    "\tup9 = tfkl.Activation('relu')(up9)\n",
    "\t\n",
    "\tatt9 = attention_block(conv1, up9, inter_channel=32)\n",
    "\tmerge9 = tfkl.concatenate([att9, up9], axis=3)\n",
    "\t\n",
    "\tconv9 = tfkl.Conv2D(64, 3, padding='same')(merge9)\n",
    "\tconv9 = tfkl.BatchNormalization()(conv9)\n",
    "\tconv9 = tfkl.Activation('relu')(conv9)\n",
    "\tconv9 = tfkl.Conv2D(64, 3, padding='same')(conv9)\n",
    "\tconv9 = tfkl.BatchNormalization()(conv9)\n",
    "\tconv9 = tfkl.Activation('relu')(conv9)\n",
    "\t\n",
    "\t# Output\n",
    "\toutputs = tfkl.Conv2D(num_classes, 1, activation='softmax')(conv9)\n",
    "\t\n",
    "\tmodel = tfk.Model(inputs=inputs, outputs=outputs, name='AttentionUWNet')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ASPP_model(img_size: tuple[int, int, int], num_classes: int):\n",
    "\t\n",
    "\tinitializer = tf.keras.initializers.HeNormal()\n",
    "\tregularizer = tf.keras.regularizers.l2(1e-4)\n",
    "\n",
    "\tinputs = tfkl.Input(shape=img_size)\n",
    "\n",
    "\tdef conv_block(x, filters, kernel_size=(3, 3), activation=\"relu\", batch_norm=True, dropout_rate=0.2):\n",
    "\t\tx = tfkl.Conv2D(\n",
    "\t\t\tfilters,\n",
    "\t\t\tkernel_size,\n",
    "\t\t\tpadding=\"same\",\n",
    "\t\t\tkernel_initializer=initializer,\n",
    "\t\t\tkernel_regularizer=regularizer,\n",
    "\t\t)(x)\n",
    "\t\tif batch_norm:\n",
    "\t\t\tx = tfkl.BatchNormalization()(x)\n",
    "\t\tx = tfkl.Activation(activation)(x)\n",
    "\t\tif dropout_rate > 0:\n",
    "\t\t\tx = tfkl.SpatialDropout2D(dropout_rate)(x)\n",
    "\t\treturn x\n",
    "\n",
    "\tdef encoder_block(x, filters, dropout_rate=0.2):\n",
    "\t\tx = conv_block(x, filters, dropout_rate=dropout_rate)\n",
    "\t\tx = conv_block(x, filters, dropout_rate=dropout_rate)\n",
    "\t\tp = tfkl.MaxPooling2D(2)(x)\n",
    "\t\treturn x, p\n",
    "\n",
    "\tdef atrous_spatial_pyramid_pooling(x, dropout_rate=0.3):\n",
    "\t\tdims = x.shape[1:3]\n",
    "\t\tpool = tfkl.GlobalAveragePooling2D()(x)\n",
    "\t\tpool = tfkl.Reshape((1, 1, x.shape[-1]))(pool)\n",
    "\t\tpool = tfkl.Conv2D(\n",
    "\t\t\t256, \n",
    "\t\t\t1, \n",
    "\t\t\tpadding=\"same\", \n",
    "\t\t\tkernel_initializer=initializer, \n",
    "\t\t\tkernel_regularizer=regularizer,\n",
    "\t\t)(pool)\n",
    "\t\tpool = tfkl.UpSampling2D(size=dims, interpolation=\"bilinear\")(pool)\n",
    "\t\tpool = tfkl.SpatialDropout2D(dropout_rate)(pool)\n",
    "\n",
    "\t\tconv_1x1 = tfkl.Conv2D(\n",
    "\t\t\t256,\n",
    "\t\t\t1,\n",
    "\t\t\tpadding=\"same\",\n",
    "\t\t\tkernel_initializer=initializer,\n",
    "\t\t\tkernel_regularizer=regularizer,\n",
    "\t\t)(x)\n",
    "\t\tatrous_6 = tfkl.Conv2D(\n",
    "\t\t\t256,\n",
    "\t\t\t3,\n",
    "\t\t\tdilation_rate=6,\n",
    "\t\t\tpadding=\"same\",\n",
    "\t\t\tkernel_initializer=initializer,\n",
    "\t\t\tkernel_regularizer=regularizer,\n",
    "\t\t)(x)\n",
    "\t\tatrous_12 = tfkl.Conv2D(\n",
    "\t\t\t256,\n",
    "\t\t\t3,\n",
    "\t\t\tdilation_rate=12,\n",
    "\t\t\tpadding=\"same\",\n",
    "\t\t\tkernel_initializer=initializer,\n",
    "\t\t\tkernel_regularizer=regularizer,\n",
    "\t\t)(x)\n",
    "\t\tatrous_18 = tfkl.Conv2D(\n",
    "\t\t\t256,\n",
    "\t\t\t3,\n",
    "\t\t\tdilation_rate=18,\n",
    "\t\t\tpadding=\"same\",\n",
    "\t\t\tkernel_initializer=initializer,\n",
    "\t\t\tkernel_regularizer=regularizer,\n",
    "\t\t)(x)\n",
    "\n",
    "\t\tx = tfkl.Concatenate()([pool, conv_1x1, atrous_6, atrous_12, atrous_18])\n",
    "\t\tx = tfkl.Conv2D(\n",
    "\t\t\t256,\n",
    "\t\t\t1,\n",
    "\t\t\tpadding=\"same\",\n",
    "\t\t\tkernel_initializer=initializer,\n",
    "\t\t\tkernel_regularizer=regularizer,\n",
    "\t\t)(x)\n",
    "\t\tx = tfkl.SpatialDropout2D(dropout_rate)(x)\n",
    "\t\treturn x\n",
    "\n",
    "\tdef decoder_block(x, skip, filters, dropout_rate=0.2):\n",
    "\t\tx = tfkl.Conv2DTranspose(\n",
    "\t\t\tfilters,\n",
    "\t\t\t2,\n",
    "\t\t\tstrides=2,\n",
    "\t\t\tpadding=\"same\",\n",
    "\t\t\tkernel_initializer=initializer,\n",
    "\t\t\tkernel_regularizer=regularizer,\n",
    "\t\t)(x)\n",
    "\t\tx = tfkl.Concatenate()([x, skip])\n",
    "\t\tx = conv_block(x, filters, dropout_rate=dropout_rate)\n",
    "\t\treturn x\n",
    "\n",
    "\t# Encoder\n",
    "\tfilters = [64, 128, 256, 512]\n",
    "\tskips = []\n",
    "\tx = inputs\n",
    "\tfor f in filters:\n",
    "\t\tskip, x = encoder_block(x, f, dropout_rate=0.2)\n",
    "\t\tskips.append(skip)\n",
    "\n",
    "\t# Bottleneck with ASPP\n",
    "\tx = conv_block(x, 1024, dropout_rate=0.3)\n",
    "\tx = atrous_spatial_pyramid_pooling(x, dropout_rate=0.3)\n",
    "\n",
    "\t# Decoder\n",
    "\tskips = skips[::-1]\n",
    "\tdecoder_filters = [512, 256, 128, 64]\n",
    "\tfor skip, f in zip(skips, decoder_filters):\n",
    "\t\tx = decoder_block(x, skip, f, dropout_rate=0.2)\n",
    "\n",
    "\t# Final convolutional layer\n",
    "\toutputs = tfkl.Conv2D(\n",
    "\t\tnum_classes, \n",
    "\t\t1, \n",
    "\t\tactivation=\"softmax\", \n",
    "\t\tkernel_initializer=initializer, \n",
    "\t\tkernel_regularizer=regularizer,\n",
    "\t)(x)\n",
    "\n",
    "\tmodel = tf.keras.Model(inputs, outputs)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_RockSeg(img_size: tuple[int, int, int], num_classes: int):\n",
    "\t\"\"\"\n",
    "\tBuilds the RockSeg model, adjusted for input image size 64x128.\n",
    "\n",
    "\tParameters:\n",
    "\t\timg_size (tuple): Input image dimensions (height, width, channels).\n",
    "\t\tnum_classes (int): Number of output classes.\n",
    "\n",
    "\tReturns:\n",
    "\t\ttf.keras.Model: RockSeg model instance.\n",
    "\t\"\"\"\n",
    "\n",
    "\tinitializer = tf.keras.initializers.HeNormal()\n",
    "\tregularizer = tf.keras.regularizers.l2(1e-4)  # L2 regularization with strength 1e-4\n",
    "\n",
    "\tdef resnet_block(input_tensor, filters, kernel_size=3, activation='relu', stack=2, name=''):\n",
    "\t\tx = input_tensor\n",
    "\t\tfor i in range(stack):\n",
    "\t\t\tx = tfkl.Conv2D(filters, kernel_size=kernel_size, padding='same', kernel_initializer=initializer,kernel_regularizer=regularizer, name=name + f'conv{i + 1}')(x)\n",
    "\t\t\tx = tfkl.BatchNormalization(name=name + f'bn{i + 1}')(x)\n",
    "\t\t\tx = tfkl.Activation(activation, name=name + f'activation{i + 1}')(x)\n",
    "\t\treturn x\n",
    "\n",
    "\tdef transformer_block(input_tensor, embed_dim, num_heads, name=''):\n",
    "\t\tx = tfkl.Conv2D(filters=256, kernel_size=(1, 1), padding='same', kernel_initializer=initializer,kernel_regularizer=regularizer, name=name + 'transformer_11')(input_tensor)\n",
    "\t\tx = tfkl.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"same\", name=name + 'avg_pool')(x)\n",
    "\t\tx = tf.keras.tfkl.LayerNormalization(name=name + 'ln')(x)\n",
    "\t\tattention_output = tf.keras.tfkl.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name=name + 'mha')(x, x)\n",
    "\t\tx = tf.keras.tfkl.Add(name=name + 'skip1')([x, attention_output])\n",
    "\t\tfeed_forward = tfkl.Dense(embed_dim, activation='relu', kernel_initializer=initializer,kernel_regularizer=regularizer, name=name + 'dense')(x)\n",
    "\t\tx = tf.keras.tfkl.Add(name=name + 'skip2')([x, feed_forward])\n",
    "\t\treturn x\n",
    "\n",
    "\tdef multiscale_feature_fusion(feature_maps, name=''):\n",
    "\t\tbase_channels = feature_maps[len(feature_maps) // 2].shape[-1]\n",
    "\t\tconsistent_features = [\n",
    "\t\t\ttfkl.Conv2D(base_channels, kernel_size=(1, 1), padding='same', kernel_initializer=initializer,kernel_regularizer=regularizer, name=name + f'conv_{i}')(fm) if fm.shape[-1] != base_channels else fm\n",
    "\t\t\tfor i, fm in enumerate(feature_maps)\n",
    "\t\t]\n",
    "\t\tbase_height, base_width = consistent_features[len(consistent_features) // 2].shape[1:3]\n",
    "\t\tresized_features = [\n",
    "\t\t\ttfkl.UpSampling2D(size=(base_height // fm.shape[1], base_width // fm.shape[2]), interpolation='bilinear', name=name + f'up_{i}')(fm)\n",
    "\t\t\tif fm.shape[1] < base_height else\n",
    "\t\t\ttfkl.MaxPooling2D(pool_size=(fm.shape[1] // base_height, fm.shape[2] // base_width), name=name + f'pool_{i}')(fm)\n",
    "\t\t\tfor i, fm in enumerate(consistent_features)\n",
    "\t\t]\n",
    "\t\tfused = tfkl.Concatenate(name=name + 'concat')(resized_features)\n",
    "\t\treturn tfkl.Conv2D(base_channels, kernel_size=1, padding='same', activation='relu', kernel_initializer=initializer,kernel_regularizer=regularizer, name=name + 'conv_fused')(fused)\n",
    "\n",
    "\tinput_layer = tfkl.Input(shape=img_size, name='input_layer')\n",
    "\n",
    "\tconv1 = tfkl.Conv2D(filters=64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializer,kernel_regularizer=regularizer, name='conv1')(input_layer)\n",
    "\tconv1_upsampled = tfkl.UpSampling2D(size=(2, 2), interpolation='bilinear', name='conv1_upsampled')(conv1)\n",
    "\tmaxpool1 = tfkl.MaxPooling2D(pool_size=(2, 2), name='pool1')(conv1)\n",
    "\n",
    "\tresnet1 = resnet_block(maxpool1, filters=64, stack=2, name='resnet1_')\n",
    "\tresnet2 = resnet_block(resnet1, filters=128, stack=2, name='resnet2_')\n",
    "\n",
    "\ttransformer = transformer_block(resnet2, embed_dim=256, num_heads=4, name='transformer_')\n",
    "\n",
    "\tbottleneck = tfkl.Conv2D(filters=512, kernel_size=3, padding='same', activation='relu', kernel_initializer=initializer, kernel_regularizer=regularizer, name='bottleneck')(transformer)\n",
    "\n",
    "\tmsf1 = multiscale_feature_fusion([resnet1, resnet2, bottleneck], name='msf1_')\n",
    "\tmsf1_upsampled = tfkl.UpSampling2D(size=(2, 2), interpolation='bilinear', name='msf1_upsample')(msf1)\n",
    "\n",
    "\tresnet1_upsampled = tfkl.UpSampling2D(size=(2, 2), interpolation='bilinear', name='resnet1_upsample')(resnet1)\n",
    "\n",
    "\tconcat1 = tfkl.Concatenate(name='concat1')([msf1_upsampled, resnet1_upsampled])\n",
    "\tdecoder1 = tfkl.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', kernel_initializer=initializer, kernel_regularizer=regularizer, name='decoder1')(concat1)\n",
    "\n",
    "\tupsample2 = tfkl.UpSampling2D(size=(2, 2), interpolation='bilinear', name='upsample2')(decoder1)\n",
    "\n",
    "\tconcat2 = tfkl.Concatenate(name='concat2')([upsample2, conv1_upsampled])\n",
    "\tdecoder2 = tfkl.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', kernel_initializer=initializer,kernel_regularizer=regularizer, name='decoder2')(concat2)\n",
    "\n",
    "\tupsample3 = tfkl.UpSampling2D(size=(1, 1), interpolation='bilinear', name='upsample3')(decoder2)\n",
    "\n",
    "\toutput_layer = tfkl.Conv2D(num_classes, kernel_size=1, padding='same', activation=\"softmax\", kernel_initializer=initializer, name='output_layer')(upsample3)\n",
    "\n",
    "\tmodel = tf.keras.Model(inputs=input_layer, outputs=output_layer, name='RockSeg')\n",
    "\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_TURKEYSEG(img_size: tuple[int, int, int], num_classes: int):\n",
    "\tdef conv_block(inputs, filters, kernel_size=3, dilation_rate=1, use_se=True, name=\"conv_block\"):\n",
    "\t\t\"\"\"A convolutional block with optional squeeze-and-excitation.\"\"\"\n",
    "\t\tx = tfkl.Conv2D(filters, kernel_size, padding='same', dilation_rate=dilation_rate, activation='relu', name=f\"{name}_conv1\")(inputs)\n",
    "\t\tx = tfkl.Conv2D(filters, kernel_size, padding='same', activation='relu', name=f\"{name}_conv2\")(x)\n",
    "\t\tif use_se:\n",
    "\t\t\tse = tfkl.GlobalAveragePooling2D(name=f\"{name}_se_pool\")(x)\n",
    "\t\t\tse = tfkl.Dense(filters // 16, activation='relu', name=f\"{name}_se_dense1\")(se)\n",
    "\t\t\tse = tfkl.Dense(filters, activation='sigmoid', name=f\"{name}_se_dense2\")(se)\n",
    "\t\t\tse = tfkl.Reshape((1, 1, filters), name=f\"{name}_se_reshape\")(se)\n",
    "\t\t\tx = tfkl.multiply([x, se], name=f\"{name}_se_mult\")\n",
    "\t\treturn x\n",
    "\n",
    "\tdef down_block(inputs, filters, name=\"down_block\"):\n",
    "\t\t\"\"\"Downsampling block with max pooling.\"\"\"\n",
    "\t\tx = conv_block(inputs, filters, name=f\"{name}_conv\")\n",
    "\t\tp = tfkl.MaxPooling2D(pool_size=(2, 2), name=f\"{name}_pool\")(x)\n",
    "\t\treturn x, p\n",
    "\n",
    "\tdef up_block(inputs, skip, filters, name=\"up_block\"):\n",
    "\t\t\"\"\"Upsampling block with skip connections.\"\"\"\n",
    "\t\tx = tfkl.Conv2DTranspose(filters, kernel_size=3, strides=2, padding='same', name=f\"{name}_upsample\")(inputs)\n",
    "\t\tx = tfkl.concatenate([x, skip], name=f\"{name}_concat\")\n",
    "\t\tx = conv_block(x, filters, name=f\"{name}_conv\")\n",
    "\t\treturn x\n",
    "\n",
    "\tdef bottleneck_with_attention(inputs, filters, name=\"bottleneck\"):\n",
    "\t\t\"\"\"Bottleneck with parallel dilated convolutions and MultiHeadAttention.\"\"\"\n",
    "\t\t# Parallel dilated convolutions\n",
    "\t\td1 = tfkl.Conv2D(filters, kernel_size=3, dilation_rate=1, padding='same', activation='relu', name=f\"{name}_dil1\")(inputs)\n",
    "\t\td2 = tfkl.Conv2D(filters, kernel_size=3, dilation_rate=2, padding='same', activation='relu', name=f\"{name}_dil2\")(inputs)\n",
    "\t\td3 = tfkl.Conv2D(filters, kernel_size=3, dilation_rate=4, padding='same', activation='relu', name=f\"{name}_dil3\")(inputs)\n",
    "\t\tfused = tfkl.Add(name=f\"{name}_fused\")([d1, d2, d3])\n",
    "\t\t\n",
    "\t\t# Reshape for attention\n",
    "\t\tb, h, w, c = fused.shape  # Batch, Height, Width, Channels\n",
    "\t\treshaped = tfkl.Reshape((h * w, c), name=f\"{name}_reshape\")(fused)\n",
    "\t\t\n",
    "\t\t# MultiHeadAttention\n",
    "\t\tattention_output = tfkl.MultiHeadAttention(num_heads=4, key_dim=c, name=f\"{name}_mha\")(reshaped, reshaped)\n",
    "\t\tattention_output = tfkl.Reshape((h, w, c), name=f\"{name}_attention_reshape\")(attention_output)\n",
    "\t\t\n",
    "\t\t# Merge attention with fused features\n",
    "\t\toutput = tfkl.Add(name=f\"{name}_attention_fusion\")([fused, attention_output])\n",
    "\t\t\n",
    "\t\treturn output\n",
    "\n",
    "\tinputs = tf.keras.Input(shape=img_size, name=\"input_image\")\n",
    "\n",
    "\t# Encoder (global context path)\n",
    "\tg1, p1 = down_block(inputs, 64, name=\"global_down1\")\n",
    "\tg2, p2 = down_block(p1, 128, name=\"global_down2\")\n",
    "\tg3, p3 = down_block(p2, 256, name=\"global_down3\")\n",
    "\n",
    "\t# Bottleneck (global context)\n",
    "\tg_bottleneck = bottleneck_with_attention(p3, 512, name=\"global_bottleneck\")\n",
    "\n",
    "\t# Decoder (global context path)\n",
    "\tg_up3 = up_block(g_bottleneck, g3, 256, name=\"global_up3\")\n",
    "\tg_up2 = up_block(g_up3, g2, 128, name=\"global_up2\")\n",
    "\tg_up1 = up_block(g_up2, g1, 64, name=\"global_up1\")\n",
    "\n",
    "\t# Local path (for fine details)\n",
    "\tl1, lp1 = down_block(inputs, 32, name=\"local_down1\")\n",
    "\tl2, lp2 = down_block(lp1, 64, name=\"local_down2\")\n",
    "\tl_bottleneck = bottleneck_with_attention(lp2, 128, name=\"local_bottleneck\")\n",
    "\tl_up2 = up_block(l_bottleneck, l2, 64, name=\"local_up2\")\n",
    "\tl_up1 = up_block(l_up2, l1, 32, name=\"local_up1\")\n",
    "\n",
    "\t# Fusion of global and local paths\n",
    "\tfusion = tfkl.concatenate([g_up1, l_up1], name=\"fusion_concat\")\n",
    "\tfusion = conv_block(fusion, 64, name=\"fusion_conv\")\n",
    "\n",
    "\t# Output layer\n",
    "\toutputs = tfkl.Conv2D(num_classes, kernel_size=1, activation='softmax', name=\"output_layer\")(fusion)\n",
    "\n",
    "\treturn tfk.Model(inputs, outputs, name=\"MarsSegmentationModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "\t'U_NET': build_U_NET,\n",
    "\t'U_NET_XCEPTION': build_U_NET_XCEPTION,\n",
    "\t'UWNet': build_ATTENTION_UW_NET,\n",
    "\t'ASPP' : build_ASPP_model,\n",
    "\t'ROCKSEG' : build_RockSeg,\n",
    "\t'TURKEYSEG' : build_TURKEYSEG\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:00:15.940614Z",
     "iopub.status.busy": "2024-11-21T21:00:15.940360Z",
     "iopub.status.idle": "2024-11-21T21:00:15.949708Z",
     "shell.execute_reply": "2024-11-21T21:00:15.948895Z",
     "shell.execute_reply.started": "2024-11-21T21:00:15.940588Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fit_model(model, data_loader=None, epochs=100, validation_data_loader=None):\n",
    "\tassert(data_loader is not None)\n",
    "\tassert(validation_data_loader is not None)\n",
    "\tfit_history = model.fit(\n",
    "\t\t\t\tdata_loader,\n",
    "\t\t\t\tepochs=epochs,\n",
    "\t\t\t\tvalidation_data=validation_data_loader,\n",
    "\t\t\t\tclass_weight=class_weights,\n",
    "\t\t\t\tcallbacks=get_callbacks()\n",
    "\t\t\t).history\n",
    "\treturn fit_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:00:15.980359Z",
     "iopub.status.busy": "2024-11-21T21:00:15.980111Z",
     "iopub.status.idle": "2024-11-21T21:00:15.995129Z",
     "shell.execute_reply": "2024-11-21T21:00:15.994358Z",
     "shell.execute_reply.started": "2024-11-21T21:00:15.980335Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Taken from https://github.com/SeanSdahl/RangerOptimizerTensorflow/blob/master/module.py\n",
    "def build_ranger(lr=1e-3, weight_decay=0.0):\n",
    "\ttry:\n",
    "\t\timport tensorflow_addons as tfa\n",
    "\texcept:\n",
    "\t\traise Exception(\"You have to install tensorflow_addons package for Ranger. Please note that this package is available up to tensorflow==2.14\")\n",
    "\tdef ranger(sync_period=6,\n",
    "\t\t\tslow_step_size=0.5,\n",
    "\t\t\tlearning_rate=lr,\n",
    "\t\t\tbeta_1=0.9,\n",
    "\t\t\tbeta_2=0.999,\n",
    "\t\t\tepsilon=1e-7,\n",
    "\t\t\tweight_decay=weight_decay,\n",
    "\t\t\tamsgrad=False,\n",
    "\t\t\tsma_threshold=5.0,\n",
    "\t\t\ttotal_steps=0,\n",
    "\t\t\twarmup_proportion=0.1,\n",
    "\t\t\tmin_lr=0.,\n",
    "\t\t\tname=\"Ranger\"):\n",
    "\t\tinner = tfa.optimizers.RectifiedAdam(learning_rate, beta_1, beta_2, epsilon, weight_decay, amsgrad, sma_threshold, total_steps, warmup_proportion, min_lr, name)\n",
    "\t\toptim = tfa.optimizers.Lookahead(inner, sync_period, slow_step_size, name)\n",
    "\t\treturn optim\n",
    "\treturn ranger()\n",
    "\n",
    "def get_optimizer(opt, batch_size, lr, **kwargs):\n",
    "\tdecay = opt_exp_decay_rate\n",
    "\tif opt == \"SGD\":\n",
    "\t\toptimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9 if 'momentum' not in kwargs else kwargs['momentum'])\n",
    "\t\tif decay is not None:\n",
    "\t\t\tlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "\t\t\t\t\tinitial_learning_rate= lr,\n",
    "\t\t\t\t\tdecay_steps=opt_decay_epoch_delta * (X_train.shape[0] // batch_size),  # Decay every 7 epochs\n",
    "\t\t\t\t\tdecay_rate=opt_exp_decay_rate,\n",
    "\t\t\t\t\tstaircase=True\n",
    "\t\t\t)\n",
    "\t\t\toptimizer.learning_rate = lr_schedule\n",
    "\t\t\tprint(f'Using {opt} optimizer with exp decay {decay} (momentum = {optimizer.momentum})')\n",
    "\t\t\treturn optimizer\n",
    "\t\telse:\n",
    "\t\t\toptimizer.learning_rate = lr\n",
    "\t\t\tprint(f'Using {opt} optimizer (momentum = {optimizer.momentum})')\n",
    "\t\t\treturn optimizer\n",
    "\n",
    "\telif opt == \"Adam\":\n",
    "\t\tif 'weight_decay' in kwargs:\n",
    "\t\t\toptimizer = tf.keras.optimizers.Adam(weight_decay=kwargs['weight_decay'])\n",
    "\t\telse:\n",
    "\t\t\toptimizer = tf.keras.optimizers.Adam()\n",
    "\t\tif decay is not None:\n",
    "\t\t\tlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "\t\t\t\t\tinitial_learning_rate=lr,\n",
    "\t\t\t\t\tdecay_steps=opt_decay_epoch_delta * (X_train.shape[0] // batch_size),  # Decay every 7 epochs\n",
    "\t\t\t\t\tdecay_rate=opt_exp_decay_rate,\n",
    "\t\t\t\t\tstaircase=True\n",
    "\t\t\t)\n",
    "\t\t\toptimizer.learning_rate = lr_schedule\n",
    "\t\t\tprint(f'Using {opt} optimizer with exp decay of {decay} weight decay = {optimizer.weight_decay}')\n",
    "\t\t\treturn optimizer\n",
    "\t\telse:\n",
    "\t\t\toptimizer.learning_rate = lr\n",
    "\t\t\tprint(f'Using {opt} optimizer (weight decay = {optimizer.weight_decay})')\n",
    "\t\t\treturn optimizer\n",
    "\n",
    "\telif opt == \"AdamW\":\n",
    "\t\tif 'weight_decay' in kwargs:\n",
    "\t\t\toptimizer = tf.keras.optimizers.AdamW(weight_decay=kwargs['weight_decay'])\n",
    "\t\telse:\n",
    "\t\t\toptimizer = tf.keras.optimizers.AdamW()\n",
    "\t\tif decay is not None:\n",
    "\t\t\tlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "\t\t\t\t\tinitial_learning_rate= lr,\n",
    "\t\t\t\t\tdecay_steps=opt_decay_epoch_delta * (X_train.shape[0] // batch_size),  # Decay every 7 epochs\n",
    "\t\t\t\t\tdecay_rate=opt_exp_decay_rate,\n",
    "\t\t\t\t\tstaircase=True\n",
    "\t\t\t)\n",
    "\t\t\toptimizer.learning_rate = lr_schedule\n",
    "\t\t\tprint(f'Using {opt} optimizer with exp decay of {decay} weight decay = {optimizer.weight_decay}')\n",
    "\t\t\treturn optimizer\n",
    "\t\telse:\n",
    "\t\t\toptimizer.learning_rate = lr\n",
    "\t\t\tprint(f'Using {opt} optimizer (weight decay = {optimizer.weight_decay})')\n",
    "\t\t\treturn optimizer\n",
    "\n",
    "\telif opt == \"Lion\":\n",
    "\t\tif 'weight_decay' in kwargs:\n",
    "\t\t\toptimizer = tf.keras.optimizers.Lion(weight_decay=kwargs['weight_decay'])\n",
    "\t\telse:\n",
    "\t\t\toptimizer = tf.keras.optimizers.Lion()\n",
    "\t\tif decay is not None:\n",
    "\t\t\tlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "\t\t\t\t\tinitial_learning_rate= lr,\n",
    "\t\t\t\t\tdecay_steps=opt_decay_epoch_delta * (X_train.shape[0] // batch_size),  # Decay every 7 epochs\n",
    "\t\t\t\t\tdecay_rate=opt_exp_decay_rate,\n",
    "\t\t\t\t\tstaircase=True\n",
    "\t\t\t)\n",
    "\t\t\toptimizer.learning_rate = lr_schedule\n",
    "\t\t\tprint(f'Using {opt} optimizer with exp decay of {decay} weight decay = {optimizer.weight_decay}')\n",
    "\t\t\treturn optimizer\n",
    "\t\telse:\n",
    "\t\t\toptimizer.learning_rate = lr\n",
    "\t\t\tprint(f'Using {opt} optimizer (weight decay = {optimizer.weight_decay})')\n",
    "\t\t\treturn optimizer\n",
    "\telif opt == \"Ranger\":\n",
    "\t\toptimizer = build_ranger(lr=lr, weight_decay=0.0 if 'weight_decay' not in kwargs else kwargs['weight_decay'])\n",
    "\t\tif decay is not None:\n",
    "\t\t\traise RuntimeError(\"Not supported\")\n",
    "\t\telse:\n",
    "\t\t\toptimizer.learning_rate = lr\n",
    "\t\t\tprint(f'Uusing {opt} optimizer')\n",
    "\t\t\treturn optimizer\n",
    "\tprint(f\"Starting learning rate: {lr} and batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:00:15.996454Z",
     "iopub.status.busy": "2024-11-21T21:00:15.996146Z",
     "iopub.status.idle": "2024-11-21T21:00:16.006383Z",
     "shell.execute_reply": "2024-11-21T21:00:16.005722Z",
     "shell.execute_reply.started": "2024-11-21T21:00:15.996418Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def display_model(model):\n",
    "\tif not quick_run:\n",
    "\t\t# Display a summary of the model architecture\n",
    "\t\tmodel.summary(expand_nested=True)\n",
    "\t\t# Display model architecture with layer shapes and trainable parameters\n",
    "\t\ttfk.utils.plot_model(model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)\n",
    "\telse:\n",
    "\t\t# Just print the total parameters\n",
    "\t\tprint(f\"Total parameters: {model.count_params()/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ£ Define and display model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom Mean Intersection Over Union metric: the competition excludes the background class\n",
    "class MeanIntersectionOverUnion(tf.keras.metrics.MeanIoU):\n",
    "\tdef __init__(self, num_classes, labels_to_exclude=None, name=\"mean_iou\", dtype=None):\n",
    "\t\tsuper(MeanIntersectionOverUnion, self).__init__(num_classes=num_classes, name=name, dtype=dtype)\n",
    "\t\tif labels_to_exclude is None:\n",
    "\t\t\tlabels_to_exclude = [0]  # Default to excluding label 0\n",
    "\t\tself.labels_to_exclude = labels_to_exclude\n",
    "\n",
    "\tdef update_state(self, y_true, y_pred, sample_weight=None):\n",
    "\t\t# Convert predictions to class labels\n",
    "\t\ty_pred = tf.math.argmax(y_pred, axis=-1)\n",
    "\n",
    "\t\t# Flatten the tensors\n",
    "\t\ty_true = tf.reshape(y_true, [-1])\n",
    "\t\ty_pred = tf.reshape(y_pred, [-1])\n",
    "\n",
    "\t\t# Apply mask to exclude specified labels\n",
    "\t\tfor label in self.labels_to_exclude:\n",
    "\t\t\tmask = tf.not_equal(y_true, label)\n",
    "\t\t\ty_true = tf.boolean_mask(y_true, mask)\n",
    "\t\t\ty_pred = tf.boolean_mask(y_pred, mask)\n",
    "\n",
    "\t\t# Update the state\n",
    "\t\treturn super().update_state(y_true, y_pred, sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§—ðŸ»â€â™‚ï¸ Train and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:00:16.008010Z",
     "iopub.status.busy": "2024-11-21T21:00:16.007418Z",
     "iopub.status.idle": "2024-11-22T01:38:32.003037Z",
     "shell.execute_reply": "2024-11-22T01:38:32.001978Z",
     "shell.execute_reply.started": "2024-11-21T21:00:16.007972Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not model_filename_override:\n",
    "\tmodel = model_dict[model_name](IMG_SIZE + (1,), NUM_CLASSES)\n",
    "\tmiou = MeanIntersectionOverUnion(num_classes=NUM_CLASSES, labels_to_exclude=[0])\n",
    "\n",
    "\thistories = []\n",
    "\n",
    "\tfor ts in training_schedule:\n",
    "\t\tmodel.compile(loss=loss_fn,\n",
    "\t\t\t\t\t\t\t\t\toptimizer=get_optimizer(batch_size=ts['batch_size'], lr = ts['lr'], opt=ts['opt_name']), \n",
    "\t\t\t\t\t\t\t\t\tmetrics=['accuracy', miou])\n",
    "\t\tdisplay_model(model)\n",
    "\t\tval_data_loader = get_dataset(X_val, y_val, batch_size=ts['batch_size'])\n",
    "\t\tif ts['augmentation']:\n",
    "\t\t\tif ts.get('enlarge_dataset_with_custom_np_ds', False):\n",
    "\t\t\t\t# This example enlarges the images with a lot of bg labels (>=90%)\n",
    "\t\t\t\tbg_augmented_data = get_enlarged_dataset(bg_imgs, bg_labels, build_augmentation_bg)\n",
    "\t\t\t\t# You can add additional data and then using additional_ds_concat=[bg_augmented_data, ...]\n",
    "\t\t\t\tdata_loader = get_dataset(X_train, y_train, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tbatch_size=ts['batch_size'], \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\taugmentations=apply_augmentation,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\taugmentation_repetition=ts['augmentation_repetition'], \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tconcat_and_shuffle_aug_with_no_aug=ts['include_non_augmented'], additional_ds_concat=[bg_augmented_data])\n",
    "\t\t\t\tprint(f\"Training with augmentation x{ts['augmentation_repetition']}, enlarge_dataset_with_custom_np_ds and {'none ' if not ts['include_non_augmented'] else ''}non-augmented data.\")\n",
    "\t\t\telse:\n",
    "\t\t\t\tdata_loader = get_dataset(X_train, y_train, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tbatch_size=ts['batch_size'], \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\taugmentations=apply_augmentation,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\taugmentation_repetition=ts['augmentation_repetition'], \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tconcat_and_shuffle_aug_with_no_aug=ts['include_non_augmented'])\n",
    "\t\t\t\tprint(f\"Training with augmentation x{ts['augmentation_repetition']} and {'none ' if not ts['include_non_augmented'] else ''}non-augmented data.\")\n",
    "\t\telse:\n",
    "\t\t\tdata_loader = get_dataset(X_train, y_train, batch_size=ts['batch_size'])\n",
    "\t\t\tprint(f'Fitting model without augmentation')\n",
    "\t\tfit_history = fit_model(model, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tdata_loader=data_loader, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tvalidation_data_loader=val_data_loader,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tepochs = ts['epochs'])\n",
    "\t\thistories.append(fit_history)\n",
    "\n",
    "\t# Calculate and print the final validation accuracy\n",
    "\tfinal_val_meanIoU = round(max(histories[-1]['val_mean_iou'])* 100, 2)\n",
    "\tprint(f'Final validation Mean Intersection Over Union: {final_val_meanIoU}%')\n",
    "\n",
    "\t# Save intermediate model\n",
    "\tmodel_filename = f'{model_name}-{str(final_val_meanIoU)}-{datetime.now().strftime(\"%y%m%d_%H%M\")}.keras'\n",
    "\tmodel.save(model_filename)\n",
    "\n",
    "\t# Free memory by deleting the model instance\n",
    "\tif FREE_MODEL:\n",
    "\t\tdel model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T01:38:32.004572Z",
     "iopub.status.busy": "2024-11-22T01:38:32.004292Z",
     "iopub.status.idle": "2024-11-22T01:38:32.539916Z",
     "shell.execute_reply": "2024-11-22T01:38:32.538708Z",
     "shell.execute_reply.started": "2024-11-22T01:38:32.004546Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_training(fit):\n",
    "\t# Plot and display training and validation loss\n",
    "\tplt.figure(figsize=(18, 3))\n",
    "\tplt.plot(fit['loss'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=2)\n",
    "\tplt.plot(fit['val_loss'], label='Validation', alpha=0.9, color='#5a9aa5', linewidth=2)\n",
    "\tplt.title('Cross Entropy')\n",
    "\tplt.legend()\n",
    "\tplt.grid(alpha=0.3)\n",
    "\tplt.show()\n",
    "\n",
    "\t# Plot and display training and validation accuracy\n",
    "\tplt.figure(figsize=(18, 3))\n",
    "\tplt.plot(fit['accuracy'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=2)\n",
    "\tplt.plot(fit['val_accuracy'], label='Validation', alpha=0.9, color='#5a9aa5', linewidth=2)\n",
    "\tplt.title('Accuracy')\n",
    "\tplt.legend()\n",
    "\tplt.grid(alpha=0.3)\n",
    "\tplt.show()\n",
    "\n",
    "\t# Plot and display training and validation mean IoU\n",
    "\tplt.figure(figsize=(18, 3))\n",
    "\tplt.plot(fit['mean_iou'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=2)\n",
    "\tplt.plot(fit['val_mean_iou'], label='Validation', alpha=0.9, color='#5a9aa5', linewidth=2)\n",
    "\tplt.title('Mean Intersection over Union')\n",
    "\tplt.legend()\n",
    "\tplt.grid(alpha=0.3)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_filename_override:\n",
    "\tfor fit_history in histories:\n",
    "\t\tplot_training(fit_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœðŸ¿ Make prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = get_dataset(X_test, y_test, batch_size=8)\n",
    "\n",
    "if model_filename_override:\n",
    "\tmodel = tfk.models.load_model(model_filename_override, compile=False)\n",
    "\tmodel.compile(\n",
    "\t\tloss=loss_fn,\n",
    "\t\toptimizer=get_optimizer(),\n",
    "\t\tmetrics=[\"accuracy\", miou]\n",
    "\t)\n",
    "\tdisplay_model(model)\n",
    "\n",
    "# Evaluate the model on the test set and print the results\n",
    "test_loss, test_accuracy, test_mean_iou = model.evaluate(test_dataset, verbose=1)\n",
    "print(f'Test Accuracy: {round(test_accuracy, 4)}')\n",
    "print(f'Test Mean Intersection over Union: {round(test_mean_iou, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_segmentation_colormap(num_classes):\n",
    "\t\"\"\"\n",
    "\tCreate a linear colormap using a predefined palette.\n",
    "\tUses 'viridis' as default because it is perceptually uniform\n",
    "\tand works well for colorblindness.\n",
    "\t\"\"\"\n",
    "\treturn plt.cm.viridis(np.linspace(0, 1, num_classes))\n",
    "\n",
    "def apply_colormap(label, colormap=None):\n",
    "\t\"\"\"\n",
    "\tApply the colormap to a label.\n",
    "\t\"\"\"\n",
    "\t# Ensure label is 2D\n",
    "\tlabel = np.squeeze(label)\n",
    "\n",
    "\tif colormap is None:\n",
    "\t\tnum_classes = len(np.unique(label))\n",
    "\t\tcolormap = create_segmentation_colormap(num_classes)\n",
    "\n",
    "\t# Apply the colormap\n",
    "\tcolored = colormap[label.astype(int)]\n",
    "\n",
    "\treturn colored\n",
    "\n",
    "def plot_triptychs(dataset, model, num_samples=1):\n",
    "\t\"\"\"\n",
    "\tPlot triptychs (original image, true mask, predicted mask) for samples from a tf.data.Dataset\n",
    "\n",
    "\tParameters:\n",
    "\tdataset: tf.data.Dataset - The dataset containing image-label pairs\n",
    "\tmodel: tf.keras.Model - The trained model to generate predictions\n",
    "\tnum_samples: int - Number of samples to plot\n",
    "\t\"\"\"\n",
    "\t# Take samples from the dataset\n",
    "\tsamples = dataset.take(num_samples)\n",
    "\n",
    "\tfor images, labels in samples:\n",
    "\t\t# If we have a batch, take the first example\n",
    "\t\tif len(images.shape) == 4:  # Batch of images\n",
    "\t\t\timages = images[0:1]\n",
    "\t\t\tlabels = labels[0:1]\n",
    "\n",
    "\t\t# Generate predictions\n",
    "\t\tpred = model.predict(images, verbose=0)\n",
    "\t\tpred = tf.math.argmax(pred, axis=-1)\n",
    "\n",
    "\t\t# Create colormap based on number of classes in labels\n",
    "\t\tlabels_np = labels.numpy()\n",
    "\t\tnum_classes = len(np.unique(labels_np))\n",
    "\t\tcolormap = create_segmentation_colormap(num_classes)\n",
    "\n",
    "\t\t# Create figure with subplots\n",
    "\t\tfig, axes = plt.subplots(1, 3, figsize=(20, 4))\n",
    "\n",
    "\t\t# Plot original image\n",
    "\t\taxes[0].set_title(\"Original Image\")\n",
    "\t\taxes[0].imshow(images[0])\n",
    "\t\taxes[0].axis('off')\n",
    "\n",
    "\t\t# Plot original mask\n",
    "\t\taxes[1].set_title(\"Original Mask\")\n",
    "\t\tcolored_label = apply_colormap(labels[0], colormap)\n",
    "\t\taxes[1].imshow(colored_label)\n",
    "\t\taxes[1].axis('off')\n",
    "\n",
    "\t\t# Plot predicted mask\n",
    "\t\taxes[2].set_title(\"Predicted Mask\")\n",
    "\t\tcolored_pred = apply_colormap(pred[0], colormap)\n",
    "\t\taxes[2].imshow(colored_pred)\n",
    "\t\taxes[2].axis('off')\n",
    "\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.show()\n",
    "\t\tplt.close()\n",
    "\n",
    "#plot_triptychs(test_dataset, model, num_samples=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸŽ° Make prediction on competition test set and create csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not loading the model but using the python env model as there is a current error on the `MeanIntersectionOverUnion` class which is not serializable making the model not loadable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(hidden_X_test.shape)\n",
    "preds = model.predict(hidden_X_test)\n",
    "preds = np.argmax(preds, axis=-1)\n",
    "print(f\"Predictions shape: {preds.shape}\")\n",
    "\n",
    "def y_to_df(y) -> pd.DataFrame:\n",
    "\t\"\"\"Converts segmentation predictions into a DataFrame format for Kaggle.\"\"\"\n",
    "\tn_samples = len(y)\n",
    "\ty_flat = y.reshape(n_samples, -1)\n",
    "\tdf = pd.DataFrame(y_flat)\n",
    "\tdf[\"id\"] = np.arange(n_samples)\n",
    "\tcols = [\"id\"] + [col for col in df.columns if col != \"id\"]\n",
    "\treturn df[cols]\n",
    "\n",
    "submission_filename = f'submission_{datetime.now().strftime(\"%y%m%d_%H%M\")}.csv'\n",
    "submission_df = y_to_df(preds)\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "print('Submission saved in', submission_filename)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPhN8z97sycURDAjAFsp+EI",
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6047840,
     "sourceId": 9855285,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6047865,
     "sourceId": 9855319,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 159775,
     "modelInstanceId": 137060,
     "sourceId": 161170,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 162416,
     "modelInstanceId": 139795,
     "sourceId": 164339,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 164634,
     "modelInstanceId": 142056,
     "sourceId": 166955,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 165351,
     "modelInstanceId": 142773,
     "sourceId": 167821,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 170852,
     "modelInstanceId": 148341,
     "sourceId": 174251,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
